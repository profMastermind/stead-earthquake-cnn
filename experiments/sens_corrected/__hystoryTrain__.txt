Training loss on iteration 0 = 26.013710021972656
Training loss on iteration 20 = 11.480087304115296
Training loss on iteration 40 = 9.727334809303283
Training loss on iteration 60 = 9.340861129760743
Training loss on iteration 80 = 8.980554008483887
Training loss on iteration 100 = 9.06407949924469
Training loss on iteration 120 = 8.294565725326539
Training loss on iteration 140 = 9.682080912590028
Training loss on iteration 160 = 7.263257741928101
Training loss on iteration 180 = 9.390201234817505
Training loss on iteration 200 = 8.355379712581634
Training loss on iteration 220 = 8.648834538459777
Training loss on iteration 240 = 8.63383502960205
Training loss on iteration 260 = 7.995120859146118
Training loss on iteration 280 = 8.850285458564759
Training loss on iteration 300 = 8.704782080650329
Training loss on iteration 320 = 8.423755288124084
Training loss on iteration 340 = 9.256818294525146
Training loss on iteration 360 = 7.605768156051636
Training loss on iteration 380 = 12.091742420196534
Training loss on iteration 400 = 8.504149770736694
Training loss on iteration 420 = 7.863569164276123
Training loss on iteration 440 = 8.51161024570465
Training loss on iteration 460 = 7.7443196177482605
Training loss on iteration 480 = 9.087236714363097
Training loss on iteration 500 = 8.518888974189759
Training loss on iteration 520 = 7.952024459838867
Training loss on iteration 540 = 9.141019558906555
Training loss on iteration 560 = 8.611992883682252
Training loss on iteration 580 = 8.312099528312682
Training loss on iteration 600 = 7.66029417514801
Training loss on iteration 620 = 9.752397918701172
Training loss on iteration 640 = 8.455734729766846
Training loss on iteration 660 = 8.467166829109193
Training loss on iteration 680 = 7.832120609283447
Training loss on iteration 700 = 9.57783055305481
Training loss on iteration 720 = 8.270650267601013
Training loss on iteration 740 = 9.102514481544494
Training loss on iteration 760 = 8.832620239257812
Training loss on iteration 780 = 8.589762425422668
Training loss on iteration 800 = 8.652995920181274
Training loss on iteration 820 = 7.694216465950012
Training loss on iteration 840 = 8.752602887153625
Training loss on iteration 860 = 8.1169238448143
Training loss on iteration 880 = 6.681063938140869
Training loss on iteration 900 = 16.02635827064514
Training loss on iteration 920 = 7.52518972158432
Training loss on iteration 940 = 8.483459424972533
Training loss on iteration 960 = 8.384673976898194
Training loss on iteration 980 = 8.745075511932374
Training loss on iteration 1000 = 7.5758211493492125
Training loss on iteration 1020 = 7.225893497467041
Training loss on iteration 1040 = 8.103746366500854
Training loss on iteration 1060 = 6.898183107376099
Training loss on iteration 1080 = 6.8047315835952755
Training loss on iteration 1100 = 7.237768590450287
Training loss on iteration 1120 = 5.877648425102234
Training loss on iteration 1140 = 4.624124205112457
Training loss on iteration 1160 = 4.540672945976257
Training loss on iteration 1180 = 4.9024770855903625
Training loss on iteration 1200 = 4.645089101791382
Training loss on iteration 1220 = 4.245284068584442
Training loss on iteration 1240 = 3.602926033735275
Training loss on iteration 1260 = 3.576618140935898
Training loss on iteration 1280 = 4.181602942943573
Training loss on iteration 1300 = 3.6254012286663055
Training loss on iteration 1320 = 3.3095430195331574
Training loss on iteration 1340 = 3.3325345396995543
Training loss on iteration 1360 = 2.819168335199356
Training loss on iteration 1380 = 3.4777249813079836
Training loss on iteration 1400 = 3.3221361339092255
Training loss on iteration 1420 = 2.656503689289093
Training loss on iteration 1440 = 2.79720561504364
Training loss on iteration 1460 = 2.5357929944992064
Training loss on iteration 1480 = 2.5683094203472137
Training loss on iteration 1500 = 2.5196225345134735
Training loss on iteration 1520 = 2.5440230548381804
Training loss on iteration 1540 = 3.1918833673000337
Training loss on iteration 1560 = 7.144051367044449
Training loss on iteration 1580 = 7.503286230564117
Training loss on iteration 1600 = 12.967438280582428
Training loss on iteration 1620 = 5.461064779758454
Training loss on iteration 1640 = 4.785232603549957
Training loss on iteration 1660 = 4.337911832332611
Training loss on iteration 1680 = 5.853111708164215
Training loss on iteration 1700 = 4.898880302906036
Training loss on iteration 1720 = 4.132950663566589
Training loss on iteration 1740 = 3.865648078918457
Training loss on iteration 1760 = 3.130269157886505
Training loss on iteration 1780 = 3.8463491439819335
Training loss on iteration 1800 = 3.912901186943054
Training loss on iteration 1820 = 3.855532819032669
Training loss on iteration 1840 = 3.8057718515396117
Training loss on iteration 1860 = 4.069468903541565
Training loss on iteration 1880 = 4.293089145421982
Training loss on iteration 1900 = 4.042584609985352
Training loss on iteration 1920 = 3.3844911098480224
Training loss on iteration 1940 = 2.801006889343262
Training loss on iteration 1960 = 3.655866080522537
Training loss on iteration 1980 = 3.017682647705078
Training loss on iteration 2000 = 2.859192210435867
Training loss on iteration 2020 = 2.7324100852012636
Training loss on iteration 2040 = 3.095786291360855
Training loss on iteration 2060 = 3.40833899974823
Training loss on iteration 2080 = 3.1104650139808654
Training loss on iteration 2100 = 2.9297036588191987
Training loss on iteration 2120 = 2.702768063545227
Training loss on iteration 2140 = 2.377609932422638
Training loss on iteration 2160 = 2.8360321283340455
Training loss on iteration 2180 = 3.0247667014598845
Training loss on iteration 2200 = 2.6958537340164184
Training loss on iteration 2220 = 2.8091730296611788
Training loss on iteration 2240 = 2.8403174698352815
Training loss on iteration 2260 = 2.8792065739631654
Training loss on iteration 2280 = 2.991443598270416
Training loss on iteration 2300 = 2.669258028268814
Training loss on iteration 2320 = 2.4289803236722944
Training loss on iteration 2340 = 2.5771307229995726
Training loss on iteration 2360 = 2.646705907583237
Training loss on iteration 2380 = 2.4521647572517393
Training loss on iteration 2400 = 2.273218125104904
Training loss on iteration 2420 = 2.1784374356269836
Training loss on iteration 2440 = 2.1373060762882234
Training loss on iteration 2460 = 3.040569323301315
Training loss on iteration 2480 = 2.765696978569031
Training loss on iteration 2500 = 2.24660410284996
Training loss on iteration 2520 = 3.3428964734077455
Training loss on iteration 2540 = 2.473907208442688
Training loss on iteration 2560 = 2.160263276100159
Training loss on iteration 2580 = 2.671186572313309
Training loss on iteration 2600 = 2.889340955018997
Training loss on iteration 2620 = 2.227560567855835
Training loss on iteration 2640 = 2.312137943506241
Training loss on iteration 2660 = 2.52529553771019
Training loss on iteration 2680 = 2.3752219676971436
Training loss on iteration 2700 = 2.7174112439155578
Training loss on iteration 2720 = 2.665497201681137
Training loss on iteration 2740 = 2.386300820112228
Training loss on iteration 2760 = 2.732577604055405
Training loss on iteration 2780 = 2.949549400806427
Training loss on iteration 2800 = 4.816445702314377
Training loss on iteration 2820 = 2.652685117721558
Training loss on iteration 2840 = 1.9327844560146332
Training loss on iteration 2860 = 2.4152418792247774
Training loss on iteration 2880 = 2.238442587852478
Training loss on iteration 2900 = 2.82216591835022
Training loss on iteration 2920 = 2.2594904482364653
Training loss on iteration 2940 = 2.2957780718803407
Training loss on iteration 2960 = 2.7020603686571123
Training loss on iteration 2980 = 3.517256796360016
Training loss on iteration 3000 = 2.4307406902313233
Training loss on iteration 3020 = 2.5198703050613402
Training loss on iteration 3040 = 2.026904973387718
Training loss on iteration 3060 = 2.5158454716205596
Training loss on iteration 3080 = 2.78217157125473
Training loss on iteration 3100 = 2.572861832380295
Training loss on iteration 3120 = 2.0200377970933916
Training loss on iteration 3140 = 2.4567525923252105
Training loss on iteration 3160 = 1.8653484404087066
Training loss on iteration 3180 = 2.525537359714508
Training loss on iteration 3200 = 6.794920670986175
Training loss on iteration 3220 = 3.1473238289356233
Training loss on iteration 3240 = 3.1687886893749235
Training loss on iteration 3260 = 2.7910447120666504
Training loss on iteration 3280 = 2.792406350374222
Training loss on iteration 3300 = 2.3165302097797396
Training loss on iteration 3320 = 2.693428820371628
Training loss on iteration 3340 = 2.380455565452576
Training loss on iteration 3360 = 2.40766721367836
Training loss on iteration 3380 = 2.8597933292388915
Training loss on iteration 3400 = 2.714804744720459
Training loss on iteration 3420 = 2.5961670696735384
Training loss on iteration 3440 = 2.6592753171920775
Training loss on iteration 3460 = 2.312057766318321
Training loss on iteration 3480 = 2.5505011856555937
Training loss on iteration 3500 = 2.2383350849151613
Training loss on iteration 3520 = 2.4085251867771147
Training loss on iteration 3540 = 3.0725773870944977
Training loss on iteration 3560 = 2.728679233789444
Training loss on iteration 3580 = 2.2052094876766204
Training loss on iteration 3600 = 2.2780497789382936
Training loss on iteration 3620 = 2.2592133581638336
Training loss on iteration 3640 = 2.2970930516719816
Training loss on iteration 3660 = 2.506460726261139
Training loss on iteration 3680 = 2.1907089352607727
Training loss on iteration 3700 = 2.327382504940033
Training loss on iteration 3720 = 2.6757165670394896
Training loss on iteration 3740 = 1.8912942320108415
Training loss on iteration 3760 = 1.8235960692167281
Training loss on iteration 3780 = 2.6230110228061676
Training loss on iteration 3800 = 1.9607504695653915
Training loss on iteration 3820 = 2.826450899243355
Training loss on iteration 3840 = 1.8248655915260314
Training loss on iteration 3860 = 1.9939276307821274
Training loss on iteration 3880 = 1.789375177025795
Training loss on iteration 3900 = 1.956605064868927
Training loss on iteration 3920 = 2.4773331373929977
Training loss on iteration 3940 = 2.4617197513580322
Training loss on iteration 3960 = 2.3542207181453705
Training loss on iteration 3980 = 2.14703164100647
Training loss on iteration 4000 = 2.6268764078617095
Training loss on iteration 4020 = 1.7437990754842758
Training loss on iteration 4040 = 2.058343082666397
Training loss on iteration 4060 = 1.8044021666049956
Training loss on iteration 4080 = 1.7530840128660201
Training loss on iteration 4100 = 1.5814985394477845
Training loss on iteration 4120 = 1.7700922071933747
Training loss on iteration 4140 = 1.8510035783052445
Training loss on iteration 4160 = 2.07207804620266
Training loss on iteration 4180 = 1.565642949938774
Training loss on iteration 4200 = 1.720557776093483
Training loss on iteration 4220 = 1.9738494634628296
Training loss on iteration 4240 = 2.7983385741710665
Training loss on iteration 4260 = 2.2552034854888916
Training loss on iteration 4280 = 2.005063071846962
Training loss on iteration 4300 = 1.9692297518253326
Training loss on iteration 4320 = 2.212196570634842
Training loss on iteration 4340 = 2.2331385374069215
Training loss on iteration 4360 = 1.8808693051338197
Training loss on iteration 4380 = 2.2846642315387724
Training loss on iteration 4400 = 2.309136301279068
Training loss on iteration 4420 = 2.2133787661790847
Training loss on iteration 4440 = 1.6461582839488984
Training loss on iteration 4460 = 2.1805444300174712
Training loss on iteration 4480 = 1.4826388984918595
Training loss on iteration 4500 = 2.0989941388368605
Training loss on iteration 4520 = 1.9813799917697907
Training loss on iteration 4540 = 1.9095943808555602
Training loss on iteration 4560 = 1.8595019072294234
Training loss on iteration 4580 = 2.0734698116779327
Training loss on iteration 4600 = 2.0807994186878203
Training loss on iteration 4620 = 1.8563381612300873
Training loss on iteration 4640 = 1.5818100333213807
Training loss on iteration 4660 = 2.2257414042949675
Training loss on iteration 4680 = 1.74779052734375
Training loss on iteration 4700 = 2.029690235853195
Training loss on iteration 4720 = 2.2193316400051115
Training loss on iteration 4740 = 2.001167544722557
Training loss on iteration 4760 = 1.8034504234790802
Training loss on iteration 4780 = 2.439138853549957
Training loss on iteration 4800 = 1.777454736828804
Training loss on iteration 4820 = 2.4070701479911802
Training loss on iteration 4840 = 3.876371443271637
Training loss on iteration 4860 = 2.3773149609565736
Training loss on iteration 4880 = 2.548816353082657
Training loss on iteration 4900 = 1.965130838751793
Training loss on iteration 4920 = 2.1373850107192993
Training loss on iteration 4940 = 2.112510022521019
Training loss on iteration 4960 = 2.070026528835297
Training loss on iteration 4980 = 2.06046182513237
Training loss on iteration 5000 = 2.3457007825374605
Training loss on iteration 5020 = 1.6639165222644805
Training loss on iteration 5040 = 2.102497923374176
Training loss on iteration 5060 = 1.9689501821994781
Training loss on iteration 5080 = 1.9342012882232666
Training loss on iteration 5100 = 2.077343937754631
Training loss on iteration 5120 = 2.183673596382141
Training loss on iteration 5140 = 1.7180861830711365
Training loss on iteration 5160 = 1.7578662276268004
Training loss on iteration 5180 = 1.740015333890915
Training loss on iteration 5200 = 1.714438584446907
Training loss on iteration 5220 = 1.7956046402454375
Training loss on iteration 5240 = 1.9089727818965911
Training loss on iteration 5260 = 1.8035722225904465
Training loss on iteration 5280 = 1.8525445729494094
Training loss on iteration 5300 = 2.148723563551903
Training loss on iteration 5320 = 1.8619101077318192
Training loss on iteration 5340 = 2.404427257180214
Training loss on iteration 5360 = 1.6738380819559098
Training loss on iteration 5380 = 2.0201421856880186
Training loss on iteration 5400 = 1.6025272011756897
Training loss on iteration 5420 = 1.3307097792625426
Training loss on iteration 5440 = 1.8573737621307373
Training loss on iteration 5460 = 1.9526434600353242
Training loss on iteration 5480 = 1.6270250469446181
Training loss on iteration 5500 = 1.869297844171524
Training loss on iteration 5520 = 1.7708602368831634
Training loss on iteration 5540 = 1.569667825102806
Training loss on iteration 5560 = 1.738362967967987
Training loss on iteration 5580 = 1.5304985612630844
Training loss on iteration 5600 = 1.731757226586342
Training loss on iteration 5620 = 1.8172700732946396
Training loss on iteration 5640 = 1.7974807128310204
Training loss on iteration 5660 = 1.792509764432907
Training loss on iteration 5680 = 1.8157527953386308
Training loss on iteration 5700 = 1.7816090494394303
Training loss on iteration 5720 = 1.5797314405441285
Training loss on iteration 5740 = 1.7992596954107285
Training loss on iteration 5760 = 2.0055388152599334
Training loss on iteration 5780 = 1.625922015309334
Training loss on iteration 5800 = 1.6988654464483262
Training loss on iteration 5820 = 1.774147954583168
Training loss on iteration 5840 = 1.8627305448055267
Training loss on iteration 5860 = 1.8136084735393525
Training loss on iteration 5880 = 1.6975844383239747
Training loss on iteration 5900 = 2.1080028146505354
Training loss on iteration 5920 = 2.105133572220802
Training loss on iteration 5940 = 1.6447247505187987
Training loss on iteration 5960 = 1.3686074644327164
Training loss on iteration 5980 = 1.7551907837390899
Training loss on iteration 6000 = 1.8835032284259796
Training loss on iteration 6020 = 1.3710876166820527
Training loss on iteration 6040 = 1.8636716842651366
Training loss on iteration 6060 = 1.919806808233261
Training loss on iteration 6080 = 1.994009366631508
Training loss on iteration 6100 = 2.116171681880951
Training loss on iteration 6120 = 1.4807270467281342
Training loss on iteration 6140 = 1.7401218295097352
Training loss on iteration 6160 = 1.5987436920404434
Training loss on iteration 6180 = 2.0929891884326937
Training loss on iteration 6200 = 2.039980211853981
Training loss on iteration 6220 = 1.7394036412239076
Training loss on iteration 6240 = 1.974357432126999
Training loss on iteration 6260 = 1.6940748244524002
Training loss on iteration 6280 = 1.4132081598043442
Training loss on iteration 6300 = 1.8920384466648101
Training loss on iteration 6320 = 1.652336373925209
Training loss on iteration 6340 = 1.5639753669500351
Training loss on iteration 6360 = 1.6478103160858155
Training loss on iteration 6380 = 1.6664834529161454
Training loss on iteration 0 = 2.262901782989502
Training loss on iteration 20 = 1.378364908695221
Training loss on iteration 40 = 1.71807062625885
Training loss on iteration 60 = 1.5790699809789657
Training loss on iteration 80 = 1.6408835977315903
Training loss on iteration 100 = 1.6066932380199432
Training loss on iteration 120 = 1.6170482277870177
Training loss on iteration 140 = 1.6759012430906295
Training loss on iteration 160 = 1.7445701837539673
Training loss on iteration 180 = 1.8764663249254228
Training loss on iteration 200 = 1.7404105752706527
Training loss on iteration 220 = 1.791849023103714
Training loss on iteration 240 = 1.503625574707985
Training loss on iteration 260 = 1.5765409201383591
Training loss on iteration 280 = 1.7106652289628983
Training loss on iteration 300 = 1.5316494673490524
Training loss on iteration 320 = 1.8791813045740127
Training loss on iteration 340 = 1.9100413352251053
Training loss on iteration 360 = 1.5244839817285538
Training loss on iteration 380 = 1.3319439619779587
Training loss on iteration 400 = 1.1434526100754738
Training loss on iteration 420 = 1.1589640438556672
Training loss on iteration 440 = 1.6284343510866166
Training loss on iteration 460 = 1.5300339609384537
Training loss on iteration 480 = 1.7095876723527907
Training loss on iteration 500 = 1.537133201956749
Training loss on iteration 520 = 1.5710350900888443
Training loss on iteration 540 = 1.388897955417633
Training loss on iteration 560 = 1.2600383579730987
Training loss on iteration 580 = 1.364740315079689
Training loss on iteration 600 = 1.5742368757724763
Training loss on iteration 620 = 1.5605738878250122
Training loss on iteration 640 = 1.319221132993698
Training loss on iteration 660 = 1.9620418161153794
Training loss on iteration 680 = 1.5119538754224777
Training loss on iteration 700 = 1.7793629497289658
Training loss on iteration 720 = 1.3385975375771522
Training loss on iteration 740 = 1.5215931624174117
Training loss on iteration 760 = 1.5396083295345306
Training loss on iteration 780 = 1.0833173528313638
Training loss on iteration 800 = 1.6766135931015014
Training loss on iteration 820 = 1.8037618845701218
Training loss on iteration 840 = 1.4407788619399071
Training loss on iteration 860 = 1.6236862540245056
Training loss on iteration 880 = 1.466167402267456
Training loss on iteration 900 = 1.1656030505895614
Training loss on iteration 920 = 1.2225150033831595
Training loss on iteration 940 = 1.2183003306388855
Training loss on iteration 960 = 1.544623574614525
Training loss on iteration 980 = 1.9696016967296601
Training loss on iteration 1000 = 1.5390110611915588
Training loss on iteration 1020 = 1.5169041156768799
Training loss on iteration 1040 = 1.9958934009075164
Training loss on iteration 1060 = 1.414714413881302
Training loss on iteration 1080 = 1.3791722297668456
Training loss on iteration 1100 = 1.2722247898578645
Training loss on iteration 1120 = 1.3026650428771973
Training loss on iteration 1140 = 1.4307307839393615
Training loss on iteration 1160 = 1.5319854706525802
Training loss on iteration 1180 = 1.162428829073906
Training loss on iteration 1200 = 1.3162523627281189
Training loss on iteration 1220 = 1.1759819775819778
Training loss on iteration 1240 = 1.531567218899727
Training loss on iteration 1260 = 1.2029440268874168
Training loss on iteration 1280 = 1.4757181376218795
Training loss on iteration 1300 = 1.3255309551954269
Training loss on iteration 1320 = 1.4119897782802582
Training loss on iteration 1340 = 1.3978453785181046
Training loss on iteration 1360 = 1.6391002714633942
Training loss on iteration 1380 = 1.1549714744091033
Training loss on iteration 1400 = 1.6985808998346328
Training loss on iteration 1420 = 1.5947305619716645
Training loss on iteration 1440 = 1.237422177195549
Training loss on iteration 1460 = 1.3585578575730324
Training loss on iteration 1480 = 1.3720051229000092
Training loss on iteration 1500 = 1.411296436190605
Training loss on iteration 1520 = 1.3421434178948402
Training loss on iteration 1540 = 1.1872633576393128
Training loss on iteration 1560 = 1.2885821357369422
Training loss on iteration 1580 = 1.6282269269227982
Training loss on iteration 1600 = 1.502123013138771
Training loss on iteration 1620 = 1.3621502846479416
Training loss on iteration 1640 = 1.3006866961717605
Training loss on iteration 1660 = 1.2846429646015167
Training loss on iteration 1680 = 1.7097030401229858
Training loss on iteration 1700 = 1.614681500196457
Training loss on iteration 1720 = 1.5830147117376328
Training loss on iteration 1740 = 1.3672047674655914
Training loss on iteration 1760 = 1.5378107696771621
Training loss on iteration 1780 = 1.5165584474802016
Training loss on iteration 1800 = 1.5454740703105927
Training loss on iteration 1820 = 1.39569531083107
Training loss on iteration 1840 = 1.6139943808317185
Training loss on iteration 1860 = 1.4588333129882813
Training loss on iteration 1880 = 1.235906821489334
Training loss on iteration 1900 = 1.2012129202485085
Training loss on iteration 1920 = 1.4645495265722275
Training loss on iteration 1940 = 1.5927374228835105
Training loss on iteration 1960 = 1.2560993880033493
Training loss on iteration 1980 = 1.414449506998062
Training loss on iteration 2000 = 1.4163896784186363
Training loss on iteration 2020 = 1.6779128670692445
Training loss on iteration 2040 = 1.285192148387432
Training loss on iteration 2060 = 1.2588069498538972
Training loss on iteration 2080 = 1.1187741309404373
Training loss on iteration 2100 = 1.8009492993354796
Training loss on iteration 2120 = 1.6369729429483413
Training loss on iteration 2140 = 1.3855663329362868
Training loss on iteration 2160 = 1.2849936336278915
Training loss on iteration 2180 = 1.188696813583374
Training loss on iteration 2200 = 1.1626218929886818
Training loss on iteration 2220 = 1.3932305842638015
Training loss on iteration 2240 = 1.06227465569973
Training loss on iteration 2260 = 1.5261141002178191
Training loss on iteration 2280 = 1.909747388958931
Training loss on iteration 2300 = 1.171911597251892
Training loss on iteration 2320 = 1.6486064672470093
Training loss on iteration 2340 = 1.563862407207489
Training loss on iteration 2360 = 1.5087189257144928
Training loss on iteration 2380 = 1.4548313736915588
Training loss on iteration 2400 = 1.290664854645729
Training loss on iteration 2420 = 2.005013370513916
Training loss on iteration 2440 = 1.7008751541376115
Training loss on iteration 2460 = 1.5102012634277344
Training loss on iteration 2480 = 1.1871313780546189
Training loss on iteration 2500 = 1.2491053238511085
Training loss on iteration 2520 = 1.5783087223768235
Training loss on iteration 2540 = 1.5649836093187333
Training loss on iteration 2560 = 1.2204855144023896
Training loss on iteration 2580 = 1.2195606023073196
Training loss on iteration 2600 = 1.5971364468336104
Training loss on iteration 2620 = 1.3984365314245224
Training loss on iteration 2640 = 1.5355055809020997
Training loss on iteration 2660 = 1.481029886007309
Training loss on iteration 2680 = 1.6020482182502747
Training loss on iteration 2700 = 1.5925216406583786
Training loss on iteration 2720 = 1.2627069503068924
Training loss on iteration 2740 = 1.6299394220113754
Training loss on iteration 2760 = 1.4381998836994172
Training loss on iteration 2780 = 1.1850355386734008
Training loss on iteration 2800 = 1.236273357272148
Training loss on iteration 2820 = 1.1436584413051605
Training loss on iteration 2840 = 1.0931975573301316
Training loss on iteration 2860 = 1.0375965803861618
Training loss on iteration 2880 = 1.325975129008293
Training loss on iteration 2900 = 1.4406509160995484
Training loss on iteration 2920 = 1.7265904873609543
Training loss on iteration 2940 = 1.5777983844280243
Training loss on iteration 2960 = 1.489758461713791
Training loss on iteration 2980 = 1.6882723599672318
Training loss on iteration 3000 = 1.3772086307406426
Training loss on iteration 3020 = 1.6343830108642579
Training loss on iteration 3040 = 1.600039142370224
Training loss on iteration 3060 = 1.3713926881551743
Training loss on iteration 3080 = 1.2899248450994492
Training loss on iteration 3100 = 1.3068866953253746
Training loss on iteration 3120 = 1.1401837706565856
Training loss on iteration 3140 = 1.4172815188765526
Training loss on iteration 3160 = 1.342293804883957
Training loss on iteration 3180 = 1.341810515522957
Training loss on iteration 3200 = 1.4356163024902344
Training loss on iteration 3220 = 1.2957901388406754
Training loss on iteration 3240 = 1.3454579174518586
Training loss on iteration 3260 = 1.4099276021122933
Training loss on iteration 3280 = 1.3559455931186677
Training loss on iteration 3300 = 1.2162401169538497
Training loss on iteration 3320 = 1.3479049801826477
Training loss on iteration 3340 = 1.6293485015630722
Training loss on iteration 3360 = 1.4214706107974053
Training loss on iteration 3380 = 1.204431301355362
Training loss on iteration 3400 = 1.0228725418448448
Training loss on iteration 3420 = 1.3354245498776436
Training loss on iteration 3440 = 1.3437776252627374
Training loss on iteration 3460 = 1.1750534310936929
Training loss on iteration 3480 = 1.4594875276088715
Training loss on iteration 3500 = 1.4487026274204253
Training loss on iteration 3520 = 1.4662050276994705
Training loss on iteration 3540 = 1.9206053376197816
Training loss on iteration 3560 = 1.2382678151130677
Training loss on iteration 3580 = 1.475191530585289
Training loss on iteration 3600 = 1.3531846582889557
Training loss on iteration 3620 = 1.3340433061122894
Training loss on iteration 3640 = 1.2554091721773148
Training loss on iteration 3660 = 1.1642234146595
Training loss on iteration 3680 = 1.6390486776828765
Training loss on iteration 3700 = 1.6176709473133086
Training loss on iteration 3720 = 1.681180626153946
Training loss on iteration 3740 = 1.5706343293190002
Training loss on iteration 3760 = 1.4618897259235382
Training loss on iteration 3780 = 1.1138838291168214
Training loss on iteration 3800 = 1.1268329709768294
Training loss on iteration 3820 = 1.4049275070428848
Training loss on iteration 3840 = 1.1322555124759675
Training loss on iteration 3860 = 1.1931270450353622
Training loss on iteration 3880 = 1.2389582499861718
Training loss on iteration 3900 = 1.4717570558190345
Training loss on iteration 3920 = 1.363675944507122
Training loss on iteration 3940 = 1.2489873975515366
Training loss on iteration 3960 = 1.163144825398922
Training loss on iteration 3980 = 0.9306412592530251
Training loss on iteration 4000 = 0.9514278322458267
Training loss on iteration 4020 = 1.3935433760285378
Training loss on iteration 4040 = 1.2830020919442178
Training loss on iteration 4060 = 1.1700545787811278
Training loss on iteration 4080 = 1.0661365956068038
Training loss on iteration 4100 = 1.229599453508854
Training loss on iteration 4120 = 1.4030493378639222
Training loss on iteration 4140 = 1.3126023709774017
Training loss on iteration 4160 = 1.0009417846798896
Training loss on iteration 4180 = 1.3474312290549277
Training loss on iteration 4200 = 1.1944341748952865
Training loss on iteration 4220 = 1.5125605791807175
Training loss on iteration 4240 = 1.2039813071489334
Training loss on iteration 4260 = 1.1826058194041251
Training loss on iteration 4280 = 1.1061857372522355
Training loss on iteration 4300 = 1.2495836287736892
Training loss on iteration 4320 = 1.191055391728878
Training loss on iteration 4340 = 1.0904185250401497
Training loss on iteration 4360 = 1.5214811444282532
Training loss on iteration 4380 = 1.525400510430336
Training loss on iteration 4400 = 1.588145299255848
Training loss on iteration 4420 = 1.5796771883964538
Training loss on iteration 4440 = 1.2661452621221543
Training loss on iteration 4460 = 1.3846840992569924
Training loss on iteration 4480 = 1.195495292544365
Training loss on iteration 4500 = 1.7956270664930343
Training loss on iteration 4520 = 1.32603120803833
Training loss on iteration 4540 = 1.1649756893515586
Training loss on iteration 4560 = 0.979547281563282
Training loss on iteration 4580 = 1.2579907581210137
Training loss on iteration 4600 = 1.3635569840669632
Training loss on iteration 4620 = 1.3279915064573289
Training loss on iteration 4640 = 1.3253167793154716
Training loss on iteration 4660 = 1.2995805770158768
Training loss on iteration 4680 = 1.0974865421652793
Training loss on iteration 4700 = 1.2522248446941375
Training loss on iteration 4720 = 1.2759827390313148
Training loss on iteration 4740 = 1.1826229736208915
Training loss on iteration 4760 = 1.5394742503762244
Training loss on iteration 4780 = 1.3912310183048249
Training loss on iteration 4800 = 1.3700913459062576
Training loss on iteration 4820 = 1.355203452706337
Training loss on iteration 4840 = 1.2241584926843643
Training loss on iteration 4860 = 1.3758057355880737
Training loss on iteration 4880 = 1.218443827331066
Training loss on iteration 4900 = 1.0474905699491501
Training loss on iteration 4920 = 1.253096404671669
Training loss on iteration 4940 = 1.0771095693111419
Training loss on iteration 4960 = 0.9582168653607368
Training loss on iteration 4980 = 1.1392286002635956
Training loss on iteration 5000 = 1.3381012126803398
Training loss on iteration 5020 = 1.261413460969925
Training loss on iteration 5040 = 1.0715635314583778
Training loss on iteration 5060 = 0.9892020329833031
Training loss on iteration 5080 = 1.1034629121422768
Training loss on iteration 5100 = 1.1688832506537437
Training loss on iteration 5120 = 1.1936285749077797
Training loss on iteration 5140 = 1.1556921690702437
Training loss on iteration 5160 = 0.9465167745947838
Training loss on iteration 5180 = 1.1025318264961244
Training loss on iteration 5200 = 0.9765184700489045
Training loss on iteration 5220 = 1.228555217385292
Training loss on iteration 5240 = 1.4475988894701004
Training loss on iteration 5260 = 1.10460008084774
Training loss on iteration 5280 = 1.3392803102731705
Training loss on iteration 5300 = 1.2607670962810515
Training loss on iteration 5320 = 1.4818333089351654
Training loss on iteration 5340 = 1.3011110037565232
Training loss on iteration 5360 = 1.1013229072093964
Training loss on iteration 5380 = 1.2466814249753952
Training loss on iteration 5400 = 1.2624839276075364
Training loss on iteration 5420 = 1.427868740260601
Training loss on iteration 5440 = 1.209422491490841
Training loss on iteration 5460 = 1.08606096804142
Training loss on iteration 5480 = 0.9721234112977981
Training loss on iteration 5500 = 1.0722070425748824
Training loss on iteration 5520 = 1.661354699730873
Training loss on iteration 5540 = 1.3163839906454087
Training loss on iteration 5560 = 1.0346913784742355
Training loss on iteration 5580 = 1.0913204908370973
Training loss on iteration 5600 = 1.277143344283104
Training loss on iteration 5620 = 1.5248746678233147
Training loss on iteration 5640 = 1.3451282158493996
Training loss on iteration 5660 = 1.1696252718567848
Training loss on iteration 5680 = 0.990610545873642
Training loss on iteration 5700 = 0.930788604915142
Training loss on iteration 5720 = 1.1386051416397094
Training loss on iteration 5740 = 0.9692987158894539
Training loss on iteration 5760 = 1.1524403795599938
Training loss on iteration 5780 = 1.0770932897925376
Training loss on iteration 5800 = 1.0930531188845634
Training loss on iteration 5820 = 0.9584646746516228
Training loss on iteration 5840 = 1.2094742208719254
Training loss on iteration 5860 = 1.1271000936627389
Training loss on iteration 5880 = 1.0660839810967446
Training loss on iteration 5900 = 1.1375733494758606
Training loss on iteration 5920 = 1.274404138326645
Training loss on iteration 5940 = 1.1596548497676848
Training loss on iteration 5960 = 1.1751707643270493
Training loss on iteration 5980 = 1.0640467047691344
Training loss on iteration 6000 = 1.0519777968525887
Training loss on iteration 6020 = 1.0635013327002525
Training loss on iteration 6040 = 1.2956547349691392
Training loss on iteration 6060 = 1.321690532565117
Training loss on iteration 6080 = 1.2055289521813393
Training loss on iteration 6100 = 1.1599379867315291
Training loss on iteration 6120 = 1.2106314525008202
Training loss on iteration 6140 = 1.2130683183670044
Training loss on iteration 6160 = 0.9901117339730263
Training loss on iteration 6180 = 1.3173220470547675
Training loss on iteration 6200 = 1.2044057905673982
Training loss on iteration 6220 = 0.949714207649231
Training loss on iteration 6240 = 1.0987041860818862
Training loss on iteration 6260 = 1.184262189269066
Training loss on iteration 6280 = 1.2310954913496972
Training loss on iteration 6300 = 1.216914001107216
Training loss on iteration 6320 = 1.0797982454299926
Training loss on iteration 6340 = 0.9825556084513665
Training loss on iteration 6360 = 1.1854909658432007
Training loss on iteration 6380 = 1.1812053591012954
Training loss on iteration 0 = 0.9388856887817383
Training loss on iteration 20 = 1.4417240470647812
Training loss on iteration 40 = 0.7344085648655891
Training loss on iteration 60 = 1.0643834367394447
Training loss on iteration 80 = 0.8231124714016914
Training loss on iteration 100 = 1.1349720075726508
Training loss on iteration 120 = 1.2099164232611657
Training loss on iteration 140 = 1.148480622470379
Training loss on iteration 160 = 1.1125933527946472
Training loss on iteration 180 = 0.7677499935030937
Training loss on iteration 200 = 1.058089804649353
Training loss on iteration 220 = 1.1150420889258386
Training loss on iteration 240 = 0.9082057729363442
Training loss on iteration 260 = 1.136967995762825
Training loss on iteration 280 = 0.8087619990110397
Training loss on iteration 300 = 1.1444708555936813
Training loss on iteration 320 = 1.0983442470431328
Training loss on iteration 340 = 1.13238425552845
Training loss on iteration 360 = 1.1909721881151198
Training loss on iteration 380 = 1.0978151679039
Training loss on iteration 400 = 1.1869093030691147
Training loss on iteration 420 = 1.0936753511428834
Training loss on iteration 440 = 0.9741505861282349
Training loss on iteration 460 = 1.0195549130439758
Training loss on iteration 480 = 1.285896584391594
Training loss on iteration 500 = 1.0104200527071954
Training loss on iteration 520 = 1.232783055305481
Training loss on iteration 540 = 0.9481022715568542
Training loss on iteration 560 = 1.221145297586918
Training loss on iteration 580 = 0.8627485170960426
Training loss on iteration 600 = 0.9493831813335418
Training loss on iteration 620 = 0.8985992178320885
Training loss on iteration 640 = 0.9203757867217064
Training loss on iteration 660 = 1.2095019400119782
Training loss on iteration 680 = 1.1302392095327378
Training loss on iteration 700 = 0.9864553183317184
Training loss on iteration 720 = 0.9832803398370743
Training loss on iteration 740 = 0.9621785327792167
Training loss on iteration 760 = 1.0172593116760253
Training loss on iteration 780 = 1.0484566241502762
Training loss on iteration 800 = 0.897666658461094
Training loss on iteration 820 = 1.0428311318159103
Training loss on iteration 840 = 0.9893354251980782
Training loss on iteration 860 = 1.01367317289114
Training loss on iteration 880 = 1.371423414349556
Training loss on iteration 900 = 0.9512494221329689
Training loss on iteration 920 = 0.9456371575593948
Training loss on iteration 940 = 1.0510445982217789
Training loss on iteration 960 = 1.2810563802719117
Training loss on iteration 980 = 1.17490716278553
Training loss on iteration 1000 = 1.219853973388672
Training loss on iteration 1020 = 0.9502422392368317
Training loss on iteration 1040 = 1.3144635319709779
Training loss on iteration 1060 = 1.191887167096138
Training loss on iteration 1080 = 0.886915197968483
Training loss on iteration 1100 = 1.1219651699066162
Training loss on iteration 1120 = 0.9407840684056282
Training loss on iteration 1140 = 1.1129537656903268
Training loss on iteration 1160 = 1.4450154632329941
Training loss on iteration 1180 = 1.2361605942249299
Training loss on iteration 1200 = 1.1305693432688713
Training loss on iteration 1220 = 0.9908086627721786
Training loss on iteration 1240 = 1.0494551718235017
Training loss on iteration 1260 = 1.136975857615471
Training loss on iteration 1280 = 1.50511015355587
Training loss on iteration 1300 = 1.0225083917379378
Training loss on iteration 1320 = 1.052055937051773
Training loss on iteration 1340 = 1.1852588087320328
Training loss on iteration 1360 = 1.2683541804552079
Training loss on iteration 1380 = 1.28634369969368
Training loss on iteration 1400 = 1.3745317727327346
Training loss on iteration 1420 = 1.0599804401397706
Training loss on iteration 1440 = 1.102337196469307
Training loss on iteration 1460 = 1.073681902885437
Training loss on iteration 1480 = 0.9957392305135727
Training loss on iteration 1500 = 0.9801012054085732
Training loss on iteration 1520 = 1.1782369002699853
Training loss on iteration 1540 = 0.9901825785636902
Training loss on iteration 1560 = 1.0386340841650963
Training loss on iteration 1580 = 1.0784035533666612
Training loss on iteration 1600 = 0.8568840891122818
Training loss on iteration 1620 = 1.004059810936451
Training loss on iteration 1640 = 0.9315906271338463
Training loss on iteration 1660 = 1.2858159422874451
Training loss on iteration 1680 = 1.1538574755191804
Training loss on iteration 1700 = 0.89257752597332
Training loss on iteration 1720 = 0.9571978971362114
Training loss on iteration 1740 = 0.9489670038223267
Training loss on iteration 1760 = 1.2200697913765908
Training loss on iteration 1780 = 1.1628090187907218
Training loss on iteration 1800 = 1.0522321969270707
Training loss on iteration 1820 = 0.9664276152849197
Training loss on iteration 1840 = 0.9780687376856804
Training loss on iteration 1860 = 0.9463080748915672
Training loss on iteration 1880 = 1.0715415269136428
Training loss on iteration 1900 = 1.1930005535483361
Training loss on iteration 1920 = 0.9549404263496399
Training loss on iteration 1940 = 0.9016409888863564
Training loss on iteration 1960 = 1.0112133890390396
Training loss on iteration 1980 = 1.092796152830124
Training loss on iteration 2000 = 0.9127197325229645
Training loss on iteration 2020 = 0.8563377887010575
Training loss on iteration 2040 = 1.0201172530651093
Training loss on iteration 2060 = 0.9491679713129997
Training loss on iteration 2080 = 1.1382901296019554
Training loss on iteration 2100 = 0.9710536286234855
Training loss on iteration 2120 = 0.8284009113907814
Training loss on iteration 2140 = 1.0770229905843736
Training loss on iteration 2160 = 1.0799399331212043
Training loss on iteration 2180 = 1.0696721538901328
Training loss on iteration 2200 = 1.2414299845695496
Training loss on iteration 2220 = 1.0553246334195137
Training loss on iteration 2240 = 0.870083524286747
Training loss on iteration 2260 = 0.922516442835331
Training loss on iteration 2280 = 0.9955443710088729
Training loss on iteration 2300 = 0.914037837088108
Training loss on iteration 2320 = 0.898792815208435
Training loss on iteration 2340 = 1.2102230839431285
Training loss on iteration 2360 = 0.9606688439846038
Training loss on iteration 2380 = 1.1080528110265733
Training loss on iteration 2400 = 0.99517642557621
Training loss on iteration 2420 = 1.0854609668254853
Training loss on iteration 2440 = 1.2393481850624084
Training loss on iteration 2460 = 1.0375401109457016
Training loss on iteration 2480 = 1.257467696070671
Training loss on iteration 2500 = 1.2307815209031105
Training loss on iteration 2520 = 1.1438344612717628
Training loss on iteration 2540 = 1.210783977806568
Training loss on iteration 2560 = 1.0031319379806518
Training loss on iteration 2580 = 1.2767546728253365
Training loss on iteration 2600 = 1.0670678198337555
Training loss on iteration 2620 = 0.8956246703863144
Training loss on iteration 2640 = 0.8038662612438202
Training loss on iteration 2660 = 0.9324547693133354
Training loss on iteration 2680 = 0.9433295756578446
Training loss on iteration 2700 = 0.9770358219742775
Training loss on iteration 2720 = 1.0675014927983284
Training loss on iteration 2740 = 1.0885378926992417
Training loss on iteration 2760 = 0.8998462915420532
Training loss on iteration 2780 = 0.9105546250939369
Training loss on iteration 2800 = 0.8802868768572807
Training loss on iteration 2820 = 1.0688936397433282
Training loss on iteration 2840 = 1.0758388176560403
Training loss on iteration 2860 = 1.0223971709609032
Training loss on iteration 2880 = 1.098861400783062
Training loss on iteration 2900 = 1.222436597943306
Training loss on iteration 2920 = 1.077998162806034
Training loss on iteration 2940 = 1.1594691008329392
Training loss on iteration 2960 = 0.9537631332874298
Training loss on iteration 2980 = 1.1951309219002724
Training loss on iteration 3000 = 0.9711446285247802
Training loss on iteration 3020 = 0.9292505711317063
Training loss on iteration 3040 = 1.0303920820355414
Training loss on iteration 3060 = 0.975603611767292
Training loss on iteration 3080 = 0.9602168574929237
Training loss on iteration 3100 = 0.9538650929927825
Training loss on iteration 3120 = 0.837601788341999
Training loss on iteration 3140 = 0.9045681953430176
Training loss on iteration 3160 = 1.1213566198945046
Training loss on iteration 3180 = 0.8424185782670974
Training loss on iteration 3200 = 1.00043885409832
Training loss on iteration 3220 = 1.0454297319054604
Training loss on iteration 3240 = 0.8922629505395889
Training loss on iteration 3260 = 1.1021236076951026
Training loss on iteration 3280 = 0.9540995389223099
Training loss on iteration 3300 = 1.1977112576365472
Training loss on iteration 3320 = 1.0964255183935165
Training loss on iteration 3340 = 1.0209994286298751
Training loss on iteration 3360 = 0.8369973123073577
Training loss on iteration 3380 = 1.0847134798765183
Training loss on iteration 3400 = 0.8050550162792206
Training loss on iteration 3420 = 0.9217006891965867
Training loss on iteration 3440 = 1.0260030135512352
Training loss on iteration 3460 = 1.0428965777158736
Training loss on iteration 3480 = 0.9279336392879486
Training loss on iteration 3500 = 0.9116287305951118
Training loss on iteration 3520 = 0.8625591799616814
Training loss on iteration 3540 = 1.0231928572058677
Training loss on iteration 3560 = 0.9252131253480911
Training loss on iteration 3580 = 1.0423275589942933
Training loss on iteration 3600 = 0.9767156630754471
Training loss on iteration 3620 = 0.7863087162375451
Training loss on iteration 3640 = 0.8718188717961312
Training loss on iteration 3660 = 1.0278860703110695
Training loss on iteration 3680 = 0.8901422798633576
Training loss on iteration 3700 = 0.9323280259966851
Training loss on iteration 3720 = 0.8845952287316322
Training loss on iteration 3740 = 0.8061508685350418
Training loss on iteration 3760 = 0.7934976920485497
Training loss on iteration 3780 = 0.8559323713183403
Training loss on iteration 3800 = 0.9349229171872139
Training loss on iteration 3820 = 0.9299661457538605
Training loss on iteration 3840 = 1.0692712470889092
Training loss on iteration 3860 = 1.1227549001574517
Training loss on iteration 3880 = 0.8347099304199219
Training loss on iteration 3900 = 1.1096481263637543
Training loss on iteration 3920 = 1.1695239529013635
Training loss on iteration 3940 = 0.9001320764422417
Training loss on iteration 3960 = 1.0979754999279976
Training loss on iteration 3980 = 0.8934368908405304
Training loss on iteration 4000 = 1.1950470000505446
Training loss on iteration 4020 = 1.0575178548693658
Training loss on iteration 4040 = 1.2124148949980735
Training loss on iteration 4060 = 1.054015326499939
Training loss on iteration 4080 = 1.0712753117084504
Training loss on iteration 4100 = 1.1064282953739166
Training loss on iteration 4120 = 0.816428741812706
Training loss on iteration 4140 = 0.9810630813241005
Training loss on iteration 4160 = 0.8114546567201615
Training loss on iteration 4180 = 1.0390102952718734
Training loss on iteration 4200 = 1.129446592926979
Training loss on iteration 4220 = 0.840625710785389
Training loss on iteration 4240 = 1.0479661643505096
Training loss on iteration 4260 = 0.7939639702439308
Training loss on iteration 4280 = 0.9406953252851963
Training loss on iteration 4300 = 0.8002802759408951
Training loss on iteration 4320 = 0.9474898397922515
Training loss on iteration 4340 = 0.8394844308495522
Training loss on iteration 4360 = 0.9710606724023819
Training loss on iteration 4380 = 0.8289412751793861
Training loss on iteration 4400 = 1.0053754433989526
Training loss on iteration 4420 = 1.0083689346909523
Training loss on iteration 4440 = 1.2710420191287994
Training loss on iteration 4460 = 0.8229847192764282
Training loss on iteration 4480 = 1.037346638739109
Training loss on iteration 4500 = 0.8934906959533692
Training loss on iteration 4520 = 1.0937085032463074
Training loss on iteration 4540 = 1.023709774762392
Training loss on iteration 4560 = 0.9768063396215438
Training loss on iteration 4580 = 1.074817569553852
Training loss on iteration 4600 = 1.2309628963470458
Training loss on iteration 4620 = 1.1116020992398261
Training loss on iteration 4640 = 1.1228856801986695
Training loss on iteration 4660 = 0.915227261185646
Training loss on iteration 4680 = 0.8761361181735993
Training loss on iteration 4700 = 0.8629299759864807
Training loss on iteration 4720 = 0.7720303535461426
Training loss on iteration 4740 = 0.8774476200342178
Training loss on iteration 4760 = 0.7668003410100936
Training loss on iteration 4780 = 0.8183555632829667
Training loss on iteration 4800 = 0.8396000638604164
Training loss on iteration 4820 = 0.9278943479061127
Training loss on iteration 4840 = 1.093335673213005
Training loss on iteration 4860 = 0.9720808178186416
Training loss on iteration 4880 = 1.0107158407568932
Training loss on iteration 4900 = 0.8090912029147148
Training loss on iteration 4920 = 0.9965582191944122
Training loss on iteration 4940 = 0.8070816025137901
Training loss on iteration 4960 = 0.8535395503044129
Training loss on iteration 4980 = 1.2703798055648803
Training loss on iteration 5000 = 0.7479387745261192
Training loss on iteration 5020 = 1.0377602249383926
Training loss on iteration 5040 = 1.0344417348504067
Training loss on iteration 5060 = 0.8192035093903541
Training loss on iteration 5080 = 0.9892048604786396
Training loss on iteration 5100 = 0.6209017276763916
Training loss on iteration 5120 = 0.9266273856163025
Training loss on iteration 5140 = 1.1593524426221848
Training loss on iteration 5160 = 0.7092366635799408
Training loss on iteration 5180 = 1.0769299894571305
Training loss on iteration 5200 = 0.9062516242265701
Training loss on iteration 5220 = 0.836939612030983
Training loss on iteration 5240 = 0.8286373406648636
Training loss on iteration 5260 = 1.0479716822504996
Training loss on iteration 5280 = 1.276070800423622
Training loss on iteration 5300 = 1.0425660878419876
Training loss on iteration 5320 = 0.9766782030463219
Training loss on iteration 5340 = 0.9979755565524101
Training loss on iteration 5360 = 0.8494284629821778
Training loss on iteration 5380 = 0.789642134308815
Training loss on iteration 5400 = 0.7982522040605545
Training loss on iteration 5420 = 0.8234564006328583
Training loss on iteration 5440 = 0.9583795607089997
Training loss on iteration 5460 = 0.7796674937009811
Training loss on iteration 5480 = 0.7908854395151138
Training loss on iteration 5500 = 0.8525421127676964
Training loss on iteration 5520 = 0.7198084130883217
Training loss on iteration 5540 = 1.2652755349874496
Training loss on iteration 5560 = 1.199160671234131
Training loss on iteration 5580 = 0.7921036958694458
Training loss on iteration 5600 = 0.9588973432779312
Training loss on iteration 5620 = 0.890627758204937
Training loss on iteration 5640 = 1.0851052850484848
Training loss on iteration 5660 = 0.916803976893425
Training loss on iteration 5680 = 0.8727616310119629
Training loss on iteration 5700 = 0.9363197952508926
Training loss on iteration 5720 = 1.0092303216457368
Training loss on iteration 5740 = 0.7542266741394996
Training loss on iteration 5760 = 0.9308125957846641
Training loss on iteration 5780 = 0.7986611127853394
Training loss on iteration 5800 = 1.1134621530771256
Training loss on iteration 5820 = 1.0906868442893027
Training loss on iteration 5840 = 0.9760108157992363
Training loss on iteration 5860 = 0.9479156926274299
Training loss on iteration 5880 = 1.1649088740348816
Training loss on iteration 5900 = 0.9572002500295639
Training loss on iteration 5920 = 1.164257438480854
Training loss on iteration 5940 = 1.0287957340478897
Training loss on iteration 5960 = 0.8580455735325814
Training loss on iteration 5980 = 0.8775262907147408
Training loss on iteration 6000 = 0.8516940176486969
Training loss on iteration 6020 = 0.7725186288356781
Training loss on iteration 6040 = 0.8190563395619392
Training loss on iteration 6060 = 0.7752746760845184
Training loss on iteration 6080 = 0.9720118746161461
Training loss on iteration 6100 = 0.9183118760585784
Training loss on iteration 6120 = 1.0526488468050956
Training loss on iteration 6140 = 0.909313689172268
Training loss on iteration 6160 = 1.100598691403866
Training loss on iteration 6180 = 0.9529525250196457
Training loss on iteration 6200 = 0.8660275489091873
Training loss on iteration 6220 = 0.897905184328556
Training loss on iteration 6240 = 0.8350401192903518
Training loss on iteration 6260 = 1.01865346878767
Training loss on iteration 6280 = 0.6410257756710053
Training loss on iteration 6300 = 0.9230788215994835
Training loss on iteration 6320 = 0.7065991565585137
Training loss on iteration 6340 = 0.9066289246082306
Training loss on iteration 6360 = 0.8405277878046036
Training loss on iteration 6380 = 0.8888553768396378
Training loss on iteration 0 = 0.4586246609687805
Training loss on iteration 20 = 1.010409101843834
Training loss on iteration 40 = 0.7707002431154251
Training loss on iteration 60 = 0.6538170389831066
Training loss on iteration 80 = 0.9021644294261932
Training loss on iteration 100 = 0.779258380830288
Training loss on iteration 120 = 0.7894050985574722
Training loss on iteration 140 = 0.8241870895028114
Training loss on iteration 160 = 0.8594376504421234
Training loss on iteration 180 = 0.9641505807638169
Training loss on iteration 200 = 0.9305807635188103
Training loss on iteration 220 = 0.9648999512195587
Training loss on iteration 240 = 0.9038179039955139
Training loss on iteration 260 = 0.8949083462357521
Training loss on iteration 280 = 0.8506912782788276
Training loss on iteration 300 = 0.9104064092040062
Training loss on iteration 320 = 0.776109878718853
Training loss on iteration 340 = 0.9021562620997429
Training loss on iteration 360 = 0.8392343759536743
Training loss on iteration 380 = 1.028861589729786
Training loss on iteration 400 = 0.8189318284392357
Training loss on iteration 420 = 0.6815126702189446
Training loss on iteration 440 = 0.8694920063018798
Training loss on iteration 460 = 0.8878924906253814
Training loss on iteration 480 = 0.9465850472450257
Training loss on iteration 500 = 0.8300944536924362
Training loss on iteration 520 = 0.7857913121581077
Training loss on iteration 540 = 0.8328215382993222
Training loss on iteration 560 = 0.9654351949691773
Training loss on iteration 580 = 0.8040953010320664
Training loss on iteration 600 = 0.9012308306992054
Training loss on iteration 620 = 0.9627144262194633
Training loss on iteration 640 = 0.8211070403456688
Training loss on iteration 660 = 0.7345929175615311
Training loss on iteration 680 = 0.9066209122538567
Training loss on iteration 700 = 0.7378037095069885
Training loss on iteration 720 = 0.8963398218154908
Training loss on iteration 740 = 0.6048850163817405
Training loss on iteration 760 = 1.0899187907576562
Training loss on iteration 780 = 0.8611659482121468
Training loss on iteration 800 = 0.8416318878531456
Training loss on iteration 820 = 0.8735166862607002
Training loss on iteration 840 = 0.8677635654807091
Training loss on iteration 860 = 0.8610996320843697
Training loss on iteration 880 = 0.910017867386341
Training loss on iteration 900 = 0.7571712553501129
Training loss on iteration 920 = 1.1025944843888282
Training loss on iteration 940 = 0.974048963189125
Training loss on iteration 960 = 0.7704134210944176
Training loss on iteration 980 = 0.8661585420370101
Training loss on iteration 1000 = 0.9517330229282379
Training loss on iteration 1020 = 1.079380628466606
Training loss on iteration 1040 = 0.8621160089969635
Training loss on iteration 1060 = 0.941397400200367
Training loss on iteration 1080 = 0.9791283667087555
Training loss on iteration 1100 = 0.6251292489469051
Training loss on iteration 1120 = 0.8217789113521576
Training loss on iteration 1140 = 0.8802056729793548
Training loss on iteration 1160 = 0.8821555987000466
Training loss on iteration 1180 = 0.8978189036250115
Training loss on iteration 1200 = 0.7990004539489746
Training loss on iteration 1220 = 0.7651166766881943
Training loss on iteration 1240 = 0.8262374252080917
Training loss on iteration 1260 = 0.9402598783373832
Training loss on iteration 1280 = 1.0206795066595078
Training loss on iteration 1300 = 0.8461096122860908
Training loss on iteration 1320 = 0.7238685734570026
Training loss on iteration 1340 = 0.9073074422776699
Training loss on iteration 1360 = 0.9684065610170365
Training loss on iteration 1380 = 0.8575759157538414
Training loss on iteration 1400 = 0.854609589278698
Training loss on iteration 1420 = 0.7793288722634315
Training loss on iteration 1440 = 0.8463522985577583
Training loss on iteration 1460 = 0.909835098683834
Training loss on iteration 1480 = 0.9691390484571457
Training loss on iteration 1500 = 0.8122700542211533
Training loss on iteration 1520 = 1.0236977115273476
Training loss on iteration 1540 = 0.944523399323225
Training loss on iteration 1560 = 0.8406158775091171
Training loss on iteration 1580 = 0.7900555759668351
Training loss on iteration 1600 = 1.0969944432377816
Training loss on iteration 1620 = 0.9560091987252235
Training loss on iteration 1640 = 0.9002655372023582
Training loss on iteration 1660 = 0.653025783598423
Training loss on iteration 1680 = 0.8231277838349342
Training loss on iteration 1700 = 1.0691247761249543
Training loss on iteration 1720 = 0.8070332676172256
Training loss on iteration 1740 = 0.9211574092507362
Training loss on iteration 1760 = 0.8850175410509109
Training loss on iteration 1780 = 1.0656716018915176
Training loss on iteration 1800 = 0.8457785055041314
Training loss on iteration 1820 = 0.7985480159521103
Training loss on iteration 1840 = 0.8486790537834168
Training loss on iteration 1860 = 0.7565920256078243
Training loss on iteration 1880 = 0.7893074706196785
Training loss on iteration 1900 = 0.7961354061961174
Training loss on iteration 1920 = 0.8208736598491668
Training loss on iteration 1940 = 0.915955913066864
Training loss on iteration 1960 = 0.909826235473156
Training loss on iteration 1980 = 0.7893531635403633
Training loss on iteration 2000 = 1.7493914395570755
Training loss on iteration 2020 = 1.2094793349504471
Training loss on iteration 2040 = 1.140929889678955
Training loss on iteration 2060 = 0.8299705594778061
Training loss on iteration 2080 = 0.955445122718811
Training loss on iteration 2100 = 0.9947000414133071
Training loss on iteration 2120 = 0.9655806884169579
Training loss on iteration 2140 = 1.2038586482405662
Training loss on iteration 2160 = 1.0095975175499916
Training loss on iteration 2180 = 1.275187823176384
Training loss on iteration 2200 = 0.9871528834104538
Training loss on iteration 2220 = 1.0245712041854858
Training loss on iteration 2240 = 0.8537034377455711
Training loss on iteration 2260 = 0.7595359787344933
Training loss on iteration 2280 = 0.730553637444973
Training loss on iteration 2300 = 0.6699416905641555
Training loss on iteration 2320 = 1.3170728042721749
Training loss on iteration 2340 = 0.9886771708726882
Training loss on iteration 2360 = 1.0700793087482452
Training loss on iteration 2380 = 0.9065142259001732
Training loss on iteration 2400 = 0.9830307886004448
Training loss on iteration 2420 = 0.945906876027584
Training loss on iteration 2440 = 0.9597655817866325
Training loss on iteration 2460 = 0.7760681360960007
Training loss on iteration 2480 = 0.7697654113173484
Training loss on iteration 2500 = 0.9908600583672523
Training loss on iteration 2520 = 0.9196899324655533
Training loss on iteration 2540 = 0.9274114221334457
Training loss on iteration 2560 = 0.8570547699928284
Training loss on iteration 2580 = 0.778430700302124
Training loss on iteration 2600 = 0.8551410779356956
Training loss on iteration 2620 = 0.7790813356637954
Training loss on iteration 2640 = 0.782289981842041
Training loss on iteration 2660 = 1.0618669852614402
Training loss on iteration 2680 = 0.9243225574493408
Training loss on iteration 2700 = 0.9279074296355247
Training loss on iteration 2720 = 0.9549448400735855
Training loss on iteration 2740 = 0.9046051904559136
Training loss on iteration 2760 = 0.9393724888563156
Training loss on iteration 2780 = 0.837373249232769
Training loss on iteration 2800 = 1.047525192797184
Training loss on iteration 2820 = 1.2298233225941657
Training loss on iteration 2840 = 0.948953877389431
Training loss on iteration 2860 = 0.8685176879167557
Training loss on iteration 2880 = 0.854683269560337
Training loss on iteration 2900 = 0.6948413863778115
Training loss on iteration 2920 = 0.9428917273879052
Training loss on iteration 2940 = 0.9015521973371505
Training loss on iteration 2960 = 0.73987468034029
Training loss on iteration 2980 = 0.7400684639811516
Training loss on iteration 3000 = 0.6456656113266945
Training loss on iteration 3020 = 0.7767810225486755
Training loss on iteration 3040 = 0.7847606271505356
Training loss on iteration 3060 = 0.9838306039571763
Training loss on iteration 3080 = 0.936838586628437
Training loss on iteration 3100 = 0.6396599486470222
Training loss on iteration 3120 = 0.8839695826172829
Training loss on iteration 3140 = 0.7800677545368672
Training loss on iteration 3160 = 0.7694599732756615
Training loss on iteration 3180 = 0.743441042304039
Training loss on iteration 3200 = 0.862903481721878
Training loss on iteration 3220 = 0.8500754162669182
Training loss on iteration 3240 = 0.9396347463130951
Training loss on iteration 3260 = 0.890435516834259
Training loss on iteration 3280 = 0.9619816616177559
Training loss on iteration 3300 = 10.037997043132782
Training loss on iteration 3320 = 1.548406943678856
Training loss on iteration 3340 = 1.30584674179554
Training loss on iteration 3360 = 1.2522824704647064
Training loss on iteration 3380 = 1.3584001898765563
Training loss on iteration 3400 = 1.3053907871246337
Training loss on iteration 3420 = 16.184373864531516
Training loss on iteration 3440 = 2.637687599658966
Training loss on iteration 3460 = 1.6564650893211366
Training loss on iteration 3480 = 3.396571859717369
Training loss on iteration 3500 = 2.5875521421432497
Training loss on iteration 3520 = 1.2469298452138902
Training loss on iteration 3540 = 1.3338715344667436
Training loss on iteration 3560 = 1.243245705962181
Training loss on iteration 3580 = 1.1067897707223893
Training loss on iteration 3600 = 1.0725967049598695
Training loss on iteration 3620 = 0.9338531136512757
Training loss on iteration 3640 = 0.9325873240828514
Training loss on iteration 3660 = 0.8740881234407425
Training loss on iteration 3680 = 0.972537562251091
Training loss on iteration 3700 = 0.8725273996591568
Training loss on iteration 3720 = 0.8742152616381645
Training loss on iteration 3740 = 0.8842738643288612
Training loss on iteration 3760 = 1.209218515455723
Training loss on iteration 3780 = 0.8177170261740685
Training loss on iteration 3800 = 1.170042024552822
Training loss on iteration 3820 = 0.792470508813858
Training loss on iteration 3840 = 1.1587097600102425
Training loss on iteration 3860 = 1.0092089012265206
Training loss on iteration 3880 = 0.8440968841314316
Training loss on iteration 3900 = 1.2088200181722641
Training loss on iteration 3920 = 0.9831208810210228
Training loss on iteration 3940 = 0.8622120335698128
Training loss on iteration 3960 = 0.8368520408868789
Training loss on iteration 3980 = 0.8729664742946625
Training loss on iteration 4000 = 0.8809322133660317
Training loss on iteration 4020 = 0.9463255792856217
Training loss on iteration 4040 = 1.1242333590984344
Training loss on iteration 4060 = 0.9413751944899559
Training loss on iteration 4080 = 0.9748432323336601
Training loss on iteration 4100 = 0.7915988057851792
Training loss on iteration 4120 = 0.8673601076006889
Training loss on iteration 4140 = 1.0677621588110924
Training loss on iteration 4160 = 1.0499053865671157
Training loss on iteration 4180 = 0.9686912566423416
Training loss on iteration 4200 = 1.0079136341810226
Training loss on iteration 4220 = 1.0170533180236816
Training loss on iteration 4240 = 0.8669334143400192
Training loss on iteration 4260 = 0.8329033419489861
Training loss on iteration 4280 = 0.7495598316192627
Training loss on iteration 4300 = 0.8255692675709725
Training loss on iteration 4320 = 0.861471152305603
Training loss on iteration 4340 = 0.7518523544073105
Training loss on iteration 4360 = 0.6910844199359417
Training loss on iteration 4380 = 0.6706038847565651
Training loss on iteration 4400 = 0.8080430701375008
Training loss on iteration 4420 = 0.7733535416424274
Training loss on iteration 4440 = 0.8984724134206772
Training loss on iteration 4460 = 0.9307423159480095
Training loss on iteration 4480 = 0.7559093967080116
Training loss on iteration 4500 = 0.7321775853633881
Training loss on iteration 4520 = 0.7554819941520691
Training loss on iteration 4540 = 0.8761858358979225
Training loss on iteration 4560 = 0.7983808070421219
Training loss on iteration 4580 = 0.6852741435170173
Training loss on iteration 4600 = 0.7409290507435798
Training loss on iteration 4620 = 0.7993324190378189
Training loss on iteration 4640 = 0.9475666522979737
Training loss on iteration 4660 = 0.9992250666022301
Training loss on iteration 4680 = 1.041208240389824
Training loss on iteration 4700 = 0.9806553661823273
Training loss on iteration 4720 = 0.6856408849358558
Training loss on iteration 4740 = 0.8495249211788177
Training loss on iteration 4760 = 0.9605086624622345
Training loss on iteration 4780 = 0.9271904841065407
Training loss on iteration 4800 = 0.845744389295578
Training loss on iteration 4820 = 0.7018534630537033
Training loss on iteration 4840 = 0.7643770039081573
Training loss on iteration 4860 = 0.9272820621728897
Training loss on iteration 4880 = 0.9609492465853691
Training loss on iteration 4900 = 0.8438037350773812
Training loss on iteration 4920 = 0.5725683495402336
Training loss on iteration 4940 = 1.051442588865757
Training loss on iteration 4960 = 0.7064289897680283
Training loss on iteration 4980 = 1.0157305404543877
Training loss on iteration 5000 = 0.8953553512692451
Training loss on iteration 5020 = 0.886441396176815
Training loss on iteration 5040 = 0.9285719349980355
Training loss on iteration 5060 = 0.7805845305323601
Training loss on iteration 5080 = 0.7352708652615547
Training loss on iteration 5100 = 0.8094991743564606
Training loss on iteration 5120 = 0.9467872172594071
Training loss on iteration 5140 = 0.919763907790184
Training loss on iteration 5160 = 0.805605462193489
Training loss on iteration 5180 = 0.8803218334913254
Training loss on iteration 5200 = 0.8789184883236885
Training loss on iteration 5220 = 1.0683533400297165
Training loss on iteration 5240 = 0.9088984817266464
Training loss on iteration 5260 = 0.6873793467879296
Training loss on iteration 5280 = 0.721776208281517
Training loss on iteration 5300 = 1.1072995483875274
Training loss on iteration 5320 = 0.7256583064794541
Training loss on iteration 5340 = 0.905501788854599
Training loss on iteration 5360 = 0.7698004230856895
Training loss on iteration 5380 = 0.8776487231254577
Training loss on iteration 5400 = 0.8342058688402176
Training loss on iteration 5420 = 0.8702408954501152
Training loss on iteration 5440 = 1.0034215152263641
Training loss on iteration 5460 = 0.6498523682355881
Training loss on iteration 5480 = 1.121949101984501
Training loss on iteration 5500 = 1.0885173395276069
Training loss on iteration 5520 = 0.9108329012989997
Training loss on iteration 5540 = 0.6995623514056206
Training loss on iteration 5560 = 0.7753305248916149
Training loss on iteration 5580 = 0.9495792701840401
Training loss on iteration 5600 = 0.8128115095198154
Training loss on iteration 5620 = 0.7090612024068832
Training loss on iteration 5640 = 0.8573587357997894
Training loss on iteration 5660 = 0.8694104567170143
Training loss on iteration 5680 = 0.8813665762543679
Training loss on iteration 5700 = 0.8410579949617386
Training loss on iteration 5720 = 0.8102420270442963
Training loss on iteration 5740 = 0.715602008998394
Training loss on iteration 5760 = 0.8249476701021194
Training loss on iteration 5780 = 0.6069289073348045
Training loss on iteration 5800 = 0.8965759903192521
Training loss on iteration 5820 = 0.9285864859819413
Training loss on iteration 5840 = 0.8171349495649338
Training loss on iteration 5860 = 0.8078747540712357
Training loss on iteration 5880 = 0.7358260437846184
Training loss on iteration 5900 = 0.6200995579361915
Training loss on iteration 5920 = 0.7387112207710743
Training loss on iteration 5940 = 0.7419421806931495
Training loss on iteration 5960 = 0.8732232168316841
Training loss on iteration 5980 = 0.9004431709647178
Training loss on iteration 6000 = 0.9337010130286216
Training loss on iteration 6020 = 0.9188556477427483
Training loss on iteration 6040 = 0.6563853308558464
Training loss on iteration 6060 = 0.679717256128788
Training loss on iteration 6080 = 0.9968001127243042
Training loss on iteration 6100 = 1.0536527022719384
Training loss on iteration 6120 = 0.7078919783234596
Training loss on iteration 6140 = 0.7577184945344925
Training loss on iteration 6160 = 0.6986554443836213
Training loss on iteration 6180 = 0.8074074015021324
Training loss on iteration 6200 = 0.9103890553116798
Training loss on iteration 6220 = 0.6414251908659935
Training loss on iteration 6240 = 0.8351848617196083
Training loss on iteration 6260 = 0.8214927591383457
Training loss on iteration 6280 = 0.5748906150460243
Training loss on iteration 6300 = 0.8768734037876129
Training loss on iteration 6320 = 0.7462536126375199
Training loss on iteration 6340 = 0.914959841966629
Training loss on iteration 6360 = 0.7610205411911011
Training loss on iteration 6380 = 0.8393809452652932
Training loss on iteration 0 = 0.28653720021247864
Training loss on iteration 20 = 0.7562722310423851
Training loss on iteration 40 = 0.8828401863574982
Training loss on iteration 60 = 0.9023231491446495
Training loss on iteration 80 = 0.8665390655398368
Training loss on iteration 100 = 0.9931432798504829
Training loss on iteration 120 = 0.7728402063250541
Training loss on iteration 140 = 0.8546537414193154
Training loss on iteration 160 = 0.8326493412256241
Training loss on iteration 180 = 0.7379927918314934
Training loss on iteration 200 = 0.6981289491057396
Training loss on iteration 220 = 0.8770565956830978
Training loss on iteration 240 = 0.8067154482007026
Training loss on iteration 260 = 0.7564410373568535
Training loss on iteration 280 = 0.7470821619033814
Training loss on iteration 300 = 0.7319795355200768
Training loss on iteration 320 = 0.8271816805005073
Training loss on iteration 340 = 0.7418271034955979
Training loss on iteration 360 = 0.6289542429149151
Training loss on iteration 380 = 0.8785145580768585
Training loss on iteration 400 = 0.7272137172520161
Training loss on iteration 420 = 0.748541533946991
Training loss on iteration 440 = 0.6437859669327736
Training loss on iteration 460 = 0.6989418938755989
Training loss on iteration 480 = 0.8396142333745956
Training loss on iteration 500 = 0.8989002346992493
Training loss on iteration 520 = 0.6245441175997257
Training loss on iteration 540 = 0.7488242492079735
Training loss on iteration 560 = 0.8499428942799568
Training loss on iteration 580 = 0.7210557326674462
Training loss on iteration 600 = 0.7406427025794983
Training loss on iteration 620 = 0.7016845658421517
Training loss on iteration 640 = 0.7841962546110153
Training loss on iteration 660 = 0.7906382821500302
Training loss on iteration 680 = 0.7797750405967235
Training loss on iteration 700 = 0.753558249771595
Training loss on iteration 720 = 0.9170109838247299
Training loss on iteration 740 = 0.7555118516087532
Training loss on iteration 760 = 0.8708998203277588
Training loss on iteration 780 = 0.9115625873208046
Training loss on iteration 800 = 0.9580344513058663
Training loss on iteration 820 = 0.7669462084770202
Training loss on iteration 840 = 0.8434729292988777
Training loss on iteration 860 = 0.7322319619357586
Training loss on iteration 880 = 0.7140580639243126
Training loss on iteration 900 = 0.6337746188044548
Training loss on iteration 920 = 0.6788512751460075
Training loss on iteration 940 = 0.7289599820971489
Training loss on iteration 960 = 0.7066935762763024
Training loss on iteration 980 = 0.7016259461641312
Training loss on iteration 1000 = 0.8393418349325656
Training loss on iteration 1020 = 0.7833738967776298
Training loss on iteration 1040 = 0.8013106241822243
Training loss on iteration 1060 = 0.6706734403967858
Training loss on iteration 1080 = 0.6934038162231445
Training loss on iteration 1100 = 0.7384726062417031
Training loss on iteration 1120 = 0.9520107761025429
Training loss on iteration 1140 = 0.7404628425836564
Training loss on iteration 1160 = 0.7331719309091568
Training loss on iteration 1180 = 0.6364550501108169
Training loss on iteration 1200 = 0.891556841135025
Training loss on iteration 1220 = 0.8577095985412597
Training loss on iteration 1240 = 0.8829851031303406
Training loss on iteration 1260 = 0.7270073398947716
Training loss on iteration 1280 = 0.7927892133593559
Training loss on iteration 1300 = 0.7833180531859398
Training loss on iteration 1320 = 0.8908588483929634
Training loss on iteration 1340 = 0.7449565097689629
Training loss on iteration 1360 = 0.7891022771596908
Training loss on iteration 1380 = 0.8725436076521873
Training loss on iteration 1400 = 0.7284478276968003
Training loss on iteration 1420 = 0.5845396637916564
Training loss on iteration 1440 = 0.7217057809233666
Training loss on iteration 1460 = 0.6858009822666645
Training loss on iteration 1480 = 0.8740165278315544
Training loss on iteration 1500 = 0.6507294066250324
Training loss on iteration 1520 = 0.8075414627790451
Training loss on iteration 1540 = 0.8472963631153106
Training loss on iteration 1560 = 0.9177120119333267
Training loss on iteration 1580 = 0.7162482440471649
Training loss on iteration 1600 = 0.7608071237802505
Training loss on iteration 1620 = 0.8426153093576432
Training loss on iteration 1640 = 0.9020349934697152
Training loss on iteration 1660 = 0.6850788310170174
Training loss on iteration 1680 = 0.9908224016427993
Training loss on iteration 1700 = 0.8300837844610214
Training loss on iteration 1720 = 0.7139560848474502
Training loss on iteration 1740 = 0.6819744244217872
Training loss on iteration 1760 = 0.9053709290921688
Training loss on iteration 1780 = 0.821060673892498
Training loss on iteration 1800 = 0.7538405016064644
Training loss on iteration 1820 = 0.8674051061272621
Training loss on iteration 1840 = 0.8236464470624923
Training loss on iteration 1860 = 0.6948171421885491
Training loss on iteration 1880 = 0.615605442225933
Training loss on iteration 1900 = 0.8017546474933624
Training loss on iteration 1920 = 0.6299244582653045
Training loss on iteration 1940 = 0.7673005580902099
Training loss on iteration 1960 = 0.9389820292592048
Training loss on iteration 1980 = 0.7785451442003251
Training loss on iteration 2000 = 0.9517163962125779
Training loss on iteration 2020 = 0.9245666995644569
Training loss on iteration 2040 = 0.9033458963036537
Training loss on iteration 2060 = 0.7482324004173279
Training loss on iteration 2080 = 0.8648006424307824
Training loss on iteration 2100 = 0.7790274828672409
Training loss on iteration 2120 = 0.7291521184146404
Training loss on iteration 2140 = 0.755339677631855
Training loss on iteration 2160 = 0.7429844886064529
Training loss on iteration 2180 = 0.6482542432844639
Training loss on iteration 2200 = 0.5970536902546882
Training loss on iteration 2220 = 0.7686282955110073
Training loss on iteration 2240 = 0.9241497918963433
Training loss on iteration 2260 = 0.678238944709301
Training loss on iteration 2280 = 0.7041623383760452
Training loss on iteration 2300 = 0.7093491464853287
Training loss on iteration 2320 = 0.8835701704025268
Training loss on iteration 2340 = 0.7715572863817215
Training loss on iteration 2360 = 0.6768452197313308
Training loss on iteration 2380 = 0.5948593035340309
Training loss on iteration 2400 = 1.0187000885605813
Training loss on iteration 2420 = 0.9661626353859901
Training loss on iteration 2440 = 0.7845274902880192
Training loss on iteration 2460 = 0.8705247119069099
Training loss on iteration 2480 = 0.7586363166570663
Training loss on iteration 2500 = 0.7402195677161216
Training loss on iteration 2520 = 0.7419840872287751
Training loss on iteration 2540 = 0.7239796802401542
Training loss on iteration 2560 = 0.9967650949954987
Training loss on iteration 2580 = 0.9325148016214371
Training loss on iteration 2600 = 0.6432852327823639
Training loss on iteration 2620 = 0.7149045795202256
Training loss on iteration 2640 = 0.8038982778787613
Training loss on iteration 2660 = 0.7281690135598182
Training loss on iteration 2680 = 0.7013226717710495
Training loss on iteration 2700 = 0.7475591793656349
Training loss on iteration 2720 = 0.7059337809681893
Training loss on iteration 2740 = 0.8076627239584923
Training loss on iteration 2760 = 1.0077693596482278
Training loss on iteration 2780 = 0.9056558728218078
Training loss on iteration 2800 = 0.7161485642194748
Training loss on iteration 2820 = 0.7767885342240334
Training loss on iteration 2840 = 0.98992919921875
Training loss on iteration 2860 = 1.0148524165153503
Training loss on iteration 2880 = 0.8593433618545532
Training loss on iteration 2900 = 0.8860404476523399
Training loss on iteration 2920 = 0.7862836420536041
Training loss on iteration 2940 = 0.7126878157258034
Training loss on iteration 2960 = 0.83764638453722
Training loss on iteration 2980 = 0.682960969209671
Training loss on iteration 3000 = 0.7055869951844216
Training loss on iteration 3020 = 0.7509502544999123
Training loss on iteration 3040 = 0.8301693573594093
Training loss on iteration 3060 = 0.7590885907411575
Training loss on iteration 3080 = 0.8695871725678443
Training loss on iteration 3100 = 0.6841818630695343
Training loss on iteration 3120 = 0.8959638804197312
Training loss on iteration 3140 = 0.9103513017296792
Training loss on iteration 3160 = 0.8044493496417999
Training loss on iteration 3180 = 0.9827226281166077
Training loss on iteration 3200 = 0.8504929021000862
Training loss on iteration 3220 = 0.8910031601786613
Training loss on iteration 3240 = 0.8229188695549965
Training loss on iteration 3260 = 0.732198166847229
Training loss on iteration 3280 = 0.8002717211842537
Training loss on iteration 3300 = 1.0550339952111245
Training loss on iteration 3320 = 0.9067815482616425
Training loss on iteration 3340 = 0.7854566588997841
Training loss on iteration 3360 = 0.7406901195645332
Training loss on iteration 3380 = 0.9600326329469681
Training loss on iteration 3400 = 0.7340448573231697
Training loss on iteration 3420 = 0.7160879641771316
Training loss on iteration 3440 = 0.7729171961545944
Training loss on iteration 3460 = 0.9446518152952195
Training loss on iteration 3480 = 0.9266548797488212
Training loss on iteration 3500 = 0.935644119977951
Training loss on iteration 3520 = 0.8250609934329987
Training loss on iteration 3540 = 0.8477891281247139
Training loss on iteration 3560 = 0.6219164550304412
Training loss on iteration 3580 = 0.7313492938876152
Training loss on iteration 3600 = 0.8725588247179985
Training loss on iteration 3620 = 0.8309699565172195
Training loss on iteration 3640 = 0.9211286246776581
Training loss on iteration 3660 = 0.9193928360939025
Training loss on iteration 3680 = 0.8375210858881473
Training loss on iteration 3700 = 0.8884517967700958
Training loss on iteration 3720 = 0.6205029934644699
Training loss on iteration 3740 = 0.775632557272911
Training loss on iteration 3760 = 0.7828206136822701
Training loss on iteration 3780 = 0.5998148426413537
Training loss on iteration 3800 = 0.5149172239005566
Training loss on iteration 3820 = 0.6853991940617561
Training loss on iteration 3840 = 0.7066024348139763
Training loss on iteration 3860 = 0.6386881589889526
Training loss on iteration 3880 = 0.6105835273861885
Training loss on iteration 3900 = 0.7206646651029587
Training loss on iteration 3920 = 0.6232496872544289
Training loss on iteration 3940 = 0.7356271743774414
Training loss on iteration 3960 = 0.7307782799005509
Training loss on iteration 3980 = 0.8103216148912906
Training loss on iteration 4000 = 0.7989793613553047
Training loss on iteration 4020 = 0.8478630468249321
Training loss on iteration 4040 = 1.0007335498929024
Training loss on iteration 4060 = 0.7470904409885406
Training loss on iteration 4080 = 0.6047231271862984
Training loss on iteration 4100 = 1.2211121767759323
Training loss on iteration 4120 = 0.8079301536083221
Training loss on iteration 4140 = 0.7347023770213127
Training loss on iteration 4160 = 0.7413234516978264
Training loss on iteration 4180 = 0.7108304336667061
Training loss on iteration 4200 = 0.994781382381916
Training loss on iteration 4220 = 0.7253055900335312
Training loss on iteration 4240 = 0.6800879135727882
Training loss on iteration 4260 = 0.5258480802178382
Training loss on iteration 4280 = 0.6707098394632339
Training loss on iteration 4300 = 0.7715601280331612
Training loss on iteration 4320 = 0.8487779915332794
Training loss on iteration 4340 = 0.8437761917710305
Training loss on iteration 4360 = 0.6538032114505767
Training loss on iteration 4380 = 0.7455789983272553
Training loss on iteration 4400 = 0.8419189944863319
Training loss on iteration 4420 = 0.8478389039635659
Training loss on iteration 4440 = 0.8825175434350967
Training loss on iteration 4460 = 0.7780556380748749
Training loss on iteration 4480 = 0.7763261124491692
Training loss on iteration 4500 = 0.8490524679422379
Training loss on iteration 4520 = 0.6643975242972374
Training loss on iteration 4540 = 0.7544248923659325
Training loss on iteration 4560 = 0.6577413395047188
Training loss on iteration 4580 = 0.6796056434512139
Training loss on iteration 4600 = 0.6513650178909302
Training loss on iteration 4620 = 0.9615856125950814
Training loss on iteration 4640 = 0.8031833723187447
Training loss on iteration 4660 = 0.8337836995720863
Training loss on iteration 4680 = 0.9157872512936592
Training loss on iteration 4700 = 0.7574827924370766
Training loss on iteration 4720 = 0.7127272024750709
Training loss on iteration 4740 = 1.0884360030293465
Training loss on iteration 4760 = 0.6451416477560997
Training loss on iteration 4780 = 0.7127330899238586
Training loss on iteration 4800 = 0.6157945245504379
Training loss on iteration 4820 = 0.7478717371821404
Training loss on iteration 4840 = 0.6703539125621318
Training loss on iteration 4860 = 0.653559074550867
Training loss on iteration 4880 = 0.8501600340008736
Training loss on iteration 4900 = 0.786456523835659
Training loss on iteration 4920 = 0.7052678376436233
Training loss on iteration 4940 = 0.8960238292813301
Training loss on iteration 4960 = 0.7768809527158738
Training loss on iteration 4980 = 0.706805981695652
Training loss on iteration 5000 = 0.7305968299508094
Training loss on iteration 5020 = 0.7917038574814796
Training loss on iteration 5040 = 0.6886607930064201
Training loss on iteration 5060 = 0.7234278365969657
Training loss on iteration 5080 = 0.6191055580973626
Training loss on iteration 5100 = 0.6627776741981506
Training loss on iteration 5120 = 0.6054991267621517
Training loss on iteration 5140 = 0.8031684637069703
Training loss on iteration 5160 = 0.6936538547277451
Training loss on iteration 5180 = 0.8056697219610214
Training loss on iteration 5200 = 0.623380246758461
Training loss on iteration 5220 = 0.7417526736855506
Training loss on iteration 5240 = 0.7941828936338424
Training loss on iteration 5260 = 0.7540450409054756
Training loss on iteration 5280 = 0.8589237779378891
Training loss on iteration 5300 = 0.8198785096406936
Training loss on iteration 5320 = 0.7668640494346619
Training loss on iteration 5340 = 0.7434112399816513
Training loss on iteration 5360 = 0.6978880405426026
Training loss on iteration 5380 = 0.7693005666136742
Training loss on iteration 5400 = 0.7553268805146217
Training loss on iteration 5420 = 0.8090854234993458
Training loss on iteration 5440 = 0.706795385479927
Training loss on iteration 5460 = 0.7586398631334305
Training loss on iteration 5480 = 0.7772436663508415
Training loss on iteration 5500 = 0.727040083706379
Training loss on iteration 5520 = 0.7695127964019776
Training loss on iteration 5540 = 0.9770386129617691
Training loss on iteration 5560 = 0.8596609920263291
Training loss on iteration 5580 = 0.7438188776373863
Training loss on iteration 5600 = 0.9758741438388825
Training loss on iteration 5620 = 0.8138888984918594
Training loss on iteration 5640 = 0.7403232127428054
Training loss on iteration 5660 = 0.6682359397411346
Training loss on iteration 5680 = 0.6809422105550766
Training loss on iteration 5700 = 0.7161342784762382
Training loss on iteration 5720 = 0.7685739129781723
Training loss on iteration 5740 = 0.7584603205323219
Training loss on iteration 5760 = 0.9222463518381119
Training loss on iteration 5780 = 0.757181353867054
Training loss on iteration 5800 = 0.7102648839354515
Training loss on iteration 5820 = 0.6912698782980442
Training loss on iteration 5840 = 0.899566014111042
Training loss on iteration 5860 = 1.001263739913702
Training loss on iteration 5880 = 0.7479697048664093
Training loss on iteration 5900 = 0.8017508536577225
Training loss on iteration 5920 = 1.041331335902214
Training loss on iteration 5940 = 1.0149789527058601
Training loss on iteration 5960 = 0.8116077989339828
Training loss on iteration 5980 = 0.8610712975263596
Training loss on iteration 6000 = 0.7757984876632691
Training loss on iteration 6020 = 0.7260385632514954
Training loss on iteration 6040 = 0.7495080962777138
Training loss on iteration 6060 = 0.8022606670856476
Training loss on iteration 6080 = 0.816464301943779
Training loss on iteration 6100 = 0.812808384001255
Training loss on iteration 6120 = 0.6909500226378441
Training loss on iteration 6140 = 0.8831969976425171
Training loss on iteration 6160 = 0.7069046504795551
Training loss on iteration 6180 = 0.9634266793727875
Training loss on iteration 6200 = 0.8764305949211121
Training loss on iteration 6220 = 0.747705128788948
Training loss on iteration 6240 = 0.7123484596610069
Training loss on iteration 6260 = 0.7288084551692009
Training loss on iteration 6280 = 0.8240965217351913
Training loss on iteration 6300 = 0.8106918826699256
Training loss on iteration 6320 = 0.7483225464820862
Training loss on iteration 6340 = 0.7773999013006687
Training loss on iteration 6360 = 0.7939691841602325
Training loss on iteration 6380 = 0.8467949524521827
Training loss on iteration 0 = 0.6383668780326843
Training loss on iteration 20 = 0.5430017732083797
Training loss on iteration 40 = 0.7049713239073754
Training loss on iteration 60 = 0.706626957654953
Training loss on iteration 80 = 0.5967672973871231
Training loss on iteration 100 = 0.7331972941756248
Training loss on iteration 120 = 0.6705871894955635
Training loss on iteration 140 = 0.6058130979537963
Training loss on iteration 160 = 1.8838284000754357
Training loss on iteration 180 = 1.0101410053670405
Training loss on iteration 200 = 0.9739760860800744
Training loss on iteration 220 = 1.0186654552817345
Training loss on iteration 240 = 1.018130724132061
Training loss on iteration 260 = 0.7711299359798431
Training loss on iteration 280 = 1.0375397264957429
Training loss on iteration 300 = 0.8570867732167244
Training loss on iteration 320 = 0.7347046747803688
Training loss on iteration 340 = 0.782865522801876
Training loss on iteration 360 = 1.0689279049634934
Training loss on iteration 380 = 0.8237065687775612
Training loss on iteration 400 = 0.7304752171039581
Training loss on iteration 420 = 0.6703141421079636
Training loss on iteration 440 = 0.7054235428571701
Training loss on iteration 460 = 0.7740063011646271
Training loss on iteration 480 = 0.8173760548233986
Training loss on iteration 500 = 0.7873517066240311
Training loss on iteration 520 = 0.7257979333400726
Training loss on iteration 540 = 0.7066938728094101
Training loss on iteration 560 = 0.7288443848490715
Training loss on iteration 580 = 0.6492938436567783
Training loss on iteration 600 = 0.8441294685006142
Training loss on iteration 620 = 0.6751985102891922
Training loss on iteration 640 = 0.8182827875018119
Training loss on iteration 660 = 0.6538987502455711
Training loss on iteration 680 = 0.7697570770978928
Training loss on iteration 700 = 0.6603077173233032
Training loss on iteration 720 = 0.7322150111198426
Training loss on iteration 740 = 0.6865343272686004
Training loss on iteration 760 = 0.8542560912668705
Training loss on iteration 780 = 0.6783164411783218
Training loss on iteration 800 = 0.7428334221243859
Training loss on iteration 820 = 0.8899212494492531
Training loss on iteration 840 = 0.7667693674564362
Training loss on iteration 860 = 0.7775977119803429
Training loss on iteration 880 = 0.852906558662653
Training loss on iteration 900 = 0.6819657862186432
Training loss on iteration 920 = 1.0186642825603485
Training loss on iteration 940 = 0.7716228350996971
Training loss on iteration 960 = 0.7475080594420433
Training loss on iteration 980 = 0.7463543951511383
Training loss on iteration 1000 = 0.7293111085891724
Training loss on iteration 1020 = 0.7863103948533535
Training loss on iteration 1040 = 0.7161972306668758
Training loss on iteration 1060 = 0.7358337372541428
Training loss on iteration 1080 = 0.6192306362092495
Training loss on iteration 1100 = 0.7287754520773888
Training loss on iteration 1120 = 0.7600620031356812
Training loss on iteration 1140 = 0.7166349440813065
Training loss on iteration 1160 = 0.6196987748146057
Training loss on iteration 1180 = 0.6982620805501938
Training loss on iteration 1200 = 0.6913088411092758
Training loss on iteration 1220 = 0.8068636611104012
Training loss on iteration 1240 = 0.8074771478772164
Training loss on iteration 1260 = 0.7795688547194004
Training loss on iteration 1280 = 0.6732316628098488
Training loss on iteration 1300 = 0.5335140466690064
Training loss on iteration 1320 = 0.5902762033045292
Training loss on iteration 1340 = 0.8842457383871078
Training loss on iteration 1360 = 0.7755601018667221
Training loss on iteration 1380 = 0.8245355308055877
Training loss on iteration 1400 = 0.6501460373401642
Training loss on iteration 1420 = 0.8309148333966732
Training loss on iteration 1440 = 0.7015797376632691
Training loss on iteration 1460 = 0.7382204338908196
Training loss on iteration 1480 = 0.6351981461048126
Training loss on iteration 1500 = 0.7245828673243523
Training loss on iteration 1520 = 0.9768890410661697
Training loss on iteration 1540 = 0.7132397249341011
Training loss on iteration 1560 = 0.7145240172743798
Training loss on iteration 1580 = 0.7575374394655228
Training loss on iteration 1600 = 0.7921413376927375
Training loss on iteration 1620 = 0.7035090014338493
Training loss on iteration 1640 = 0.6540128216147423
Training loss on iteration 1660 = 0.7858239561319351
Training loss on iteration 1680 = 0.8758959770202637
Training loss on iteration 1700 = 0.8561809904873371
Training loss on iteration 1720 = 0.865990686416626
Training loss on iteration 1740 = 0.7770618692040443
Training loss on iteration 1760 = 0.6602038249373436
Training loss on iteration 1780 = 0.6109465748071671
Training loss on iteration 1800 = 0.8125684723258019
Training loss on iteration 1820 = 0.7743689619004727
Training loss on iteration 1840 = 0.6251321285963058
Training loss on iteration 1860 = 0.857044929265976
Training loss on iteration 1880 = 0.8889825016260147
Training loss on iteration 1900 = 0.6471554428339005
Training loss on iteration 1920 = 0.8827594071626663
Training loss on iteration 1940 = 0.700681158900261
Training loss on iteration 1960 = 0.7759413719177246
Training loss on iteration 1980 = 0.8743492230772972
Training loss on iteration 2000 = 0.7734526336193085
Training loss on iteration 2020 = 0.6077166318893432
Training loss on iteration 2040 = 0.8154030278325081
Training loss on iteration 2060 = 0.7180087089538574
Training loss on iteration 2080 = 0.6553624078631402
Training loss on iteration 2100 = 0.8215108722448349
Training loss on iteration 2120 = 0.8000066399574279
Training loss on iteration 2140 = 1.1141361966729164
Training loss on iteration 2160 = 0.6969229608774186
Training loss on iteration 2180 = 0.6252070084214211
Training loss on iteration 2200 = 0.6148722156882286
Training loss on iteration 2220 = 0.7338183358311653
Training loss on iteration 2240 = 0.7344885528087616
Training loss on iteration 2260 = 0.6622959777712822
Training loss on iteration 2280 = 0.6969411313533783
Training loss on iteration 2300 = 0.733201676607132
Training loss on iteration 2320 = 0.6383672833442688
Training loss on iteration 2340 = 0.7638922423124314
Training loss on iteration 2360 = 0.7086066424846649
Training loss on iteration 2380 = 0.9280199319124222
Training loss on iteration 2400 = 0.6820380762219429
Training loss on iteration 2420 = 0.6198481038212776
Training loss on iteration 2440 = 0.816236300766468
Training loss on iteration 2460 = 0.5911729916930198
Training loss on iteration 2480 = 0.8563204482197762
Training loss on iteration 2500 = 0.6656795211136342
Training loss on iteration 2520 = 0.6720164835453033
Training loss on iteration 2540 = 0.9457633122801781
Training loss on iteration 2560 = 0.666684353351593
Training loss on iteration 2580 = 0.6385906353592873
Training loss on iteration 2600 = 0.713636277616024
Training loss on iteration 2620 = 0.6996477648615838
Training loss on iteration 2640 = 0.7229430869221687
Training loss on iteration 2660 = 0.6520883485674858
Training loss on iteration 2680 = 0.8136864304542542
Training loss on iteration 2700 = 0.6051949843764305
Training loss on iteration 2720 = 0.7944950371980667
Training loss on iteration 2740 = 0.7678114801645279
Training loss on iteration 2760 = 0.9706492960453034
Training loss on iteration 2780 = 0.6254334501922131
Training loss on iteration 2800 = 0.7986944735050201
Training loss on iteration 2820 = 0.8834996342658996
Training loss on iteration 2840 = 0.7197323516011238
Training loss on iteration 2860 = 0.7855286315083504
Training loss on iteration 2880 = 0.6651091292500496
Training loss on iteration 2900 = 0.9345287963747978
Training loss on iteration 2920 = 0.8336022466421127
Training loss on iteration 2940 = 0.5351693473756314
Training loss on iteration 2960 = 0.565370512008667
Training loss on iteration 2980 = 0.6517871007323265
Training loss on iteration 3000 = 0.782874359190464
Training loss on iteration 3020 = 0.7851548239588737
Training loss on iteration 3040 = 0.7165563642978668
Training loss on iteration 3060 = 0.7337868005037308
Training loss on iteration 3080 = 0.7751643314957619
Training loss on iteration 3100 = 0.6487136110663414
Training loss on iteration 3120 = 0.7529137805104256
Training loss on iteration 3140 = 0.6339591339230537
Training loss on iteration 3160 = 0.6139812842011452
Training loss on iteration 3180 = 0.6779474817216397
Training loss on iteration 3200 = 0.6043138518929482
Training loss on iteration 3220 = 0.5969169452786446
Training loss on iteration 3240 = 0.6973928049206733
Training loss on iteration 3260 = 0.5721008986234665
Training loss on iteration 3280 = 0.6945167049765587
Training loss on iteration 3300 = 0.6304525174200535
Training loss on iteration 3320 = 0.6457331001758575
Training loss on iteration 3340 = 0.84231818318367
Training loss on iteration 3360 = 1.016287635266781
Training loss on iteration 3380 = 1.3721171960234642
Training loss on iteration 3400 = 1.151878370344639
Training loss on iteration 3420 = 0.9063759222626686
Training loss on iteration 3440 = 1.1117583781480789
Training loss on iteration 3460 = 0.8138253897428512
Training loss on iteration 3480 = 0.8604507490992546
Training loss on iteration 3500 = 0.8906647622585296
Training loss on iteration 3520 = 0.8708708360791206
Training loss on iteration 3540 = 0.7578151255846024
Training loss on iteration 3560 = 0.9157510682940483
Training loss on iteration 3580 = 0.7767210528254509
Training loss on iteration 3600 = 0.7139998719096183
Training loss on iteration 3620 = 0.8053977519273758
Training loss on iteration 3640 = 0.7178814545273781
Training loss on iteration 3660 = 0.6487605631351471
Training loss on iteration 3680 = 0.8880355045199394
Training loss on iteration 3700 = 0.7189122945070267
Training loss on iteration 3720 = 0.8757558047771454
Training loss on iteration 3740 = 1.0552359998226166
Training loss on iteration 3760 = 0.8015084326267242
Training loss on iteration 3780 = 0.8623073637485504
Training loss on iteration 3800 = 0.792897292971611
Training loss on iteration 3820 = 0.8004320971667767
Training loss on iteration 3840 = 1.0317986637353898
Training loss on iteration 3860 = 1.1956696599721908
Training loss on iteration 3880 = 1.2131484568119049
Training loss on iteration 3900 = 0.8930101543664932
Training loss on iteration 3920 = 0.8886513993144035
Training loss on iteration 3940 = 0.9005673483014107
Training loss on iteration 3960 = 0.7492940038442611
Training loss on iteration 3980 = 0.7369332611560822
Training loss on iteration 4000 = 0.7891696617007256
Training loss on iteration 4020 = 0.7949101015925407
Training loss on iteration 4040 = 0.7968341514468193
Training loss on iteration 4060 = 0.7323725745081902
Training loss on iteration 4080 = 0.752454599738121
Training loss on iteration 4100 = 0.735712294280529
Training loss on iteration 4120 = 0.7833627194166184
Training loss on iteration 4140 = 1.749900797009468
Training loss on iteration 4160 = 1.094709387421608
Training loss on iteration 4180 = 1.0664890974760055
Training loss on iteration 4200 = 2.4106285467743875
Training loss on iteration 4220 = 1.3176844447851181
Training loss on iteration 4240 = 0.9681126207113266
Training loss on iteration 4260 = 1.2571199834346771
Training loss on iteration 4280 = 0.7937093898653984
Training loss on iteration 4300 = 0.8394322082400322
Training loss on iteration 4320 = 1.2886724337935447
Training loss on iteration 4340 = 0.8409965127706528
Training loss on iteration 4360 = 1.1205885767936707
Training loss on iteration 4380 = 0.7068098410964012
Training loss on iteration 4400 = 0.9223552748560906
Training loss on iteration 4420 = 0.9256387010216713
Training loss on iteration 4440 = 0.9106638476252555
Training loss on iteration 4460 = 0.8512619450688362
Training loss on iteration 4480 = 0.7657526552677154
Training loss on iteration 4500 = 0.7733949765563011
Training loss on iteration 4520 = 1.0124155752360822
Training loss on iteration 4540 = 0.8863658301532269
Training loss on iteration 4560 = 0.9094349503517151
Training loss on iteration 4580 = 1.011611247062683
Training loss on iteration 4600 = 0.7624991476535797
Training loss on iteration 4620 = 0.781636405736208
Training loss on iteration 4640 = 0.8459453538060189
Training loss on iteration 4660 = 0.8205435141921044
Training loss on iteration 4680 = 0.7337311714887619
Training loss on iteration 4700 = 0.8922918438911438
Training loss on iteration 4720 = 1.0032427057623863
Training loss on iteration 4740 = 0.7856577202677727
Training loss on iteration 4760 = 0.6324923567473888
Training loss on iteration 4780 = 0.9398187652230263
Training loss on iteration 4800 = 0.800157630443573
Training loss on iteration 4820 = 1.3040542259812355
Training loss on iteration 4840 = 0.9800267159938812
Training loss on iteration 4860 = 0.9017845749855041
Training loss on iteration 4880 = 0.8325831338763237
Training loss on iteration 4900 = 0.6335713893175126
Training loss on iteration 4920 = 0.6938998654484749
Training loss on iteration 4940 = 0.6519345074892045
Training loss on iteration 4960 = 0.8880486115813255
Training loss on iteration 4980 = 0.6605440691113472
Training loss on iteration 5000 = 0.7206647023558617
Training loss on iteration 5020 = 0.5467842362821103
Training loss on iteration 5040 = 0.9477307125926018
Training loss on iteration 5060 = 0.8168118804693222
Training loss on iteration 5080 = 1.0059594482183456
Training loss on iteration 5100 = 0.8746712505817413
Training loss on iteration 5120 = 0.7326234064996242
Training loss on iteration 5140 = 0.8097529083490371
Training loss on iteration 5160 = 0.7876174867153167
Training loss on iteration 5180 = 0.737082114815712
Training loss on iteration 5200 = 0.7577330425381661
Training loss on iteration 5220 = 0.7502675995230674
Training loss on iteration 5240 = 0.7253877744078636
Training loss on iteration 5260 = 0.7355708405375481
Training loss on iteration 5280 = 0.844111755490303
Training loss on iteration 5300 = 0.744461578130722
Training loss on iteration 5320 = 0.6513678260147572
Training loss on iteration 5340 = 0.7713191464543343
Training loss on iteration 5360 = 0.6720993325114251
Training loss on iteration 5380 = 0.8288707338273525
Training loss on iteration 5400 = 0.635678020119667
Training loss on iteration 5420 = 0.7298278793692589
Training loss on iteration 5440 = 0.6835127606987953
Training loss on iteration 5460 = 0.8154542699456215
Training loss on iteration 5480 = 0.9546264663338662
Training loss on iteration 5500 = 0.9259495213627815
Training loss on iteration 5520 = 0.7095643267035484
Training loss on iteration 5540 = 0.853295736014843
Training loss on iteration 5560 = 0.8019281074404716
Training loss on iteration 5580 = 0.8365393102169036
Training loss on iteration 5600 = 0.9892885364592076
Training loss on iteration 5620 = 0.726885911822319
Training loss on iteration 5640 = 0.8405825242400169
Training loss on iteration 5660 = 0.8895178198814392
Training loss on iteration 5680 = 0.8329634994268418
Training loss on iteration 5700 = 0.907528755068779
Training loss on iteration 5720 = 0.7275081515312195
Training loss on iteration 5740 = 0.6675100296735763
Training loss on iteration 5760 = 0.6139437511563302
Training loss on iteration 5780 = 0.6551149323582649
Training loss on iteration 5800 = 0.7941400796175003
Training loss on iteration 5820 = 0.6773765116930008
Training loss on iteration 5840 = 0.602996189892292
Training loss on iteration 5860 = 0.7746003404259681
Training loss on iteration 5880 = 0.6809205882251262
Training loss on iteration 5900 = 0.6080665394663811
Training loss on iteration 5920 = 0.6437963813543319
Training loss on iteration 5940 = 0.7846292957663537
Training loss on iteration 5960 = 0.7685069136321545
Training loss on iteration 5980 = 0.6543975010514259
Training loss on iteration 6000 = 0.7023969709873199
Training loss on iteration 6020 = 0.715697494149208
Training loss on iteration 6040 = 0.6745286896824837
Training loss on iteration 6060 = 0.6365975648164749
Training loss on iteration 6080 = 0.8323119759559632
Training loss on iteration 6100 = 0.7224738970398903
Training loss on iteration 6120 = 0.7399010986089707
Training loss on iteration 6140 = 0.8457899689674377
Training loss on iteration 6160 = 0.6974023453891277
Training loss on iteration 6180 = 0.7340485841035843
Training loss on iteration 6200 = 0.7410919517278671
Training loss on iteration 6220 = 0.7213138252496719
Training loss on iteration 6240 = 0.9641600497066974
Training loss on iteration 6260 = 0.7268933579325676
Training loss on iteration 6280 = 0.6103999815881253
Training loss on iteration 6300 = 0.6864262968301773
Training loss on iteration 6320 = 0.6627124145627021
Training loss on iteration 6340 = 0.8194343656301498
Training loss on iteration 6360 = 0.6995232813060284
Training loss on iteration 6380 = 0.8250212647020817
Training loss on iteration 0 = 0.24881872534751892
Training loss on iteration 20 = 0.8227538496255875
Training loss on iteration 40 = 0.5796688959002495
Training loss on iteration 60 = 0.7610145382583141
Training loss on iteration 80 = 0.5646849758923054
Training loss on iteration 100 = 0.5338062554597854
Training loss on iteration 120 = 0.5820414267480374
Training loss on iteration 140 = 0.7229947566986084
Training loss on iteration 160 = 0.7342099919915199
Training loss on iteration 180 = 0.7224702887237072
Training loss on iteration 200 = 0.6882433623075486
Training loss on iteration 220 = 0.6995598666369915
Training loss on iteration 240 = 0.7314550518989563
Training loss on iteration 260 = 0.5734833113849163
Training loss on iteration 280 = 0.7001480638980866
Training loss on iteration 300 = 0.7689654067158699
Training loss on iteration 320 = 0.6711122289299964
Training loss on iteration 340 = 0.7012375846505166
Training loss on iteration 360 = 0.5808546900749206
Training loss on iteration 380 = 0.7494842201471329
Training loss on iteration 400 = 0.6567939624190331
Training loss on iteration 420 = 0.5966809205710888
Training loss on iteration 440 = 0.6739437147974968
Training loss on iteration 460 = 0.7751965418457984
Training loss on iteration 480 = 0.6108621701598167
Training loss on iteration 500 = 0.7442353889346123
Training loss on iteration 520 = 0.5954157114028931
Training loss on iteration 540 = 0.6487585783004761
Training loss on iteration 560 = 0.5943541780114174
Training loss on iteration 580 = 0.647802859544754
Training loss on iteration 600 = 0.7808543428778648
Training loss on iteration 620 = 0.848954938352108
Training loss on iteration 640 = 0.6621584601700305
Training loss on iteration 660 = 0.6615316808223725
Training loss on iteration 680 = 0.6668228313326836
Training loss on iteration 700 = 0.8258185520768165
Training loss on iteration 720 = 0.6749265708029271
Training loss on iteration 740 = 0.8832599103450776
Training loss on iteration 760 = 0.5134826570749282
Training loss on iteration 780 = 0.7363922476768494
Training loss on iteration 800 = 0.606652843952179
Training loss on iteration 820 = 0.6563642300665379
Training loss on iteration 840 = 0.6646652862429618
Training loss on iteration 860 = 0.7644265800714493
Training loss on iteration 880 = 0.7338608056306839
Training loss on iteration 900 = 0.715164016187191
Training loss on iteration 920 = 0.616141352057457
Training loss on iteration 940 = 0.5420143768191338
Training loss on iteration 960 = 0.5645266614854336
Training loss on iteration 980 = 0.8458036184310913
Training loss on iteration 1000 = 0.8378926128149032
Training loss on iteration 1020 = 0.5418193511664867
Training loss on iteration 1040 = 0.7039695456624031
Training loss on iteration 1060 = 0.7075243055820465
Training loss on iteration 1080 = 0.6964015170931817
Training loss on iteration 1100 = 0.6248897105455399
Training loss on iteration 1120 = 0.7521083801984787
Training loss on iteration 1140 = 0.7174478486180306
Training loss on iteration 1160 = 0.6946092486381531
Training loss on iteration 1180 = 0.6314516693353653
Training loss on iteration 1200 = 0.703590615093708
Training loss on iteration 1220 = 0.6559482961893082
Training loss on iteration 1240 = 0.7153704702854157
Training loss on iteration 1260 = 0.5793184921145439
Training loss on iteration 1280 = 0.7701718740165233
Training loss on iteration 1300 = 1.025839464366436
Training loss on iteration 1320 = 0.645856237411499
Training loss on iteration 1340 = 0.6510237634181977
Training loss on iteration 1360 = 0.69321368932724
Training loss on iteration 1380 = 0.8573033481836319
Training loss on iteration 1400 = 0.8888583764433861
Training loss on iteration 1420 = 0.6722073495388031
Training loss on iteration 1440 = 0.7455278113484383
Training loss on iteration 1460 = 0.7589717507362366
Training loss on iteration 1480 = 0.9178940698504447
Training loss on iteration 1500 = 0.5834675312042237
Training loss on iteration 1520 = 0.6025689914822578
Training loss on iteration 1540 = 0.658541065454483
Training loss on iteration 1560 = 0.6167314052581787
Training loss on iteration 1580 = 0.7071782678365708
Training loss on iteration 1600 = 0.6477918535470962
Training loss on iteration 1620 = 0.9262707844376564
Training loss on iteration 1640 = 0.5347599014639854
Training loss on iteration 1660 = 0.6385593086481094
Training loss on iteration 1680 = 0.6443703651428223
Training loss on iteration 1700 = 0.7668079003691674
Training loss on iteration 1720 = 0.7683371722698211
Training loss on iteration 1740 = 0.6344367459416389
Training loss on iteration 1760 = 0.6774307608604431
Training loss on iteration 1780 = 0.9445531815290451
Training loss on iteration 1800 = 0.7038194864988327
Training loss on iteration 1820 = 0.8739236608147621
Training loss on iteration 1840 = 0.6606048360466957
Training loss on iteration 1860 = 0.5829299062490463
Training loss on iteration 1880 = 0.7491208359599113
Training loss on iteration 1900 = 0.7974831685423851
Training loss on iteration 1920 = 0.8809700667858124
Training loss on iteration 1940 = 0.9686109438538552
Training loss on iteration 1960 = 0.7984797209501266
Training loss on iteration 1980 = 0.7822193175554275
Training loss on iteration 2000 = 0.7255521729588509
Training loss on iteration 2020 = 0.6139200448989868
Training loss on iteration 2040 = 0.8013708889484406
Training loss on iteration 2060 = 0.6057006299495697
Training loss on iteration 2080 = 0.9431998632848263
Training loss on iteration 2100 = 0.5870937258005142
Training loss on iteration 2120 = 0.7330535173416137
Training loss on iteration 2140 = 0.7827008545398713
Training loss on iteration 2160 = 0.7772584140300751
Training loss on iteration 2180 = 0.8912073194980621
Training loss on iteration 2200 = 0.7159597128629684
Training loss on iteration 2220 = 0.6934490069746971
Training loss on iteration 2240 = 0.8076908960938454
Training loss on iteration 2260 = 0.8787007167935371
Training loss on iteration 2280 = 0.5906836897134781
Training loss on iteration 2300 = 0.7380689978599548
Training loss on iteration 2320 = 0.5993114098906517
Training loss on iteration 2340 = 0.6181194365024567
Training loss on iteration 2360 = 0.6719983816146851
Training loss on iteration 2380 = 0.7369453400373459
Training loss on iteration 2400 = 0.5506336227059364
Training loss on iteration 2420 = 0.5545221723616123
Training loss on iteration 2440 = 0.6214215606451035
Training loss on iteration 2460 = 0.8294886291027069
Training loss on iteration 2480 = 0.6841791063547135
Training loss on iteration 2500 = 0.6978592433035373
Training loss on iteration 2520 = 0.6287323415279389
Training loss on iteration 2540 = 0.7269858255982399
Training loss on iteration 2560 = 0.7432414926588535
Training loss on iteration 2580 = 0.8424282476305962
Training loss on iteration 2600 = 0.8189633071422577
Training loss on iteration 2620 = 0.6836279198527336
Training loss on iteration 2640 = 0.8302812196314335
Training loss on iteration 2660 = 0.6648754499852657
Training loss on iteration 2680 = 0.6740492887794971
Training loss on iteration 2700 = 0.8441700249910354
Training loss on iteration 2720 = 1.0463308617472649
Training loss on iteration 2740 = 0.702818064391613
Training loss on iteration 2760 = 0.6325172662734986
Training loss on iteration 2780 = 0.9897366985678673
Training loss on iteration 2800 = 0.8114725589752197
Training loss on iteration 2820 = 0.6640307381749153
Training loss on iteration 2840 = 0.6623052708804608
Training loss on iteration 2860 = 0.8484099395573139
Training loss on iteration 2880 = 0.5821870170533657
Training loss on iteration 2900 = 0.8001695223152637
Training loss on iteration 2920 = 0.8555954739451408
Training loss on iteration 2940 = 0.7857884131371975
Training loss on iteration 2960 = 0.6788073956966401
Training loss on iteration 2980 = 0.419296070933342
Training loss on iteration 3000 = 0.6581850968301296
Training loss on iteration 3020 = 0.7659966394305229
Training loss on iteration 3040 = 0.7025735050439834
Training loss on iteration 3060 = 0.6749183811247349
Training loss on iteration 3080 = 0.8856630489230156
Training loss on iteration 3100 = 0.7053029909729958
Training loss on iteration 3120 = 0.6297185748815537
Training loss on iteration 3140 = 0.6413103722035884
Training loss on iteration 3160 = 0.6559990927577019
Training loss on iteration 3180 = 0.5242832556366921
Training loss on iteration 3200 = 0.6476093351840972
Training loss on iteration 3220 = 0.5934808619320393
Training loss on iteration 3240 = 0.6004178576171398
Training loss on iteration 3260 = 0.7195536881685257
Training loss on iteration 3280 = 0.6935082882642746
Training loss on iteration 3300 = 0.7163699969649315
Training loss on iteration 3320 = 0.783308357000351
Training loss on iteration 3340 = 0.8065206728875637
Training loss on iteration 3360 = 0.5624540418386459
Training loss on iteration 3380 = 0.6827302053570747
Training loss on iteration 3400 = 0.7552524864673614
Training loss on iteration 3420 = 0.741463603079319
Training loss on iteration 3440 = 0.695985259115696
Training loss on iteration 3460 = 0.5614603981375694
Training loss on iteration 3480 = 0.593866902589798
Training loss on iteration 3500 = 0.7088740184903145
Training loss on iteration 3520 = 0.7404604703187943
Training loss on iteration 3540 = 0.6701138101518154
Training loss on iteration 3560 = 0.5400870531797409
Training loss on iteration 3580 = 0.5087279513478279
Training loss on iteration 3600 = 0.6122212380170822
Training loss on iteration 3620 = 0.7602307938039303
Training loss on iteration 3640 = 0.7205496042966842
Training loss on iteration 3660 = 0.5695901244878769
Training loss on iteration 3680 = 0.7774387210607528
Training loss on iteration 3700 = 0.6904816523194313
Training loss on iteration 3720 = 0.7296701699495316
Training loss on iteration 3740 = 0.8260499358177185
Training loss on iteration 3760 = 0.716578946262598
Training loss on iteration 3780 = 0.6226234152913094
Training loss on iteration 3800 = 0.8225270062685013
Training loss on iteration 3820 = 0.5109382890164852
Training loss on iteration 3840 = 0.674681943655014
Training loss on iteration 3860 = 0.5722470834851265
Training loss on iteration 3880 = 0.5922020338475704
Training loss on iteration 3900 = 0.7536675840616226
Training loss on iteration 3920 = 0.8254664182662964
Training loss on iteration 3940 = 0.7166423216462136
Training loss on iteration 3960 = 0.8263364657759666
Training loss on iteration 3980 = 0.8020095989108086
Training loss on iteration 4000 = 0.6959663927555084
Training loss on iteration 4020 = 0.681298802793026
Training loss on iteration 4040 = 0.6109241746366024
Training loss on iteration 4060 = 0.6742347314953804
Training loss on iteration 4080 = 0.5812582097947597
Training loss on iteration 4100 = 0.724851056933403
Training loss on iteration 4120 = 0.7526665180921555
Training loss on iteration 4140 = 0.7612148508429527
Training loss on iteration 4160 = 0.7104234829545021
Training loss on iteration 4180 = 0.7343758039176465
Training loss on iteration 4200 = 0.8796196863055229
Training loss on iteration 4220 = 0.7535011596977711
Training loss on iteration 4240 = 0.5812334634363652
Training loss on iteration 4260 = 0.6830627873539925
Training loss on iteration 4280 = 0.6496043175458908
Training loss on iteration 4300 = 0.6644188068807125
Training loss on iteration 4320 = 0.6103988438844681
Training loss on iteration 4340 = 0.6761299915611744
Training loss on iteration 4360 = 0.6323057144880295
Training loss on iteration 4380 = 0.789712655544281
Training loss on iteration 4400 = 0.6275810480117798
Training loss on iteration 4420 = 0.6927691124379635
Training loss on iteration 4440 = 0.7957889512181282
Training loss on iteration 4460 = 0.8075622007250786
Training loss on iteration 4480 = 0.7057512119412422
Training loss on iteration 4500 = 0.7913813859224319
Training loss on iteration 4520 = 0.7030285507440567
Training loss on iteration 4540 = 0.902530026435852
Training loss on iteration 4560 = 0.7990522384643555
Training loss on iteration 4580 = 0.7930495858192443
Training loss on iteration 4600 = 0.7445290952920913
Training loss on iteration 4620 = 0.6199781090021134
Training loss on iteration 4640 = 0.7858658008277416
Training loss on iteration 4660 = 0.7167135313153267
Training loss on iteration 4680 = 0.8247087985277176
Training loss on iteration 4700 = 0.794327462464571
Training loss on iteration 4720 = 0.9886315137147903
Training loss on iteration 4740 = 0.7252925664186478
Training loss on iteration 4760 = 0.9199396640062332
Training loss on iteration 4780 = 0.6344502046704292
Training loss on iteration 4800 = 0.6610495865345001
Training loss on iteration 4820 = 0.6853558853268623
Training loss on iteration 4840 = 0.5853800155222416
Training loss on iteration 4860 = 0.7307955875992775
Training loss on iteration 4880 = 0.6902739509940148
Training loss on iteration 4900 = 0.6982181251049042
Training loss on iteration 4920 = 0.6225675277411937
Training loss on iteration 4940 = 0.6401796832680702
Training loss on iteration 4960 = 0.814158234000206
Training loss on iteration 4980 = 0.6570643700659275
Training loss on iteration 5000 = 0.8193269953131675
Training loss on iteration 5020 = 0.7683422803878784
Training loss on iteration 5040 = 0.49145674556493757
Training loss on iteration 5060 = 0.6328188844025135
Training loss on iteration 5080 = 0.58781927973032
Training loss on iteration 5100 = 0.7543488278985023
Training loss on iteration 5120 = 0.6384000301361084
Training loss on iteration 5140 = 0.5263105019927025
Training loss on iteration 5160 = 0.6692096397280693
Training loss on iteration 5180 = 0.5714041069149971
Training loss on iteration 5200 = 0.7554259762167931
Training loss on iteration 5220 = 0.789791502803564
Training loss on iteration 5240 = 0.5353895910084248
Training loss on iteration 5260 = 0.7308102309703827
Training loss on iteration 5280 = 0.8322133287787438
Training loss on iteration 5300 = 0.6101373270153999
Training loss on iteration 5320 = 0.6667246311903
Training loss on iteration 5340 = 0.5174001134932041
Training loss on iteration 5360 = 0.7327889956533908
Training loss on iteration 5380 = 0.5195786543190479
Training loss on iteration 5400 = 0.9665200665593148
Training loss on iteration 5420 = 0.6391935132443904
Training loss on iteration 5440 = 0.6502221822738647
Training loss on iteration 5460 = 0.723385326564312
Training loss on iteration 5480 = 0.7002102486789227
Training loss on iteration 5500 = 0.5893915593624115
Training loss on iteration 5520 = 0.7818147450685501
Training loss on iteration 5540 = 0.801689338684082
Training loss on iteration 5560 = 0.623923996090889
Training loss on iteration 5580 = 0.6828855857253074
Training loss on iteration 5600 = 0.8870030865073204
Training loss on iteration 5620 = 0.673787622153759
Training loss on iteration 5640 = 0.8366756692528725
Training loss on iteration 5660 = 0.701073594391346
Training loss on iteration 5680 = 0.6616505272686481
Training loss on iteration 5700 = 0.8072998709976673
Training loss on iteration 5720 = 0.8634144119918347
Training loss on iteration 5740 = 0.9824384614825249
Training loss on iteration 5760 = 0.6644160956144333
Training loss on iteration 5780 = 0.7470919743180275
Training loss on iteration 5800 = 0.7507079988718033
Training loss on iteration 5820 = 0.823216013610363
Training loss on iteration 5840 = 0.8568370804190636
Training loss on iteration 5860 = 0.7491166442632675
Training loss on iteration 5880 = 0.7964961156249046
Training loss on iteration 5900 = 0.6524443291127682
Training loss on iteration 5920 = 0.7862929970026016
Training loss on iteration 5940 = 0.6592603772878647
Training loss on iteration 5960 = 0.7136164858937264
Training loss on iteration 5980 = 0.6826672472059727
Training loss on iteration 6000 = 0.772102627158165
Training loss on iteration 6020 = 0.8104321882128716
Training loss on iteration 6040 = 0.9480819016695022
Training loss on iteration 6060 = 0.666750855743885
Training loss on iteration 6080 = 0.6157650023698806
Training loss on iteration 6100 = 0.5341705493628979
Training loss on iteration 6120 = 0.5718739449977874
Training loss on iteration 6140 = 0.7846400573849678
Training loss on iteration 6160 = 0.6979624785482883
Training loss on iteration 6180 = 0.8178259491920471
Training loss on iteration 6200 = 0.6292596623301506
Training loss on iteration 6220 = 0.5929362505674363
Training loss on iteration 6240 = 0.7362121686339378
Training loss on iteration 6260 = 0.6358363039791584
Training loss on iteration 6280 = 0.6287008970975876
Training loss on iteration 6300 = 0.7264509558677673
Training loss on iteration 6320 = 0.5356729440391064
Training loss on iteration 6340 = 0.5964263796806335
Training loss on iteration 6360 = 0.5550250373780727
Training loss on iteration 6380 = 0.6764084607362747
Training loss on iteration 0 = 0.27303653955459595
Training loss on iteration 20 = 0.5259537421166897
Training loss on iteration 40 = 0.7908906258642674
Training loss on iteration 60 = 0.8251693233847618
Training loss on iteration 80 = 0.6752073310315609
Training loss on iteration 100 = 0.7218536898493767
Training loss on iteration 120 = 0.6397249206900597
Training loss on iteration 140 = 0.8273698851466179
Training loss on iteration 160 = 0.6856430739164352
Training loss on iteration 180 = 0.5835187777876853
Training loss on iteration 200 = 0.6156704068183899
Training loss on iteration 220 = 0.5785465843975544
Training loss on iteration 240 = 0.6717151306569576
Training loss on iteration 260 = 0.6474240474402905
Training loss on iteration 280 = 0.5889557436108589
Training loss on iteration 300 = 0.6359355419874191
Training loss on iteration 320 = 0.5994640603661537
Training loss on iteration 340 = 0.6151704855263234
Training loss on iteration 360 = 0.6749990619719028
Training loss on iteration 380 = 0.6434040471911431
Training loss on iteration 400 = 0.5838442988693714
Training loss on iteration 420 = 0.6454911902546883
Training loss on iteration 440 = 0.6316364765167236
Training loss on iteration 460 = 0.8565843492746353
Training loss on iteration 480 = 0.8371064454317093
Training loss on iteration 500 = 0.5404215604066849
Training loss on iteration 520 = 0.6073256999254226
Training loss on iteration 540 = 0.5406016610562802
Training loss on iteration 560 = 0.6486982218921185
Training loss on iteration 580 = 0.7142727434635162
Training loss on iteration 600 = 0.7630462259054184
Training loss on iteration 620 = 0.6222800441086292
Training loss on iteration 640 = 0.6290927104651928
Training loss on iteration 660 = 0.6331809759140015
Training loss on iteration 680 = 0.6044487625360488
Training loss on iteration 700 = 0.6220542393624783
Training loss on iteration 720 = 0.5022236809134484
Training loss on iteration 740 = 0.5663518726825714
Training loss on iteration 760 = 0.5695158846676349
Training loss on iteration 780 = 0.7460825756192208
Training loss on iteration 800 = 0.705007080733776
Training loss on iteration 820 = 0.7042957812547683
Training loss on iteration 840 = 0.6680322855710983
Training loss on iteration 860 = 0.5901633881032466
Training loss on iteration 880 = 0.589481919258833
Training loss on iteration 900 = 0.4904006399214268
Training loss on iteration 920 = 0.520616427809
Training loss on iteration 940 = 0.6237929448485374
Training loss on iteration 960 = 0.7125909373164176
Training loss on iteration 980 = 0.6314136356115341
Training loss on iteration 1000 = 0.5439695671200753
Training loss on iteration 1020 = 0.607257180660963
Training loss on iteration 1040 = 0.6058706037700177
Training loss on iteration 1060 = 0.7288997754454613
Training loss on iteration 1080 = 0.648489861190319
Training loss on iteration 1100 = 0.6252520993351937
Training loss on iteration 1120 = 0.6991249866783619
Training loss on iteration 1140 = 0.6257662266492844
Training loss on iteration 1160 = 0.617260529845953
Training loss on iteration 1180 = 0.7438061088323593
Training loss on iteration 1200 = 0.5824046090245247
Training loss on iteration 1220 = 0.5908838286995888
Training loss on iteration 1240 = 0.7070024959743023
Training loss on iteration 1260 = 0.5723789900541305
Training loss on iteration 1280 = 0.8187208086252212
Training loss on iteration 1300 = 0.6189997434616089
Training loss on iteration 1320 = 0.6373596623539924
Training loss on iteration 1340 = 0.7426629796624183
Training loss on iteration 1360 = 0.5301250621676445
Training loss on iteration 1380 = 0.5516208663582802
Training loss on iteration 1400 = 0.5687136650085449
Training loss on iteration 1420 = 0.6787475503981113
Training loss on iteration 1440 = 0.6191529408097267
Training loss on iteration 1460 = 0.62725710272789
Training loss on iteration 1480 = 0.5933647587895393
Training loss on iteration 1500 = 0.5710313737392425
Training loss on iteration 1520 = 0.7443763494491578
Training loss on iteration 1540 = 0.5699282944202423
Training loss on iteration 1560 = 0.923715415596962
Training loss on iteration 1580 = 0.7483382739126683
Training loss on iteration 1600 = 0.7318652346730232
Training loss on iteration 1620 = 0.6098015017807483
Training loss on iteration 1640 = 0.7306528076529503
Training loss on iteration 1660 = 0.7367259174585342
Training loss on iteration 1680 = 0.6455331549048424
Training loss on iteration 1700 = 0.7010353729128838
Training loss on iteration 1720 = 0.899993497133255
Training loss on iteration 1740 = 0.7888386368751525
Training loss on iteration 1760 = 0.596306387335062
Training loss on iteration 1780 = 0.55917127430439
Training loss on iteration 1800 = 0.7915262088179589
Training loss on iteration 1820 = 0.766735064983368
Training loss on iteration 1840 = 0.6987675696611404
Training loss on iteration 1860 = 0.6556570969522
Training loss on iteration 1880 = 0.8153523460030556
Training loss on iteration 1900 = 0.7628264546394348
Training loss on iteration 1920 = 0.7044885426759719
Training loss on iteration 1940 = 0.6244273513555527
Training loss on iteration 1960 = 0.7002534180879593
Training loss on iteration 1980 = 0.6238853961229325
Training loss on iteration 2000 = 0.6262870460748673
Training loss on iteration 2020 = 0.847389767318964
Training loss on iteration 2040 = 0.5801919430494309
Training loss on iteration 2060 = 0.4750420093536377
Training loss on iteration 2080 = 0.45762609615921973
Training loss on iteration 2100 = 0.5865173459053039
Training loss on iteration 2120 = 0.6014279015362263
Training loss on iteration 2140 = 0.5851915717124939
Training loss on iteration 2160 = 16.799827547371386
Training loss on iteration 2180 = 0.9742597222328186
Training loss on iteration 2200 = 1.0265294149518014
Training loss on iteration 2220 = 0.8781559631228447
Training loss on iteration 2240 = 0.977374741435051
Training loss on iteration 2260 = 0.6325152978301049
Training loss on iteration 2280 = 0.8468550533056259
Training loss on iteration 2300 = 0.7167865812778473
Training loss on iteration 2320 = 0.6035372108221054
Training loss on iteration 2340 = 0.9102353408932686
Training loss on iteration 2360 = 0.9381097286939621
Training loss on iteration 2380 = 0.6681840062141419
Training loss on iteration 2400 = 0.6070857807993889
Training loss on iteration 2420 = 0.6143810600042343
Training loss on iteration 2440 = 0.7630211353302002
Training loss on iteration 2460 = 0.8202438890933991
Training loss on iteration 2480 = 0.6421797960996628
Training loss on iteration 2500 = 0.6683367945253849
Training loss on iteration 2520 = 0.7040624387562275
Training loss on iteration 2540 = 0.7625019326806068
Training loss on iteration 2560 = 0.8697474583983421
Training loss on iteration 2580 = 0.583915613591671
Training loss on iteration 2600 = 0.5167705453932285
Training loss on iteration 2620 = 0.6727305307984353
Training loss on iteration 2640 = 0.7523900151252747
Training loss on iteration 2660 = 0.7393029794096947
Training loss on iteration 2680 = 0.7468630895018578
Training loss on iteration 2700 = 0.5393473714590072
Training loss on iteration 2720 = 0.8563705503940582
Training loss on iteration 2740 = 0.6713387966156006
Training loss on iteration 2760 = 0.7560303926467895
Training loss on iteration 2780 = 0.5565145157277585
Training loss on iteration 2800 = 0.6545949682593346
Training loss on iteration 2820 = 0.7056378163397312
Training loss on iteration 2840 = 0.6680558368563652
Training loss on iteration 2860 = 0.7952591940760613
Training loss on iteration 2880 = 0.7787146583199501
Training loss on iteration 2900 = 0.5542911320924759
Training loss on iteration 2920 = 0.6929513663053513
Training loss on iteration 2940 = 0.6817637026309967
Training loss on iteration 2960 = 0.7164076626300812
Training loss on iteration 2980 = 0.4883701324462891
Training loss on iteration 3000 = 0.704749920964241
Training loss on iteration 3020 = 0.6639166198670864
Training loss on iteration 3040 = 0.45949181318283083
Training loss on iteration 3060 = 0.7654630854725838
Training loss on iteration 3080 = 0.5986387804150581
Training loss on iteration 3100 = 0.6372671902179718
Training loss on iteration 3120 = 0.7561524406075477
Training loss on iteration 3140 = 0.6016201712191105
Training loss on iteration 3160 = 0.8722714871168137
Training loss on iteration 3180 = 0.6338566482067108
Training loss on iteration 3200 = 0.7971078723669052
Training loss on iteration 3220 = 0.6716745845973492
Training loss on iteration 3240 = 0.6053858309984207
Training loss on iteration 3260 = 0.5833665497601033
Training loss on iteration 3280 = 0.6117006942629815
Training loss on iteration 3300 = 0.8505384206771851
Training loss on iteration 3320 = 0.7027974635362625
Training loss on iteration 3340 = 0.6777023404836655
Training loss on iteration 3360 = 0.6058978229761124
Training loss on iteration 3380 = 0.7560965478420257
Training loss on iteration 3400 = 0.8104959085583687
Training loss on iteration 3420 = 0.5806422494351864
Training loss on iteration 3440 = 0.5921990364789963
Training loss on iteration 3460 = 0.606557235121727
Training loss on iteration 3480 = 0.6242235831916332
Training loss on iteration 3500 = 0.6299270816147328
Training loss on iteration 3520 = 0.6880581445991993
Training loss on iteration 3540 = 0.5718456953763962
Training loss on iteration 3560 = 0.6835906408727169
Training loss on iteration 3580 = 0.6114445053040981
Training loss on iteration 3600 = 0.7025024026632309
Training loss on iteration 3620 = 0.5884812019765377
Training loss on iteration 3640 = 0.7853561662137508
Training loss on iteration 3660 = 0.5656876176595688
Training loss on iteration 3680 = 0.7184096664190293
Training loss on iteration 3700 = 0.6224619343876838
Training loss on iteration 3720 = 0.566803066432476
Training loss on iteration 3740 = 0.6780151873826981
Training loss on iteration 3760 = 0.5782249048352242
Training loss on iteration 3780 = 0.7865775331854821
Training loss on iteration 3800 = 0.8270846404135227
Training loss on iteration 3820 = 0.6780951246619225
Training loss on iteration 3840 = 0.4984330326318741
Training loss on iteration 3860 = 0.6492491580545903
Training loss on iteration 3880 = 0.6277034342288971
Training loss on iteration 3900 = 0.5951647624373436
Training loss on iteration 3920 = 0.6349707812070846
Training loss on iteration 3940 = 0.569028089940548
Training loss on iteration 3960 = 0.6533733367919922
Training loss on iteration 3980 = 0.5585570082068443
Training loss on iteration 4000 = 0.8451132670044899
Training loss on iteration 4020 = 0.8045667007565498
Training loss on iteration 4040 = 0.6246837496757507
Training loss on iteration 4060 = 0.779278016090393
Training loss on iteration 4080 = 0.7238237172365188
Training loss on iteration 4100 = 0.8212144106626511
Training loss on iteration 4120 = 0.7376334354281425
Training loss on iteration 4140 = 0.6748529180884362
Training loss on iteration 4160 = 0.5309338964521885
Training loss on iteration 4180 = 0.6259268745779991
Training loss on iteration 4200 = 0.6050779283046722
Training loss on iteration 4220 = 0.6489186778664588
Training loss on iteration 4240 = 0.7143027260899544
Training loss on iteration 4260 = 0.8778085231781005
Training loss on iteration 4280 = 0.6930037349462509
Training loss on iteration 4300 = 0.8354684367775917
Training loss on iteration 4320 = 0.525039953738451
Training loss on iteration 4340 = 0.6642281018197537
Training loss on iteration 4360 = 0.6781308948993683
Training loss on iteration 4380 = 0.5941051058471203
Training loss on iteration 4400 = 0.5988822519779206
Training loss on iteration 4420 = 0.6415368616580963
Training loss on iteration 4440 = 0.6257001116871834
Training loss on iteration 4460 = 0.6236584931612015
Training loss on iteration 4480 = 0.5725147195160389
Training loss on iteration 4500 = 0.6661766439676284
Training loss on iteration 4520 = 0.7258941918611527
Training loss on iteration 4540 = 0.6440593264997005
Training loss on iteration 4560 = 0.568627493083477
Training loss on iteration 4580 = 0.8897846326231956
Training loss on iteration 4600 = 0.6200898423790931
Training loss on iteration 4620 = 0.617596012353897
Training loss on iteration 4640 = 0.6438905134797096
Training loss on iteration 4660 = 0.7954765826463699
Training loss on iteration 4680 = 0.8108723312616348
Training loss on iteration 4700 = 0.5959519729018211
Training loss on iteration 4720 = 0.7883201494812966
Training loss on iteration 4740 = 0.5782829955220222
Training loss on iteration 4760 = 0.5346661105751991
Training loss on iteration 4780 = 0.6440525613725185
Training loss on iteration 4800 = 0.5898039676249027
Training loss on iteration 4820 = 0.7897920094430446
Training loss on iteration 4840 = 0.6305390231311321
Training loss on iteration 4860 = 0.646094536781311
Training loss on iteration 4880 = 0.6275519356131554
Training loss on iteration 4900 = 0.5266848236322403
Training loss on iteration 4920 = 0.6788576759397984
Training loss on iteration 4940 = 0.6553874976933003
Training loss on iteration 4960 = 0.7320220150053501
Training loss on iteration 4980 = 0.6224858522415161
Training loss on iteration 5000 = 0.6688332334160805
Training loss on iteration 5020 = 0.7298489674925804
Training loss on iteration 5040 = 0.7515069089829922
Training loss on iteration 5060 = 0.5802887238562107
Training loss on iteration 5080 = 0.4686615355312824
Training loss on iteration 5100 = 0.6765413627028465
Training loss on iteration 5120 = 0.6214366957545281
Training loss on iteration 5140 = 0.5427647307515144
Training loss on iteration 5160 = 0.6474202737212181
Training loss on iteration 5180 = 0.5511545702815056
Training loss on iteration 5200 = 0.7528991386294365
Training loss on iteration 5220 = 0.6069132491946221
Training loss on iteration 5240 = 0.5940703324973583
Training loss on iteration 5260 = 0.6482478752732277
Training loss on iteration 5280 = 0.5964421510696412
Training loss on iteration 5300 = 0.6122765623033046
Training loss on iteration 5320 = 0.5277982264757156
Training loss on iteration 5340 = 0.6065175279974937
Training loss on iteration 5360 = 0.5096782490611076
Training loss on iteration 5380 = 0.5037124752998352
Training loss on iteration 5400 = 0.55157515630126
Training loss on iteration 5420 = 0.5717212170362472
Training loss on iteration 5440 = 0.5692042917013168
Training loss on iteration 5460 = 0.6398531787097455
Training loss on iteration 5480 = 0.7410930715501308
Training loss on iteration 5500 = 0.588312865793705
Training loss on iteration 5520 = 0.6483699575066566
Training loss on iteration 5540 = 0.49861165434122084
Training loss on iteration 5560 = 0.7666860684752465
Training loss on iteration 5580 = 0.8294716157019139
Training loss on iteration 5600 = 0.6261415630578995
Training loss on iteration 5620 = 0.576646837592125
Training loss on iteration 5640 = 0.571305388957262
Training loss on iteration 5660 = 0.6407686069607734
Training loss on iteration 5680 = 0.6303586453199387
Training loss on iteration 5700 = 0.6680482476949692
Training loss on iteration 5720 = 0.7107410907745362
Training loss on iteration 5740 = 0.5980595365166664
Training loss on iteration 5760 = 0.6159750133752823
Training loss on iteration 5780 = 0.6683719374239445
Training loss on iteration 5800 = 0.6750107109546661
Training loss on iteration 5820 = 0.5587769187986851
Training loss on iteration 5840 = 0.6717572823166847
Training loss on iteration 5860 = 0.5472378209233284
Training loss on iteration 5880 = 0.6003527186810971
Training loss on iteration 5900 = 0.6523603841662406
Training loss on iteration 5920 = 0.6396593660116195
Training loss on iteration 5940 = 0.5309974282979966
Training loss on iteration 5960 = 0.658833210170269
Training loss on iteration 5980 = 0.5570076107978821
Training loss on iteration 6000 = 0.6441132351756096
Training loss on iteration 6020 = 0.8370699614286423
Training loss on iteration 6040 = 0.7624781861901283
Training loss on iteration 6060 = 0.6864567629992961
Training loss on iteration 6080 = 0.6289484962821007
Training loss on iteration 6100 = 0.5337046250700951
Training loss on iteration 6120 = 0.5931588932871819
Training loss on iteration 6140 = 0.9065485388040543
Training loss on iteration 6160 = 0.6893496930599212
Training loss on iteration 6180 = 0.5959313280880452
Training loss on iteration 6200 = 0.4334333322942257
Training loss on iteration 6220 = 0.5322925001382828
Training loss on iteration 6240 = 0.7280641198158264
Training loss on iteration 6260 = 0.5095005728304386
Training loss on iteration 6280 = 0.513600479438901
Training loss on iteration 6300 = 0.6144437819719315
Training loss on iteration 6320 = 0.6259080946445466
Training loss on iteration 6340 = 0.6741258859634399
Training loss on iteration 6360 = 0.7233593836426735
Training loss on iteration 6380 = 0.5353459782898426
Training loss on iteration 0 = 0.32485973834991455
Training loss on iteration 20 = 0.6768560454249382
Training loss on iteration 40 = 0.5178355030715466
Training loss on iteration 60 = 0.5866346068680286
Training loss on iteration 80 = 0.6391191057860851
Training loss on iteration 100 = 0.7064722225069999
Training loss on iteration 120 = 0.6973838478326797
Training loss on iteration 140 = 0.7207350380718708
Training loss on iteration 160 = 0.8085242331027984
Training loss on iteration 180 = 0.6410297684371471
Training loss on iteration 200 = 0.6233277939260006
Training loss on iteration 220 = 0.591308856755495
Training loss on iteration 240 = 0.6043083220720291
Training loss on iteration 260 = 0.5736152932047844
Training loss on iteration 280 = 0.6105668500065804
Training loss on iteration 300 = 0.5664180397987366
Training loss on iteration 320 = 0.6869944043457508
Training loss on iteration 340 = 0.46678859889507296
Training loss on iteration 360 = 0.47601968944072726
Training loss on iteration 380 = 0.7290385290980339
Training loss on iteration 400 = 0.5680727325379848
Training loss on iteration 420 = 0.5374985449016094
Training loss on iteration 440 = 0.5250580839812755
Training loss on iteration 460 = 0.7180874429643154
Training loss on iteration 480 = 0.5017074584960938
Training loss on iteration 500 = 0.5386447601020337
Training loss on iteration 520 = 0.59501928165555
Training loss on iteration 540 = 0.5750020660459996
Training loss on iteration 560 = 0.5286710783839226
Training loss on iteration 580 = 0.7620513007044792
Training loss on iteration 600 = 0.48984069377183914
Training loss on iteration 620 = 0.5877843387424946
Training loss on iteration 640 = 0.5869089603424072
Training loss on iteration 660 = 0.6636611118912696
Training loss on iteration 680 = 0.4682542055845261
Training loss on iteration 700 = 0.4688104510307312
Training loss on iteration 720 = 0.5823775947093963
Training loss on iteration 740 = 0.6291993238031864
Training loss on iteration 760 = 0.5454320497810841
Training loss on iteration 780 = 0.4655433870851994
Training loss on iteration 800 = 0.6124975226819516
Training loss on iteration 820 = 0.5583034671843052
Training loss on iteration 840 = 0.5693111017346382
Training loss on iteration 860 = 0.46525310501456263
Training loss on iteration 880 = 0.5066348284482955
Training loss on iteration 900 = 0.7086080573499203
Training loss on iteration 920 = 0.5133393190801143
Training loss on iteration 940 = 0.515984695404768
Training loss on iteration 960 = 0.5820130795240402
Training loss on iteration 980 = 0.5623622104525566
Training loss on iteration 1000 = 0.5346295967698097
Training loss on iteration 1020 = 0.5592455513775348
Training loss on iteration 1040 = 0.5214364677667618
Training loss on iteration 1060 = 0.5802840389311313
Training loss on iteration 1080 = 0.4767274968326092
Training loss on iteration 1100 = 0.6004398576915264
Training loss on iteration 1120 = 0.5024997420608998
Training loss on iteration 1140 = 0.6355582818388938
Training loss on iteration 1160 = 0.5911354184150696
Training loss on iteration 1180 = 0.5670666359364986
Training loss on iteration 1200 = 0.5751145370304585
Training loss on iteration 1220 = 0.455609480291605
Training loss on iteration 1240 = 0.609439442306757
Training loss on iteration 1260 = 0.739873594045639
Training loss on iteration 1280 = 0.8235542893409729
Training loss on iteration 1300 = 0.4996168062090874
Training loss on iteration 1320 = 0.5438657306134701
Training loss on iteration 1340 = 0.5421343646943569
Training loss on iteration 1360 = 0.5948902003467083
Training loss on iteration 1380 = 0.4639257490634918
Training loss on iteration 1400 = 0.5446468807756901
Training loss on iteration 1420 = 0.5152911812067031
Training loss on iteration 1440 = 0.544964087754488
Training loss on iteration 1460 = 0.7096277579665184
Training loss on iteration 1480 = 0.539136528968811
Training loss on iteration 1500 = 0.5307132609188556
Training loss on iteration 1520 = 0.5523463539779186
Training loss on iteration 1540 = 0.5790761440992356
Training loss on iteration 1560 = 0.6458066448569297
Training loss on iteration 1580 = 0.6353872448205948
Training loss on iteration 1600 = 0.5722715564072132
Training loss on iteration 1620 = 0.5678828343749046
Training loss on iteration 1640 = 0.6150922566652298
Training loss on iteration 1660 = 0.5170439466834068
Training loss on iteration 1680 = 0.6220314353704453
Training loss on iteration 1700 = 0.48306119740009307
Training loss on iteration 1720 = 0.6290855824947357
Training loss on iteration 1740 = 0.6146487802267074
Training loss on iteration 1760 = 0.6955855518579483
Training loss on iteration 1780 = 0.5718964576721192
Training loss on iteration 1800 = 0.7013383984565735
Training loss on iteration 1820 = 0.709177377820015
Training loss on iteration 1840 = 0.7047132685780525
Training loss on iteration 1860 = 0.5740535445511341
Training loss on iteration 1880 = 0.5352306306362152
Training loss on iteration 1900 = 0.5912500649690628
Training loss on iteration 1920 = 0.4832084760069847
Training loss on iteration 1940 = 0.4922467604279518
Training loss on iteration 1960 = 0.5490945436060428
Training loss on iteration 1980 = 0.5962624207139016
Training loss on iteration 2000 = 0.717689472436905
Training loss on iteration 2020 = 0.6098730996251106
Training loss on iteration 2040 = 0.6536490552127361
Training loss on iteration 2060 = 0.6174006715416909
Training loss on iteration 2080 = 0.8206169858574868
Training loss on iteration 2100 = 0.7189812019467354
Training loss on iteration 2120 = 0.6443835765123367
Training loss on iteration 2140 = 0.7959897980093956
Training loss on iteration 2160 = 0.6607957035303116
Training loss on iteration 2180 = 0.6916264399886132
Training loss on iteration 2200 = 0.5516253605484962
Training loss on iteration 2220 = 0.61063242405653
Training loss on iteration 2240 = 0.6683991238474846
Training loss on iteration 2260 = 0.6557473242282867
Training loss on iteration 2280 = 0.6584314584732056
Training loss on iteration 2300 = 0.5480960465967655
Training loss on iteration 2320 = 0.6817562013864518
Training loss on iteration 2340 = 0.6298391200602055
Training loss on iteration 2360 = 0.6409300908446312
Training loss on iteration 2380 = 0.7144378177821636
Training loss on iteration 2400 = 0.5820448942482471
Training loss on iteration 2420 = 0.6797435864806175
Training loss on iteration 2440 = 0.5921784207224846
Training loss on iteration 2460 = 0.7238313585519791
Training loss on iteration 2480 = 0.7780149400234222
Training loss on iteration 2500 = 0.7838345721364022
Training loss on iteration 2520 = 0.7832335248589516
Training loss on iteration 2540 = 0.6830092832446099
Training loss on iteration 2560 = 0.648704494535923
Training loss on iteration 2580 = 0.5537298299372196
Training loss on iteration 2600 = 0.5996020555496215
Training loss on iteration 2620 = 0.6492042362689971
Training loss on iteration 2640 = 0.6342308044433593
Training loss on iteration 2660 = 0.6744948789477349
Training loss on iteration 2680 = 0.4952380836009979
Training loss on iteration 2700 = 0.7703637585043908
Training loss on iteration 2720 = 0.6410081863403321
Training loss on iteration 2740 = 0.6926246844232082
Training loss on iteration 2760 = 0.6600122541189194
Training loss on iteration 2780 = 0.6983103550970554
Training loss on iteration 2800 = 0.6272661805152893
Training loss on iteration 2820 = 0.6134478569030761
Training loss on iteration 2840 = 0.6637657694518566
Training loss on iteration 2860 = 0.5848860293626785
Training loss on iteration 2880 = 0.5941282659769058
Training loss on iteration 2900 = 0.530346317589283
Training loss on iteration 2920 = 0.5996137797832489
Training loss on iteration 2940 = 0.729952135682106
Training loss on iteration 2960 = 0.5372821271419526
Training loss on iteration 2980 = 0.6804120540618896
Training loss on iteration 3000 = 0.7865917190909386
Training loss on iteration 3020 = 0.6499190010130406
Training loss on iteration 3040 = 0.7067652605473995
Training loss on iteration 3060 = 0.6966490045189857
Training loss on iteration 3080 = 0.700799597799778
Training loss on iteration 3100 = 0.6450687296688556
Training loss on iteration 3120 = 0.7162483185529709
Training loss on iteration 3140 = 0.5785298258066177
Training loss on iteration 3160 = 0.5888394571840763
Training loss on iteration 3180 = 0.6166970260441303
Training loss on iteration 3200 = 0.5507990658283234
Training loss on iteration 3220 = 0.6506847649812698
Training loss on iteration 3240 = 0.7813299886882306
Training loss on iteration 3260 = 0.5173327974975109
Training loss on iteration 3280 = 0.7501799300312996
Training loss on iteration 3300 = 0.6510841943323612
Training loss on iteration 3320 = 0.5291859477758407
Training loss on iteration 3340 = 0.5806335441768169
Training loss on iteration 3360 = 0.5667046837508678
Training loss on iteration 3380 = 0.7956154152750969
Training loss on iteration 3400 = 0.5882095634937287
Training loss on iteration 3420 = 0.663637624680996
Training loss on iteration 3440 = 0.6004200413823128
Training loss on iteration 3460 = 0.6097096182405949
Training loss on iteration 3480 = 0.6256723463535309
Training loss on iteration 3500 = 0.6832354709506034
Training loss on iteration 3520 = 0.5931894704699516
Training loss on iteration 3540 = 0.7467243149876595
Training loss on iteration 3560 = 0.6263549350202083
Training loss on iteration 3580 = 0.6510883301496506
Training loss on iteration 3600 = 0.5499936819076539
Training loss on iteration 3620 = 0.6394242852926254
Training loss on iteration 3640 = 0.5994790628552437
Training loss on iteration 3660 = 0.6807622030377388
Training loss on iteration 3680 = 0.5930367991328239
Training loss on iteration 3700 = 0.6329267293214798
Training loss on iteration 3720 = 0.6226922735571861
Training loss on iteration 3740 = 0.7099283263087273
Training loss on iteration 3760 = 0.5948571398854255
Training loss on iteration 3780 = 0.7623558759689331
Training loss on iteration 3800 = 0.6399806752800942
Training loss on iteration 3820 = 0.5775608956813812
Training loss on iteration 3840 = 0.5541108548641205
Training loss on iteration 3860 = 0.6002478539943695
Training loss on iteration 3880 = 0.6842813372612
Training loss on iteration 3900 = 0.7585410289466381
Training loss on iteration 3920 = 0.6134840190410614
Training loss on iteration 3940 = 0.5468139126896858
Training loss on iteration 3960 = 0.7368889838457108
Training loss on iteration 3980 = 0.6429988369345665
Training loss on iteration 4000 = 0.6047968804836273
Training loss on iteration 4020 = 0.6651140600442886
Training loss on iteration 4040 = 0.5343535758554936
Training loss on iteration 4060 = 0.4792239010334015
Training loss on iteration 4080 = 0.6631692513823509
Training loss on iteration 4100 = 0.647541930526495
Training loss on iteration 4120 = 0.6149515643715858
Training loss on iteration 4140 = 0.6405794516205787
Training loss on iteration 4160 = 0.6142519429326058
Training loss on iteration 4180 = 0.6865203619003296
Training loss on iteration 4200 = 0.6499574169516563
Training loss on iteration 4220 = 0.5354288958013058
Training loss on iteration 4240 = 0.6233832687139511
Training loss on iteration 4260 = 0.774535559117794
Training loss on iteration 4280 = 0.5932124093174934
Training loss on iteration 4300 = 0.525889153778553
Training loss on iteration 4320 = 0.6989707872271538
Training loss on iteration 4340 = 0.46145316660404206
Training loss on iteration 4360 = 0.6583567410707474
Training loss on iteration 4380 = 0.7105566754937171
Training loss on iteration 4400 = 0.5922779724001884
Training loss on iteration 4420 = 0.6229732438921929
Training loss on iteration 4440 = 0.564022234082222
Training loss on iteration 4460 = 0.7128375858068466
Training loss on iteration 4480 = 0.6286805316805839
Training loss on iteration 4500 = 0.6427787967026234
Training loss on iteration 4520 = 0.6101716443896293
Training loss on iteration 4540 = 0.8363066248595714
Training loss on iteration 4560 = 0.5579538270831108
Training loss on iteration 4580 = 0.5843507036566734
Training loss on iteration 4600 = 0.5662427343428135
Training loss on iteration 4620 = 0.6474932625889778
Training loss on iteration 4640 = 0.5046603195369244
Training loss on iteration 4660 = 0.5256860516965389
Training loss on iteration 4680 = 0.5372525468468666
Training loss on iteration 4700 = 0.559755739569664
Training loss on iteration 4720 = 0.6594924226403236
Training loss on iteration 4740 = 0.6519386053085328
Training loss on iteration 4760 = 0.5012326516211033
Training loss on iteration 4780 = 0.4492662146687508
Training loss on iteration 4800 = 0.5518189907073975
Training loss on iteration 4820 = 0.7469768732786178
Training loss on iteration 4840 = 0.6475324019789696
Training loss on iteration 4860 = 0.5579975582659245
Training loss on iteration 4880 = 0.6478732623159885
Training loss on iteration 4900 = 0.6045991629362106
Training loss on iteration 4920 = 0.733219786733389
Training loss on iteration 4940 = 0.5778337925672531
Training loss on iteration 4960 = 0.5257640048861504
Training loss on iteration 4980 = 0.6191039860248566
Training loss on iteration 5000 = 0.7249621585011482
Training loss on iteration 5020 = 0.7926779508590698
Training loss on iteration 5040 = 0.6602043263614178
Training loss on iteration 5060 = 0.5937432266771794
Training loss on iteration 5080 = 0.526899267733097
Training loss on iteration 5100 = 0.6676778577268123
Training loss on iteration 5120 = 0.5404253333806992
Training loss on iteration 5140 = 0.5396001532673835
Training loss on iteration 5160 = 0.5035890594124794
Training loss on iteration 5180 = 0.6095447458326817
Training loss on iteration 5200 = 0.5495314754545688
Training loss on iteration 5220 = 0.6892051473259926
Training loss on iteration 5240 = 0.6388768672943115
Training loss on iteration 5260 = 0.6853557735681534
Training loss on iteration 5280 = 0.6143807508051395
Training loss on iteration 5300 = 0.6122859068214893
Training loss on iteration 5320 = 0.6030555933713913
Training loss on iteration 5340 = 0.6101468056440353
Training loss on iteration 5360 = 0.6302546545863151
Training loss on iteration 5380 = 0.45713394656777384
Training loss on iteration 5400 = 0.4852456346154213
Training loss on iteration 5420 = 0.5831570260226726
Training loss on iteration 5440 = 0.6194985508918762
Training loss on iteration 5460 = 0.7070892706513405
Training loss on iteration 5480 = 0.5955178081989289
Training loss on iteration 5500 = 0.6837890028953553
Training loss on iteration 5520 = 0.5835334636271
Training loss on iteration 5540 = 0.5921218305826187
Training loss on iteration 5560 = 0.5612631276249885
Training loss on iteration 5580 = 0.6402988985180855
Training loss on iteration 5600 = 0.6833213195204735
Training loss on iteration 5620 = 0.47799992710351946
Training loss on iteration 5640 = 0.5451322838664054
Training loss on iteration 5660 = 0.6419214196503162
Training loss on iteration 5680 = 0.8125606074929237
Training loss on iteration 5700 = 0.552811237424612
Training loss on iteration 5720 = 0.8569584384560585
Training loss on iteration 5740 = 0.5730002537369728
Training loss on iteration 5760 = 0.589746730029583
Training loss on iteration 5780 = 0.671394607424736
Training loss on iteration 5800 = 0.5240618467330933
Training loss on iteration 5820 = 0.5929544478654861
Training loss on iteration 5840 = 0.4760818719863892
Training loss on iteration 5860 = 0.5196988426148892
Training loss on iteration 5880 = 0.6445185460150242
Training loss on iteration 5900 = 0.5548694223165512
Training loss on iteration 5920 = 0.5753774121403694
Training loss on iteration 5940 = 0.5650725826621056
Training loss on iteration 5960 = 0.6979988843202591
Training loss on iteration 5980 = 0.7327625840902329
Training loss on iteration 6000 = 0.6195588037371635
Training loss on iteration 6020 = 0.5762544684112072
Training loss on iteration 6040 = 0.5770778074860573
Training loss on iteration 6060 = 0.7127490520477295
Training loss on iteration 6080 = 0.705543790757656
Training loss on iteration 6100 = 0.6093261830508709
Training loss on iteration 6120 = 0.616336777806282
Training loss on iteration 6140 = 0.612071817368269
Training loss on iteration 6160 = 0.6818884193897248
Training loss on iteration 6180 = 0.6373156011104584
Training loss on iteration 6200 = 0.5838694751262665
Training loss on iteration 6220 = 0.5142810046672821
Training loss on iteration 6240 = 0.777116084843874
Training loss on iteration 6260 = 0.6195429354906082
Training loss on iteration 6280 = 0.5198752470314503
Training loss on iteration 6300 = 0.609723062813282
Training loss on iteration 6320 = 0.5134112574160099
Training loss on iteration 6340 = 0.5205264739692211
Training loss on iteration 6360 = 0.6568774566054344
Training loss on iteration 6380 = 0.6488091349601746
Training loss on iteration 0 = 0.46823784708976746
Training loss on iteration 20 = 0.5552609369158745
Training loss on iteration 40 = 0.5186905227601528
Training loss on iteration 60 = 0.5427224986255169
Training loss on iteration 80 = 0.4703051924705505
Training loss on iteration 100 = 0.5251928046345711
Training loss on iteration 120 = 0.4744566775858402
Training loss on iteration 140 = 0.5901607200503349
Training loss on iteration 160 = 0.6515340350568295
Training loss on iteration 180 = 0.5100268542766571
Training loss on iteration 200 = 0.6010956548154354
Training loss on iteration 220 = 0.7019580326974392
Training loss on iteration 240 = 0.619537216424942
Training loss on iteration 260 = 0.5915711596608162
Training loss on iteration 280 = 0.6026437550783157
Training loss on iteration 300 = 0.6016030609607697
Training loss on iteration 320 = 0.5857098400592804
Training loss on iteration 340 = 0.7120481908321381
Training loss on iteration 360 = 0.6340464204549789
Training loss on iteration 380 = 0.50670046210289
Training loss on iteration 400 = 0.6380500979721546
Training loss on iteration 420 = 0.6639749102294445
Training loss on iteration 440 = 0.48687673956155775
Training loss on iteration 460 = 0.54131860435009
Training loss on iteration 480 = 0.5403797589242458
Training loss on iteration 500 = 0.6172279253602028
Training loss on iteration 520 = 0.6101743698120117
Training loss on iteration 540 = 0.5704785592854023
Training loss on iteration 560 = 0.5996444538235665
Training loss on iteration 580 = 0.7286012724041939
Training loss on iteration 600 = 0.5732846967875957
Training loss on iteration 620 = 0.5276988789439201
Training loss on iteration 640 = 0.6111975148320198
Training loss on iteration 660 = 0.7043188005685806
Training loss on iteration 680 = 0.4783949337899685
Training loss on iteration 700 = 0.5610536716878414
Training loss on iteration 720 = 0.549025409668684
Training loss on iteration 740 = 0.6546413078904152
Training loss on iteration 760 = 0.6643619403243065
Training loss on iteration 780 = 0.4828866206109524
Training loss on iteration 800 = 0.620634351670742
Training loss on iteration 820 = 0.6952772505581379
Training loss on iteration 840 = 0.6722012817859649
Training loss on iteration 860 = 0.7004677258431912
Training loss on iteration 880 = 0.6101448483765125
Training loss on iteration 900 = 0.5077825054526329
Training loss on iteration 920 = 0.6231103554368019
Training loss on iteration 940 = 0.4950013317167759
Training loss on iteration 960 = 0.6984971232712269
Training loss on iteration 980 = 0.5469326816499234
Training loss on iteration 1000 = 0.579933075606823
Training loss on iteration 1020 = 0.5947065241634846
Training loss on iteration 1040 = 0.5430662266910076
Training loss on iteration 1060 = 0.5790412358939647
Training loss on iteration 1080 = 0.47916609421372414
Training loss on iteration 1100 = 0.53781224116683
Training loss on iteration 1120 = 0.6007829278707504
Training loss on iteration 1140 = 0.481849279999733
Training loss on iteration 1160 = 0.4865607813000679
Training loss on iteration 1180 = 0.5313390113413334
Training loss on iteration 1200 = 0.5123674869537354
Training loss on iteration 1220 = 0.5121474675834179
Training loss on iteration 1240 = 0.5111065231263637
Training loss on iteration 1260 = 0.47833077013492586
Training loss on iteration 1280 = 0.534913070499897
Training loss on iteration 1300 = 0.5491890534758568
Training loss on iteration 1320 = 0.5600731641054153
Training loss on iteration 1340 = 0.6635934218764306
Training loss on iteration 1360 = 0.5576964810490608
Training loss on iteration 1380 = 0.45891531705856325
Training loss on iteration 1400 = 0.5859491355717182
Training loss on iteration 1420 = 0.5240905985236168
Training loss on iteration 1440 = 0.6015648864209652
Training loss on iteration 1460 = 0.5257789842784405
Training loss on iteration 1480 = 0.7692820057272911
Training loss on iteration 1500 = 0.7292657800018787
Training loss on iteration 1520 = 0.5730958700180053
Training loss on iteration 1540 = 0.6385986991226673
Training loss on iteration 1560 = 0.5768522597849369
Training loss on iteration 1580 = 0.6538006708025932
Training loss on iteration 1600 = 0.6893821462988854
Training loss on iteration 1620 = 0.5629649370908737
Training loss on iteration 1640 = 0.5758166387677193
Training loss on iteration 1660 = 0.6303703427314759
Training loss on iteration 1680 = 0.5998387254774571
Training loss on iteration 1700 = 0.6223324075341224
Training loss on iteration 1720 = 0.47048499807715416
Training loss on iteration 1740 = 0.4726031251251698
Training loss on iteration 1760 = 0.5178006455302239
Training loss on iteration 1780 = 0.5557258039712906
Training loss on iteration 1800 = 0.5335762538015842
Training loss on iteration 1820 = 0.5637610167264938
Training loss on iteration 1840 = 0.5409879982471466
Training loss on iteration 1860 = 0.6384673871099948
Training loss on iteration 1880 = 0.6649773359298706
Training loss on iteration 1900 = 0.447295955568552
Training loss on iteration 1920 = 0.5705714046955108
Training loss on iteration 1940 = 0.7071701288223267
Training loss on iteration 1960 = 0.6454956904053688
Training loss on iteration 1980 = 0.5852275893092156
Training loss on iteration 2000 = 0.5185368984937668
Training loss on iteration 2020 = 0.5794268012046814
Training loss on iteration 2040 = 0.49156147688627244
Training loss on iteration 2060 = 0.6033785969018937
Training loss on iteration 2080 = 0.6368648506700992
Training loss on iteration 2100 = 0.5308151975274086
Training loss on iteration 2120 = 0.4811283811926842
Training loss on iteration 2140 = 0.47177092656493186
Training loss on iteration 2160 = 0.4933367483317852
Training loss on iteration 2180 = 0.7027058020234108
Training loss on iteration 2200 = 0.7214271202683449
Training loss on iteration 2220 = 0.5273986019194126
Training loss on iteration 2240 = 0.709968377649784
Training loss on iteration 2260 = 0.7184073761105537
Training loss on iteration 2280 = 0.6082712329924107
Training loss on iteration 2300 = 0.4881721258163452
Training loss on iteration 2320 = 0.49800874590873717
Training loss on iteration 2340 = 0.5244157172739505
Training loss on iteration 2360 = 0.559096959233284
Training loss on iteration 2380 = 0.5316333070397377
Training loss on iteration 2400 = 0.5221361935138702
Training loss on iteration 2420 = 0.5554196655750274
Training loss on iteration 2440 = 0.6282373473048211
Training loss on iteration 2460 = 0.6835856780409812
Training loss on iteration 2480 = 0.7720202043652534
Training loss on iteration 2500 = 0.8352118343114853
Training loss on iteration 2520 = 0.6296212919056415
Training loss on iteration 2540 = 0.6127619318664074
Training loss on iteration 2560 = 0.660211369395256
Training loss on iteration 2580 = 0.49897887110710143
Training loss on iteration 2600 = 0.5457766212522983
Training loss on iteration 2620 = 0.6631661385297776
Training loss on iteration 2640 = 0.5796298816800117
Training loss on iteration 2660 = 0.6207587234675884
Training loss on iteration 2680 = 0.6168957874178886
Training loss on iteration 2700 = 0.6998921483755112
Training loss on iteration 2720 = 0.7003060102462768
Training loss on iteration 2740 = 0.4654519855976105
Training loss on iteration 2760 = 0.6393551334738732
Training loss on iteration 2780 = 0.6175431206822395
Training loss on iteration 2800 = 0.6280470967292786
Training loss on iteration 2820 = 0.5607528202235699
Training loss on iteration 2840 = 0.7020868882536888
Training loss on iteration 2860 = 0.6006460949778557
Training loss on iteration 2880 = 0.6779039949178696
Training loss on iteration 2900 = 0.5604548349976539
Training loss on iteration 2920 = 0.6551876172423363
Training loss on iteration 2940 = 0.6161782510578633
Training loss on iteration 2960 = 0.6645942166447639
Training loss on iteration 2980 = 0.5897035330533982
Training loss on iteration 3000 = 0.5341522738337516
Training loss on iteration 3020 = 0.666880302131176
Training loss on iteration 3040 = 0.630841338634491
Training loss on iteration 3060 = 0.5626104205846787
Training loss on iteration 3080 = 0.5800488501787185
Training loss on iteration 3100 = 0.579590181261301
Training loss on iteration 3120 = 0.5708842232823372
Training loss on iteration 3140 = 0.44255822002887724
Training loss on iteration 3160 = 0.5970991864800453
Training loss on iteration 3180 = 0.6990975864231587
Training loss on iteration 3200 = 0.6246277436614036
Training loss on iteration 3220 = 0.6733351409435272
Training loss on iteration 3240 = 0.6309764802455902
Training loss on iteration 3260 = 0.8144033119082451
Training loss on iteration 3280 = 0.608076986670494
Training loss on iteration 3300 = 0.6272922918200493
Training loss on iteration 3320 = 0.523090997338295
Training loss on iteration 3340 = 0.7149496600031853
Training loss on iteration 3360 = 0.6992850676178932
Training loss on iteration 3380 = 0.5839309737086296
Training loss on iteration 3400 = 0.61597715690732
Training loss on iteration 3420 = 0.6211211256682873
Training loss on iteration 3440 = 0.6935072496533394
Training loss on iteration 3460 = 0.7133931286633015
Training loss on iteration 3480 = 0.6625518828630448
Training loss on iteration 3500 = 0.5803385697305202
Training loss on iteration 3520 = 0.5373413503170014
Training loss on iteration 3540 = 0.740842679142952
Training loss on iteration 3560 = 0.560654953122139
Training loss on iteration 3580 = 0.581429586559534
Training loss on iteration 3600 = 0.587667403370142
Training loss on iteration 3620 = 0.6391243085265159
Training loss on iteration 3640 = 0.5674560308456421
Training loss on iteration 3660 = 0.7831561416387558
Training loss on iteration 3680 = 0.6479923665523529
Training loss on iteration 3700 = 0.4931897483766079
Training loss on iteration 3720 = 0.4737575024366379
Training loss on iteration 3740 = 0.5274905636906624
Training loss on iteration 3760 = 0.5463643841445446
Training loss on iteration 3780 = 0.6478406362235546
Training loss on iteration 3800 = 0.5458757907152176
Training loss on iteration 3820 = 0.6340650744736195
Training loss on iteration 3840 = 0.8036766991019249
Training loss on iteration 3860 = 0.5421129807829856
Training loss on iteration 3880 = 0.5955368049442769
Training loss on iteration 3900 = 0.5084777310490608
Training loss on iteration 3920 = 0.5760520242154599
Training loss on iteration 3940 = 0.6040384843945503
Training loss on iteration 3960 = 0.5943800911307335
Training loss on iteration 3980 = 0.6682440042495728
Training loss on iteration 4000 = 0.5227365709841252
Training loss on iteration 4020 = 0.6690463915467262
Training loss on iteration 4040 = 0.57339399009943
Training loss on iteration 4060 = 0.5160539977252483
Training loss on iteration 4080 = 0.6164136782288552
Training loss on iteration 4100 = 0.6742997005581856
Training loss on iteration 4120 = 0.5215170577168464
Training loss on iteration 4140 = 0.6569056183099746
Training loss on iteration 4160 = 0.5839998006820679
Training loss on iteration 4180 = 0.6474102273583412
Training loss on iteration 4200 = 0.6157459557056427
Training loss on iteration 4220 = 0.5801274321973324
Training loss on iteration 4240 = 0.6295881204307079
Training loss on iteration 4260 = 0.5800853267312049
Training loss on iteration 4280 = 0.5732606425881386
Training loss on iteration 4300 = 0.5304936289787292
Training loss on iteration 4320 = 0.4510627530515194
Training loss on iteration 4340 = 0.6538069903850555
Training loss on iteration 4360 = 0.6039568580687046
Training loss on iteration 4380 = 0.4869943141937256
Training loss on iteration 4400 = 0.5057982906699181
Training loss on iteration 4420 = 0.5941581919789314
Training loss on iteration 4440 = 0.6653943702578544
Training loss on iteration 4460 = 0.6248812839388848
Training loss on iteration 4480 = 0.5206769146025181
Training loss on iteration 4500 = 0.6489750146865845
Training loss on iteration 4520 = 0.5679387986660004
Training loss on iteration 4540 = 0.6263106167316437
Training loss on iteration 4560 = 0.9109359115362168
Training loss on iteration 4580 = 0.43878659456968305
Training loss on iteration 4600 = 0.6145351067185402
Training loss on iteration 4620 = 0.7219087705016136
Training loss on iteration 4640 = 0.5970734670758248
Training loss on iteration 4660 = 0.6671347290277481
Training loss on iteration 4680 = 0.4685344502329826
Training loss on iteration 4700 = 0.5363826096057892
Training loss on iteration 4720 = 0.5098508723080158
Training loss on iteration 4740 = 0.585208410024643
Training loss on iteration 4760 = 0.6113410778343678
Training loss on iteration 4780 = 0.5852784082293511
Training loss on iteration 4800 = 0.4728384740650654
Training loss on iteration 4820 = 0.7306317955255508
Training loss on iteration 4840 = 0.5951210841536522
Training loss on iteration 4860 = 0.5290456168353558
Training loss on iteration 4880 = 0.5916003890335559
Training loss on iteration 4900 = 0.5842025764286518
Training loss on iteration 4920 = 0.622895885258913
Training loss on iteration 4940 = 0.5002643197774888
Training loss on iteration 4960 = 0.5180425748229027
Training loss on iteration 4980 = 0.7319303639233112
Training loss on iteration 5000 = 0.5681964084506035
Training loss on iteration 5020 = 0.592890290170908
Training loss on iteration 5040 = 0.622071135789156
Training loss on iteration 5060 = 0.6594830550253391
Training loss on iteration 5080 = 0.5253667637705803
Training loss on iteration 5100 = 0.47617322131991385
Training loss on iteration 5120 = 0.6614478170871735
Training loss on iteration 5140 = 0.7505733758211136
Training loss on iteration 5160 = 0.6321828246116639
Training loss on iteration 5180 = 0.635370397567749
Training loss on iteration 5200 = 0.7218908578157425
Training loss on iteration 5220 = 0.6356543935835361
Training loss on iteration 5240 = 0.6220881313085556
Training loss on iteration 5260 = 0.61601397767663
Training loss on iteration 5280 = 0.5194159865379333
Training loss on iteration 5300 = 0.5159401834011078
Training loss on iteration 5320 = 0.5823260463774205
Training loss on iteration 5340 = 0.7510834008455276
Training loss on iteration 5360 = 0.565290367603302
Training loss on iteration 5380 = 0.6643110521137714
Training loss on iteration 5400 = 0.5397889539599419
Training loss on iteration 5420 = 0.6099037729203701
Training loss on iteration 5440 = 0.5229546293616295
Training loss on iteration 5460 = 0.6189958393573761
Training loss on iteration 5480 = 0.5448589883744717
Training loss on iteration 5500 = 0.6701581597328186
Training loss on iteration 5520 = 0.5181852862238884
Training loss on iteration 5540 = 0.5660591721534729
Training loss on iteration 5560 = 0.6360922992229462
Training loss on iteration 5580 = 0.6587219454348088
Training loss on iteration 5600 = 0.7279221042990685
Training loss on iteration 5620 = 0.6997804999351501
Training loss on iteration 5640 = 0.6740375578403472
Training loss on iteration 5660 = 0.6143106251955033
Training loss on iteration 5680 = 0.5659430459141731
Training loss on iteration 5700 = 0.6202526390552521
Training loss on iteration 5720 = 0.6643061377108097
Training loss on iteration 5740 = 0.6025551900267601
Training loss on iteration 5760 = 0.4717922851443291
Training loss on iteration 5780 = 0.6500178411602974
Training loss on iteration 5800 = 0.51268822401762
Training loss on iteration 5820 = 0.539646702259779
Training loss on iteration 5840 = 0.6014601692557335
Training loss on iteration 5860 = 0.48714655786752703
Training loss on iteration 5880 = 0.47607504948973656
Training loss on iteration 5900 = 0.890911340713501
Training loss on iteration 5920 = 0.6375821232795715
Training loss on iteration 5940 = 0.6376554131507873
Training loss on iteration 5960 = 0.5010221228003502
Training loss on iteration 5980 = 0.6154822513461113
Training loss on iteration 6000 = 0.5718791447579861
Training loss on iteration 6020 = 0.6717663526535034
Training loss on iteration 6040 = 0.6766519606113434
Training loss on iteration 6060 = 0.7119544848799706
Training loss on iteration 6080 = 0.4633864052593708
Training loss on iteration 6100 = 0.9370396099984646
Training loss on iteration 6120 = 0.6346808761358261
Training loss on iteration 6140 = 0.6640260644257069
Training loss on iteration 6160 = 0.7442384913563729
Training loss on iteration 6180 = 0.7825262427330018
Training loss on iteration 6200 = 0.6162772536277771
Training loss on iteration 6220 = 0.612245824187994
Training loss on iteration 6240 = 0.4748063087463379
Training loss on iteration 6260 = 0.6024819642305375
Training loss on iteration 6280 = 0.6122767582535744
Training loss on iteration 6300 = 0.5508750632405282
Training loss on iteration 6320 = 0.5546727418899536
Training loss on iteration 6340 = 0.73963573127985
Training loss on iteration 6360 = 0.5643606528639793
Training loss on iteration 6380 = 0.6769427597522736
Training loss on iteration 0 = 0.5044212341308594
Training loss on iteration 20 = 0.5201461791992188
Training loss on iteration 40 = 0.513138809800148
Training loss on iteration 60 = 0.5604078248143196
Training loss on iteration 80 = 0.6499741464853287
Training loss on iteration 100 = 0.47403634339571
Training loss on iteration 120 = 0.5887697264552116
Training loss on iteration 140 = 0.47787095308303834
Training loss on iteration 160 = 0.49978303760290144
Training loss on iteration 180 = 0.5841277286410331
Training loss on iteration 200 = 0.5440160378813743
Training loss on iteration 220 = 0.5840086750686169
Training loss on iteration 240 = 0.49691108614206314
Training loss on iteration 260 = 0.42961909919977187
Training loss on iteration 280 = 0.36226070299744606
Training loss on iteration 300 = 0.520960833132267
Training loss on iteration 320 = 0.4219419606029987
Training loss on iteration 340 = 0.5751405835151673
Training loss on iteration 360 = 0.5050241030752659
Training loss on iteration 380 = 0.5082901038229466
Training loss on iteration 400 = 0.5079869225621223
Training loss on iteration 420 = 0.55361238270998
Training loss on iteration 440 = 0.516958599537611
Training loss on iteration 460 = 0.48834598511457444
Training loss on iteration 480 = 0.46455680429935453
Training loss on iteration 500 = 0.5283370368182659
Training loss on iteration 520 = 0.36145321279764175
Training loss on iteration 540 = 0.57691894993186
Training loss on iteration 560 = 0.5860569521784782
Training loss on iteration 580 = 0.43257213830947877
Training loss on iteration 600 = 0.4592286728322506
Training loss on iteration 620 = 0.40753548592329025
Training loss on iteration 640 = 0.4784394070506096
Training loss on iteration 660 = 0.6124267160892487
Training loss on iteration 680 = 0.5218698993325234
Training loss on iteration 700 = 0.4061358317732811
Training loss on iteration 720 = 0.40165876820683477
Training loss on iteration 740 = 0.6073462836444378
Training loss on iteration 760 = 0.5165979124605655
Training loss on iteration 780 = 0.46094084307551386
Training loss on iteration 800 = 0.4584373578429222
Training loss on iteration 820 = 0.472100505232811
Training loss on iteration 840 = 0.4156887948513031
Training loss on iteration 860 = 0.4789883382618427
Training loss on iteration 880 = 0.4387507773935795
Training loss on iteration 900 = 0.5179763823747635
Training loss on iteration 920 = 0.6056583538651467
Training loss on iteration 940 = 0.4534468874335289
Training loss on iteration 960 = 0.4197282031178474
Training loss on iteration 980 = 0.36159378588199614
Training loss on iteration 1000 = 0.5016161307692528
Training loss on iteration 1020 = 0.6066694565117359
Training loss on iteration 1040 = 0.4936337508261204
Training loss on iteration 1060 = 0.5092569284141064
Training loss on iteration 1080 = 0.5225922495126725
Training loss on iteration 1100 = 0.4795890532433987
Training loss on iteration 1120 = 0.43530935496091844
Training loss on iteration 1140 = 0.43822955191135404
Training loss on iteration 1160 = 0.51767463311553
Training loss on iteration 1180 = 0.4334312304854393
Training loss on iteration 1200 = 0.48910717070102694
Training loss on iteration 1220 = 0.5292829655110836
Training loss on iteration 1240 = 0.5254269033670426
Training loss on iteration 1260 = 0.501831977814436
Training loss on iteration 1280 = 0.44206442311406136
Training loss on iteration 1300 = 0.47826571241021154
Training loss on iteration 1320 = 0.4714684270322323
Training loss on iteration 1340 = 0.44145159497857095
Training loss on iteration 1360 = 0.45822530910372733
Training loss on iteration 1380 = 0.3876001887023449
Training loss on iteration 1400 = 0.4721925750374794
Training loss on iteration 1420 = 0.6019673690199852
Training loss on iteration 1440 = 0.45364327281713485
Training loss on iteration 1460 = 0.5072788022458553
Training loss on iteration 1480 = 0.5690259225666523
Training loss on iteration 1500 = 0.46149612292647363
Training loss on iteration 1520 = 0.4630171127617359
Training loss on iteration 1540 = 0.494928402453661
Training loss on iteration 1560 = 0.389645916223526
Training loss on iteration 1580 = 0.5020870037376881
Training loss on iteration 1600 = 0.5532684966921806
Training loss on iteration 1620 = 0.45068291425704954
Training loss on iteration 1640 = 0.5004650615155697
Training loss on iteration 1660 = 0.4728362187743187
Training loss on iteration 1680 = 0.5141956821084023
Training loss on iteration 1700 = 0.6276859112083912
Training loss on iteration 1720 = 0.35672096759080885
Training loss on iteration 1740 = 0.40018806234002113
Training loss on iteration 1760 = 0.6222393058240414
Training loss on iteration 1780 = 0.4494647227227688
Training loss on iteration 1800 = 0.4876043237745762
Training loss on iteration 1820 = 0.39701116681098936
Training loss on iteration 1840 = 0.458228163421154
Training loss on iteration 1860 = 0.5651957638561725
Training loss on iteration 1880 = 0.4950964540243149
Training loss on iteration 1900 = 0.4336341626942158
Training loss on iteration 1920 = 0.4742234140634537
Training loss on iteration 1940 = 0.46820303425192833
Training loss on iteration 1960 = 0.5211900338530541
Training loss on iteration 1980 = 0.4833626300096512
Training loss on iteration 2000 = 0.4776248887181282
Training loss on iteration 2020 = 0.5842401638627053
Training loss on iteration 2040 = 0.4448933519423008
Training loss on iteration 2060 = 0.4686413802206516
Training loss on iteration 2080 = 0.5179148748517036
Training loss on iteration 2100 = 0.5782251589000225
Training loss on iteration 2120 = 0.5167572773993016
Training loss on iteration 2140 = 0.45545087158679964
Training loss on iteration 2160 = 0.432186683267355
Training loss on iteration 2180 = 0.3907528556883335
Training loss on iteration 2200 = 0.39836746603250506
Training loss on iteration 2220 = 0.5161268956959247
Training loss on iteration 2240 = 0.42779917269945145
Training loss on iteration 2260 = 0.4837356202304363
Training loss on iteration 2280 = 0.4601829804480076
Training loss on iteration 2300 = 0.4965779438614845
Training loss on iteration 2320 = 0.5165439642965793
Training loss on iteration 2340 = 0.46245517432689665
Training loss on iteration 2360 = 0.47051948085427286
Training loss on iteration 2380 = 0.41431415677070615
Training loss on iteration 2400 = 0.513006667047739
Training loss on iteration 2420 = 0.4811618961393833
Training loss on iteration 2440 = 0.38971940353512763
Training loss on iteration 2460 = 0.4771968901157379
Training loss on iteration 2480 = 0.4134730979800224
Training loss on iteration 2500 = 0.5559115096926689
Training loss on iteration 2520 = 0.5127063795924187
Training loss on iteration 2540 = 0.49413168653845785
Training loss on iteration 2560 = 0.41681354194879533
Training loss on iteration 2580 = 0.41092037335038184
Training loss on iteration 2600 = 0.449688147008419
Training loss on iteration 2620 = 0.46519753634929656
Training loss on iteration 2640 = 0.49987341165542604
Training loss on iteration 2660 = 0.409195639193058
Training loss on iteration 2680 = 0.5479445599019528
Training loss on iteration 2700 = 0.4028250381350517
Training loss on iteration 2720 = 0.5417382501065731
Training loss on iteration 2740 = 0.4666104391217232
Training loss on iteration 2760 = 0.4845584765076637
Training loss on iteration 2780 = 0.42945516258478167
Training loss on iteration 2800 = 0.5260531701147556
Training loss on iteration 2820 = 0.3370675504207611
Training loss on iteration 2840 = 0.4078052222728729
Training loss on iteration 2860 = 0.42197490856051445
Training loss on iteration 2880 = 0.43716012090444567
Training loss on iteration 2900 = 0.4200537882745266
Training loss on iteration 2920 = 0.4875665120780468
Training loss on iteration 2940 = 0.43184406459331515
Training loss on iteration 2960 = 0.49577401876449584
Training loss on iteration 2980 = 0.34591816887259486
Training loss on iteration 3000 = 0.39045164436101915
Training loss on iteration 3020 = 0.476364778727293
Training loss on iteration 3040 = 0.4260784834623337
Training loss on iteration 3060 = 0.4095312848687172
Training loss on iteration 3080 = 0.425564294308424
Training loss on iteration 3100 = 0.4804828591644764
Training loss on iteration 3120 = 0.4306281991302967
Training loss on iteration 3140 = 0.46993206441402435
Training loss on iteration 3160 = 0.43268721774220464
Training loss on iteration 3180 = 0.44794969707727433
Training loss on iteration 3200 = 0.34988415241241455
Training loss on iteration 3220 = 0.46839087679982183
Training loss on iteration 3240 = 0.5239695698022843
Training loss on iteration 3260 = 0.3847584776580334
Training loss on iteration 3280 = 0.4442719243466854
Training loss on iteration 3300 = 0.4146054044365883
Training loss on iteration 3320 = 0.4510457307100296
Training loss on iteration 3340 = 0.4518174692988396
Training loss on iteration 3360 = 0.4965341553092003
Training loss on iteration 3380 = 0.40868921801447866
Training loss on iteration 3400 = 0.43835695162415506
Training loss on iteration 3420 = 0.3980792425572872
Training loss on iteration 3440 = 0.4825346268713474
Training loss on iteration 3460 = 0.47795367017388346
Training loss on iteration 3480 = 0.49753462746739385
Training loss on iteration 3500 = 0.5720603659749031
Training loss on iteration 3520 = 0.5285702407360077
Training loss on iteration 3540 = 0.4643386408686638
Training loss on iteration 3560 = 0.38893351554870603
Training loss on iteration 3580 = 0.3981478616595268
Training loss on iteration 3600 = 0.4307665407657623
Training loss on iteration 3620 = 0.5408605553209782
Training loss on iteration 3640 = 0.457310002297163
Training loss on iteration 3660 = 0.4332157656550407
Training loss on iteration 3680 = 0.5172550454735756
Training loss on iteration 3700 = 0.5122775755822658
Training loss on iteration 3720 = 0.47297456339001653
Training loss on iteration 3740 = 0.5256419286131859
Training loss on iteration 3760 = 0.5458432391285897
Training loss on iteration 3780 = 0.417927698045969
Training loss on iteration 3800 = 0.4969090849161148
Training loss on iteration 3820 = 0.43033232912421227
Training loss on iteration 3840 = 0.4099264919757843
Training loss on iteration 3860 = 0.45659316405653955
Training loss on iteration 3880 = 0.47688790410757065
Training loss on iteration 3900 = 0.33806825131177903
Training loss on iteration 3920 = 0.41263893619179726
Training loss on iteration 3940 = 0.4940368659794331
Training loss on iteration 3960 = 0.4832222633063793
Training loss on iteration 3980 = 0.5851354412734509
Training loss on iteration 4000 = 0.4484957613050938
Training loss on iteration 4020 = 0.35920924693346024
Training loss on iteration 4040 = 0.4651600278913975
Training loss on iteration 4060 = 0.521744517982006
Training loss on iteration 4080 = 0.3688273668289185
Training loss on iteration 4100 = 0.4215666800737381
Training loss on iteration 4120 = 0.5380330167710781
Training loss on iteration 4140 = 0.4197149410843849
Training loss on iteration 4160 = 0.48973803743720057
Training loss on iteration 4180 = 0.4924046866595745
Training loss on iteration 4200 = 0.44811036586761477
Training loss on iteration 4220 = 0.5003152258694172
Training loss on iteration 4240 = 0.4963839754462242
Training loss on iteration 4260 = 0.5202272467315197
Training loss on iteration 4280 = 0.5336032278835774
Training loss on iteration 4300 = 0.4401782378554344
Training loss on iteration 4320 = 0.5112083561718463
Training loss on iteration 4340 = 0.44891195446252824
Training loss on iteration 4360 = 0.3682819835841656
Training loss on iteration 4380 = 0.4270134076476097
Training loss on iteration 4400 = 0.551810110360384
Training loss on iteration 4420 = 0.42300276532769204
Training loss on iteration 4440 = 0.48882747665047643
Training loss on iteration 4460 = 0.4326925240457058
Training loss on iteration 4480 = 0.4038154162466526
Training loss on iteration 4500 = 0.4815062880516052
Training loss on iteration 4520 = 0.7228753179311752
Training loss on iteration 4540 = 0.5008011139929295
Training loss on iteration 4560 = 0.43901495188474654
Training loss on iteration 4580 = 0.634163910150528
Training loss on iteration 4600 = 0.4923699863255024
Training loss on iteration 4620 = 0.3852034442126751
Training loss on iteration 4640 = 0.4116310074925423
Training loss on iteration 4660 = 0.4664183370769024
Training loss on iteration 4680 = 0.443246190994978
Training loss on iteration 4700 = 0.5107410326600075
Training loss on iteration 4720 = 0.44895041510462763
Training loss on iteration 4740 = 0.4298908427357674
Training loss on iteration 4760 = 0.43870860561728475
Training loss on iteration 4780 = 0.399415585398674
Training loss on iteration 4800 = 0.4424018807709217
Training loss on iteration 4820 = 0.4887420438230038
Training loss on iteration 4840 = 0.4881863988935947
Training loss on iteration 4860 = 0.41877574697136877
Training loss on iteration 4880 = 0.44581909030675887
Training loss on iteration 4900 = 0.4146779619157314
Training loss on iteration 4920 = 0.473476867377758
Training loss on iteration 4940 = 0.527274989336729
Training loss on iteration 4960 = 0.37270617485046387
Training loss on iteration 4980 = 0.42655805945396424
Training loss on iteration 5000 = 0.40013250783085824
Training loss on iteration 5020 = 0.5251076273620129
Training loss on iteration 5040 = 0.40967472121119497
Training loss on iteration 5060 = 0.3958423502743244
Training loss on iteration 5080 = 0.424200838804245
Training loss on iteration 5100 = 0.6023225240409374
Training loss on iteration 5120 = 0.45997376590967176
Training loss on iteration 5140 = 0.46152031272649763
Training loss on iteration 5160 = 0.4318935379385948
Training loss on iteration 5180 = 0.45364666283130645
Training loss on iteration 5200 = 0.5230110853910446
Training loss on iteration 5220 = 0.46714205369353295
Training loss on iteration 5240 = 0.4847435846924782
Training loss on iteration 5260 = 0.53226008862257
Training loss on iteration 5280 = 0.4467438094317913
Training loss on iteration 5300 = 0.405543626844883
Training loss on iteration 5320 = 0.47655014991760253
Training loss on iteration 5340 = 0.443470199406147
Training loss on iteration 5360 = 0.4470321051776409
Training loss on iteration 5380 = 0.41847768649458883
Training loss on iteration 5400 = 0.4108739510178566
Training loss on iteration 5420 = 0.3808934323489666
Training loss on iteration 5440 = 0.3654158942401409
Training loss on iteration 5460 = 0.4750247456133366
Training loss on iteration 5480 = 0.37471468448638917
Training loss on iteration 5500 = 0.4583366349339485
Training loss on iteration 5520 = 0.41194293573498725
Training loss on iteration 5540 = 0.456846709549427
Training loss on iteration 5560 = 0.3938471354544163
Training loss on iteration 5580 = 0.47511920854449274
Training loss on iteration 5600 = 0.4080202393233776
Training loss on iteration 5620 = 0.4351664513349533
Training loss on iteration 5640 = 0.47694404497742654
Training loss on iteration 5660 = 0.46635914370417597
Training loss on iteration 5680 = 0.4804312825202942
Training loss on iteration 5700 = 0.42967295497655866
Training loss on iteration 5720 = 0.3746154636144638
Training loss on iteration 5740 = 0.4461449392139912
Training loss on iteration 5760 = 0.5599535375833511
Training loss on iteration 5780 = 0.48564532175660136
Training loss on iteration 5800 = 0.4860957324504852
Training loss on iteration 5820 = 0.4840256094932556
Training loss on iteration 5840 = 0.45803369879722594
Training loss on iteration 5860 = 0.4554594196379185
Training loss on iteration 5880 = 0.43331837132573126
Training loss on iteration 5900 = 0.4843196466565132
Training loss on iteration 5920 = 0.39164561554789545
Training loss on iteration 5940 = 0.41412101313471794
Training loss on iteration 5960 = 0.46558305248618126
Training loss on iteration 5980 = 0.47001721411943437
Training loss on iteration 6000 = 0.4410834863781929
Training loss on iteration 6020 = 0.5584601253271103
Training loss on iteration 6040 = 0.45582410916686056
Training loss on iteration 6060 = 0.41235783994197844
Training loss on iteration 6080 = 0.5818961389362812
Training loss on iteration 6100 = 0.4434406317770481
Training loss on iteration 6120 = 0.5827884092926979
Training loss on iteration 6140 = 0.44391674250364305
Training loss on iteration 6160 = 0.4552111394703388
Training loss on iteration 6180 = 0.47629653513431547
Training loss on iteration 6200 = 0.44617849960923195
Training loss on iteration 6220 = 0.4345073334872723
Training loss on iteration 6240 = 0.43162642419338226
Training loss on iteration 6260 = 0.5241728231310845
Training loss on iteration 6280 = 0.5134039014577866
Training loss on iteration 6300 = 0.5281144507229328
Training loss on iteration 6320 = 0.5869087867438794
Training loss on iteration 6340 = 0.43360564559698106
Training loss on iteration 6360 = 0.4504282087087631
Training loss on iteration 6380 = 0.4620291568338871
Training loss on iteration 0 = 0.36463940143585205
Training loss on iteration 20 = 0.510541395843029
Training loss on iteration 40 = 0.5094423770904541
Training loss on iteration 60 = 0.5393807984888553
Training loss on iteration 80 = 0.3736528784036636
Training loss on iteration 100 = 0.4672805450856686
Training loss on iteration 120 = 0.35280118957161904
Training loss on iteration 140 = 0.41968455389142034
Training loss on iteration 160 = 0.3993146844208241
Training loss on iteration 180 = 0.3572571538388729
Training loss on iteration 200 = 0.41781078949570655
Training loss on iteration 220 = 0.3372716214507818
Training loss on iteration 240 = 0.39140479937195777
Training loss on iteration 260 = 0.5053738065063953
Training loss on iteration 280 = 0.40403795391321184
Training loss on iteration 300 = 0.4546873435378075
Training loss on iteration 320 = 0.4836104862391949
Training loss on iteration 340 = 0.42552096843719484
Training loss on iteration 360 = 0.4659317694604397
Training loss on iteration 380 = 0.3985257513821125
Training loss on iteration 400 = 0.585250174254179
Training loss on iteration 420 = 0.3842987880110741
Training loss on iteration 440 = 0.4155597597360611
Training loss on iteration 460 = 0.4456272393465042
Training loss on iteration 480 = 0.44463226571679115
Training loss on iteration 500 = 0.5101977124810219
Training loss on iteration 520 = 0.34344480112195014
Training loss on iteration 540 = 0.4260953478515148
Training loss on iteration 560 = 0.3981762118637562
Training loss on iteration 580 = 0.3814168110489845
Training loss on iteration 600 = 0.48718478381633756
Training loss on iteration 620 = 0.5087205253541469
Training loss on iteration 640 = 0.3530299782752991
Training loss on iteration 660 = 0.6365494266152382
Training loss on iteration 680 = 0.4055479750037193
Training loss on iteration 700 = 0.3894954487681389
Training loss on iteration 720 = 0.545059685409069
Training loss on iteration 740 = 0.4972934119403362
Training loss on iteration 760 = 0.534759882837534
Training loss on iteration 780 = 0.5668347969651222
Training loss on iteration 800 = 0.4146742857992649
Training loss on iteration 820 = 0.36329295858740807
Training loss on iteration 840 = 0.38606257885694506
Training loss on iteration 860 = 0.43046095296740533
Training loss on iteration 880 = 0.37347590550780296
Training loss on iteration 900 = 0.3479906875640154
Training loss on iteration 920 = 0.44921809285879133
Training loss on iteration 940 = 0.37023028209805486
Training loss on iteration 960 = 0.43370805233716964
Training loss on iteration 980 = 0.47507269084453585
Training loss on iteration 1000 = 0.4408306561410427
Training loss on iteration 1020 = 0.41758457869291304
Training loss on iteration 1040 = 0.4302900113165379
Training loss on iteration 1060 = 0.5110570028424263
Training loss on iteration 1080 = 0.37953379228711126
Training loss on iteration 1100 = 0.4718407280743122
Training loss on iteration 1120 = 0.39526848755776883
Training loss on iteration 1140 = 0.49854167848825454
Training loss on iteration 1160 = 0.5221859022974968
Training loss on iteration 1180 = 0.3697253815829754
Training loss on iteration 1200 = 0.3816614359617233
Training loss on iteration 1220 = 0.46936938539147377
Training loss on iteration 1240 = 0.49265470579266546
Training loss on iteration 1260 = 0.4712070107460022
Training loss on iteration 1280 = 0.4988786168396473
Training loss on iteration 1300 = 0.4757223755121231
Training loss on iteration 1320 = 0.5452511832118034
Training loss on iteration 1340 = 0.3395968019962311
Training loss on iteration 1360 = 0.39109647572040557
Training loss on iteration 1380 = 0.43521911576390265
Training loss on iteration 1400 = 0.4513086132705212
Training loss on iteration 1420 = 0.5241991773247718
Training loss on iteration 1440 = 0.3702984869480133
Training loss on iteration 1460 = 0.39603412002325056
Training loss on iteration 1480 = 0.3964610666036606
Training loss on iteration 1500 = 0.4040219813585281
Training loss on iteration 1520 = 0.427864183485508
Training loss on iteration 1540 = 0.4560554601252079
Training loss on iteration 1560 = 0.4428624786436558
Training loss on iteration 1580 = 0.45268321260809896
Training loss on iteration 1600 = 0.4406245149672031
Training loss on iteration 1620 = 0.4115230984985828
Training loss on iteration 1640 = 0.39324466064572333
Training loss on iteration 1660 = 0.40582377649843693
Training loss on iteration 1680 = 0.3452594928443432
Training loss on iteration 1700 = 0.45467298924922944
Training loss on iteration 1720 = 0.43420940414071085
Training loss on iteration 1740 = 0.3702767685055733
Training loss on iteration 1760 = 0.39726771265268324
Training loss on iteration 1780 = 0.3063366860151291
Training loss on iteration 1800 = 0.39825882613658903
Training loss on iteration 1820 = 0.414614437520504
Training loss on iteration 1840 = 0.44571104273200035
Training loss on iteration 1860 = 0.4833566673099995
Training loss on iteration 1880 = 0.41368110105395317
Training loss on iteration 1900 = 0.38276209458708765
Training loss on iteration 1920 = 0.494579353928566
Training loss on iteration 1940 = 0.35315612703561783
Training loss on iteration 1960 = 0.5651664957404137
Training loss on iteration 1980 = 0.4398514971137047
Training loss on iteration 2000 = 0.4898464888334274
Training loss on iteration 2020 = 0.4973813362419605
Training loss on iteration 2040 = 0.46171658411622046
Training loss on iteration 2060 = 0.4699028320610523
Training loss on iteration 2080 = 0.5952705286443234
Training loss on iteration 2100 = 0.42827321141958236
Training loss on iteration 2120 = 0.521560188382864
Training loss on iteration 2140 = 0.46328319013118746
Training loss on iteration 2160 = 0.4034914433956146
Training loss on iteration 2180 = 0.5346231065690518
Training loss on iteration 2200 = 0.4050207227468491
Training loss on iteration 2220 = 0.3262009464204311
Training loss on iteration 2240 = 0.36394316256046294
Training loss on iteration 2260 = 0.5633965760469437
Training loss on iteration 2280 = 0.4128887303173542
Training loss on iteration 2300 = 0.5352749332785607
Training loss on iteration 2320 = 0.4368787072598934
Training loss on iteration 2340 = 0.5732492178678512
Training loss on iteration 2360 = 0.4325755424797535
Training loss on iteration 2380 = 0.41427136212587357
Training loss on iteration 2400 = 0.43536172062158585
Training loss on iteration 2420 = 0.3735730297863483
Training loss on iteration 2440 = 0.47333686500787736
Training loss on iteration 2460 = 0.3708098828792572
Training loss on iteration 2480 = 0.5434946037828923
Training loss on iteration 2500 = 0.5068471424281598
Training loss on iteration 2520 = 0.404339437186718
Training loss on iteration 2540 = 0.3707773968577385
Training loss on iteration 2560 = 0.3743621908128262
Training loss on iteration 2580 = 0.4397299952805042
Training loss on iteration 2600 = 0.4329264208674431
Training loss on iteration 2620 = 0.5146433599293232
Training loss on iteration 2640 = 0.4196808896958828
Training loss on iteration 2660 = 0.3440171904861927
Training loss on iteration 2680 = 0.4679453432559967
Training loss on iteration 2700 = 0.5195475816726685
Training loss on iteration 2720 = 0.40742723271250725
Training loss on iteration 2740 = 0.4757556147873402
Training loss on iteration 2760 = 0.3564390651881695
Training loss on iteration 2780 = 0.46632755249738694
Training loss on iteration 2800 = 0.36217007637023924
Training loss on iteration 2820 = 0.4904675640165806
Training loss on iteration 2840 = 0.42492865547537806
Training loss on iteration 2860 = 0.3884974576532841
Training loss on iteration 2880 = 0.3861656472086906
Training loss on iteration 2900 = 0.4977565579116344
Training loss on iteration 2920 = 0.403161196783185
Training loss on iteration 2940 = 0.47004408240318296
Training loss on iteration 2960 = 0.49294140934944153
Training loss on iteration 2980 = 0.4549434222280979
Training loss on iteration 3000 = 0.4363423749804497
Training loss on iteration 3020 = 0.4166776679456234
Training loss on iteration 3040 = 0.3908186420798302
Training loss on iteration 3060 = 0.49492167830467226
Training loss on iteration 3080 = 0.43215816617012026
Training loss on iteration 3100 = 0.39082988575100897
Training loss on iteration 3120 = 0.4257344201207161
Training loss on iteration 3140 = 0.4136142134666443
Training loss on iteration 3160 = 0.42788653671741483
Training loss on iteration 3180 = 0.3733246996998787
Training loss on iteration 3200 = 0.42635243758559227
Training loss on iteration 3220 = 0.45740338042378426
Training loss on iteration 3240 = 0.5356709986925126
Training loss on iteration 3260 = 0.4860324442386627
Training loss on iteration 3280 = 0.4350726053118706
Training loss on iteration 3300 = 0.43596557527780533
Training loss on iteration 3320 = 0.41783677190542223
Training loss on iteration 3340 = 0.40959946885704995
Training loss on iteration 3360 = 0.4405981436371803
Training loss on iteration 3380 = 0.39691959619522094
Training loss on iteration 3400 = 0.42126006856560705
Training loss on iteration 3420 = 0.4528896242380142
Training loss on iteration 3440 = 0.48395976424217224
Training loss on iteration 3460 = 0.5004452601075172
Training loss on iteration 3480 = 0.3769925393164158
Training loss on iteration 3500 = 0.495489177107811
Training loss on iteration 3520 = 0.4080751329660416
Training loss on iteration 3540 = 0.5470057182013989
Training loss on iteration 3560 = 0.5479756206274032
Training loss on iteration 3580 = 0.43024556562304495
Training loss on iteration 3600 = 0.39148381650447844
Training loss on iteration 3620 = 0.4338879235088825
Training loss on iteration 3640 = 0.3963787563145161
Training loss on iteration 3660 = 0.4232770025730133
Training loss on iteration 3680 = 0.5611390367150306
Training loss on iteration 3700 = 0.3707411251962185
Training loss on iteration 3720 = 0.4671608477830887
Training loss on iteration 3740 = 0.5777617178857326
Training loss on iteration 3760 = 0.5431416153907775
Training loss on iteration 3780 = 0.4147387817502022
Training loss on iteration 3800 = 0.39822629392147063
Training loss on iteration 3820 = 0.3964132845401764
Training loss on iteration 3840 = 0.44535093754529953
Training loss on iteration 3860 = 0.45308558717370034
Training loss on iteration 3880 = 0.40721296295523646
Training loss on iteration 3900 = 0.4456097662448883
Training loss on iteration 3920 = 0.4264470212161541
Training loss on iteration 3940 = 0.3651683822274208
Training loss on iteration 3960 = 0.34940436482429504
Training loss on iteration 3980 = 0.368297190964222
Training loss on iteration 4000 = 0.4469605013728142
Training loss on iteration 4020 = 0.4278216637670994
Training loss on iteration 4040 = 0.4214160665869713
Training loss on iteration 4060 = 0.44999908171594144
Training loss on iteration 4080 = 0.3527046747505665
Training loss on iteration 4100 = 0.3966114141047001
Training loss on iteration 4120 = 0.482383294403553
Training loss on iteration 4140 = 0.45635091364383695
Training loss on iteration 4160 = 0.41171338856220246
Training loss on iteration 4180 = 0.3912683367729187
Training loss on iteration 4200 = 0.43429310992360115
Training loss on iteration 4220 = 0.5135037481784821
Training loss on iteration 4240 = 0.36519126668572427
Training loss on iteration 4260 = 0.510172963887453
Training loss on iteration 4280 = 0.4101202987134457
Training loss on iteration 4300 = 0.4235680565237999
Training loss on iteration 4320 = 0.4588442400097847
Training loss on iteration 4340 = 0.32427689582109454
Training loss on iteration 4360 = 0.45809073448181153
Training loss on iteration 4380 = 0.4392253264784813
Training loss on iteration 4400 = 0.4234896242618561
Training loss on iteration 4420 = 0.43918724060058595
Training loss on iteration 4440 = 0.4995578326284885
Training loss on iteration 4460 = 0.38514465987682345
Training loss on iteration 4480 = 0.4611945278942585
Training loss on iteration 4500 = 0.3912446558475494
Training loss on iteration 4520 = 0.2954732045531273
Training loss on iteration 4540 = 0.3610624812543392
Training loss on iteration 4560 = 0.4139511756598949
Training loss on iteration 4580 = 0.517743831127882
Training loss on iteration 4600 = 0.42596677467226984
Training loss on iteration 4620 = 0.39135555624961854
Training loss on iteration 4640 = 0.4309883341193199
Training loss on iteration 4660 = 0.5002621486783028
Training loss on iteration 4680 = 0.5752555973827839
Training loss on iteration 4700 = 0.5610763236880303
Training loss on iteration 4720 = 0.5282434843480587
Training loss on iteration 4740 = 0.3973986066877842
Training loss on iteration 4760 = 4.460131654143334
Training loss on iteration 4780 = 0.5493181124329567
Training loss on iteration 4800 = 0.5103887334465981
Training loss on iteration 4820 = 0.573085081577301
Training loss on iteration 4840 = 0.47805474400520326
Training loss on iteration 4860 = 0.4572332825511694
Training loss on iteration 4880 = 0.46575573161244394
Training loss on iteration 4900 = 0.5652374520897865
Training loss on iteration 4920 = 0.4448187492787838
Training loss on iteration 4940 = 0.399080578237772
Training loss on iteration 4960 = 0.38154354244470595
Training loss on iteration 4980 = 0.5035110272467136
Training loss on iteration 5000 = 0.6034414872527123
Training loss on iteration 5020 = 0.44141874760389327
Training loss on iteration 5040 = 0.4365391157567501
Training loss on iteration 5060 = 0.47910828813910483
Training loss on iteration 5080 = 0.4446983478963375
Training loss on iteration 5100 = 0.4285735696554184
Training loss on iteration 5120 = 0.4467659920454025
Training loss on iteration 5140 = 0.4113614968955517
Training loss on iteration 5160 = 0.3927631929516792
Training loss on iteration 5180 = 0.47478272020816803
Training loss on iteration 5200 = 0.4356329932808876
Training loss on iteration 5220 = 0.3130896959453821
Training loss on iteration 5240 = 0.4459721989929676
Training loss on iteration 5260 = 0.4318934790790081
Training loss on iteration 5280 = 0.43863756135106086
Training loss on iteration 5300 = 0.38204372450709345
Training loss on iteration 5320 = 0.5110054053366184
Training loss on iteration 5340 = 0.4519632950425148
Training loss on iteration 5360 = 0.41547244414687157
Training loss on iteration 5380 = 0.47625132501125333
Training loss on iteration 5400 = 0.39601661264896393
Training loss on iteration 5420 = 0.3990770675241947
Training loss on iteration 5440 = 0.41225542947649957
Training loss on iteration 5460 = 0.4993944324553013
Training loss on iteration 5480 = 0.4139652609825134
Training loss on iteration 5500 = 0.46542505025863645
Training loss on iteration 5520 = 0.38450398445129397
Training loss on iteration 5540 = 0.49014548435807226
Training loss on iteration 5560 = 0.3922836035490036
Training loss on iteration 5580 = 0.48845024034380913
Training loss on iteration 5600 = 0.4420574113726616
Training loss on iteration 5620 = 0.4207717806100845
Training loss on iteration 5640 = 0.4036283515393734
Training loss on iteration 5660 = 0.41552221924066546
Training loss on iteration 5680 = 0.43436519280076025
Training loss on iteration 5700 = 0.5255988240242004
Training loss on iteration 5720 = 0.5071807555854321
Training loss on iteration 5740 = 0.4628921374678612
Training loss on iteration 5760 = 0.40022711679339407
Training loss on iteration 5780 = 0.4598463006317616
Training loss on iteration 5800 = 0.5900747746229171
Training loss on iteration 5820 = 0.4280071943998337
Training loss on iteration 5840 = 0.40826522558927536
Training loss on iteration 5860 = 0.48043200597167013
Training loss on iteration 5880 = 0.43912591859698297
Training loss on iteration 5900 = 0.4786292590200901
Training loss on iteration 5920 = 0.6130106627941132
Training loss on iteration 5940 = 0.43206638619303706
Training loss on iteration 5960 = 0.48871408104896547
Training loss on iteration 5980 = 0.45499357432126997
Training loss on iteration 6000 = 0.3693158678710461
Training loss on iteration 6020 = 0.5677086234092712
Training loss on iteration 6040 = 0.5095697864890099
Training loss on iteration 6060 = 0.4170888654887676
Training loss on iteration 6080 = 0.3548674263060093
Training loss on iteration 6100 = 0.4207861967384815
Training loss on iteration 6120 = 0.3926830418407917
Training loss on iteration 6140 = 0.41300242468714715
Training loss on iteration 6160 = 0.41770955696702006
Training loss on iteration 6180 = 0.4696042202413082
Training loss on iteration 6200 = 0.46574592366814616
Training loss on iteration 6220 = 0.4353280007839203
Training loss on iteration 6240 = 0.4561198577284813
Training loss on iteration 6260 = 0.4499905489385128
Training loss on iteration 6280 = 0.6157494395971298
Training loss on iteration 6300 = 0.43247347995638846
Training loss on iteration 6320 = 0.4183046206831932
Training loss on iteration 6340 = 0.41053288504481317
Training loss on iteration 6360 = 0.47249859422445295
Training loss on iteration 6380 = 0.4582058027386665
Training loss on iteration 0 = 0.5205546617507935
Training loss on iteration 20 = 0.3653089553117752
Training loss on iteration 40 = 0.3958143383264542
Training loss on iteration 60 = 0.3407353714108467
Training loss on iteration 80 = 0.3552734352648258
Training loss on iteration 100 = 0.435251821577549
Training loss on iteration 120 = 0.33019976764917375
Training loss on iteration 140 = 0.3886860698461533
Training loss on iteration 160 = 0.3646754570305347
Training loss on iteration 180 = 0.3632537417113781
Training loss on iteration 200 = 0.4239846505224705
Training loss on iteration 220 = 0.38558190762996675
Training loss on iteration 240 = 0.37773502841591833
Training loss on iteration 260 = 0.36775955408811567
Training loss on iteration 280 = 0.29133795723319056
Training loss on iteration 300 = 0.3395322371274233
Training loss on iteration 320 = 0.5903477646410465
Training loss on iteration 340 = 0.39583361223340036
Training loss on iteration 360 = 0.38867041319608686
Training loss on iteration 380 = 0.45234856456518174
Training loss on iteration 400 = 0.41329028755426406
Training loss on iteration 420 = 0.3872198164463043
Training loss on iteration 440 = 0.45624393299221994
Training loss on iteration 460 = 0.33717940002679825
Training loss on iteration 480 = 0.3681816227734089
Training loss on iteration 500 = 0.2803037740290165
Training loss on iteration 520 = 0.38367232754826547
Training loss on iteration 540 = 0.33931558951735497
Training loss on iteration 560 = 0.29760907366871836
Training loss on iteration 580 = 0.45519667528569696
Training loss on iteration 600 = 0.3809418469667435
Training loss on iteration 620 = 0.32022308856248854
Training loss on iteration 640 = 0.34864202290773394
Training loss on iteration 660 = 0.35891861096024513
Training loss on iteration 680 = 0.3614188939332962
Training loss on iteration 700 = 0.38065145164728165
Training loss on iteration 720 = 0.421713325381279
Training loss on iteration 740 = 0.42501409724354744
Training loss on iteration 760 = 0.41489588245749476
Training loss on iteration 780 = 0.517349573969841
Training loss on iteration 800 = 0.4499320738017559
Training loss on iteration 820 = 0.46152632385492326
Training loss on iteration 840 = 0.4113373585045338
Training loss on iteration 860 = 0.3945619106292725
Training loss on iteration 880 = 0.40507437884807584
Training loss on iteration 900 = 0.5237739890813827
Training loss on iteration 920 = 0.4475303702056408
Training loss on iteration 940 = 0.35030251294374465
Training loss on iteration 960 = 0.35906490311026573
Training loss on iteration 980 = 0.4176903061568737
Training loss on iteration 1000 = 0.4116051658987999
Training loss on iteration 1020 = 0.39199682846665385
Training loss on iteration 1040 = 0.3821942709386349
Training loss on iteration 1060 = 0.3912343204021454
Training loss on iteration 1080 = 0.38495136573910715
Training loss on iteration 1100 = 0.4293836779892445
Training loss on iteration 1120 = 0.3837218575179577
Training loss on iteration 1140 = 0.44856652021408083
Training loss on iteration 1160 = 0.33606757819652555
Training loss on iteration 1180 = 0.3720820344984531
Training loss on iteration 1200 = 0.3402771823108196
Training loss on iteration 1220 = 0.4616144970059395
Training loss on iteration 1240 = 0.39606414511799815
Training loss on iteration 1260 = 0.3787607751786709
Training loss on iteration 1280 = 0.4233593478798866
Training loss on iteration 1300 = 0.44194771274924277
Training loss on iteration 1320 = 0.36508882194757464
Training loss on iteration 1340 = 0.42171297743916514
Training loss on iteration 1360 = 0.514126855134964
Training loss on iteration 1380 = 0.3801947794854641
Training loss on iteration 1400 = 0.37161523923277856
Training loss on iteration 1420 = 0.4019556850194931
Training loss on iteration 1440 = 0.39260059222579
Training loss on iteration 1460 = 0.6022004552185536
Training loss on iteration 1480 = 0.450175741314888
Training loss on iteration 1500 = 0.3992808885872364
Training loss on iteration 1520 = 0.4103116773068905
Training loss on iteration 1540 = 0.41367878168821337
Training loss on iteration 1560 = 0.4258947163820267
Training loss on iteration 1580 = 0.3617445841431618
Training loss on iteration 1600 = 0.41236194893717765
Training loss on iteration 1620 = 0.4085280355066061
Training loss on iteration 1640 = 0.5468460783362389
Training loss on iteration 1660 = 0.42010937705636026
Training loss on iteration 1680 = 0.43358437195420263
Training loss on iteration 1700 = 0.4144935108721256
Training loss on iteration 1720 = 0.39540347680449484
Training loss on iteration 1740 = 0.3712354563176632
Training loss on iteration 1760 = 0.4717087734490633
Training loss on iteration 1780 = 0.3543200470507145
Training loss on iteration 1800 = 0.35944945365190506
Training loss on iteration 1820 = 0.38212510496377944
Training loss on iteration 1840 = 0.3354596421122551
Training loss on iteration 1860 = 0.34807100594043733
Training loss on iteration 1880 = 0.4631879538297653
Training loss on iteration 1900 = 0.411718387901783
Training loss on iteration 1920 = 0.38982187062501905
Training loss on iteration 1940 = 0.37572727650403975
Training loss on iteration 1960 = 0.3997346334159374
Training loss on iteration 1980 = 0.3853301122784615
Training loss on iteration 2000 = 0.38836529701948164
Training loss on iteration 2020 = 0.38511077761650087
Training loss on iteration 2040 = 0.3976789265871048
Training loss on iteration 2060 = 0.3528608210384846
Training loss on iteration 2080 = 0.37325176149606704
Training loss on iteration 2100 = 0.398548049479723
Training loss on iteration 2120 = 0.3880680501461029
Training loss on iteration 2140 = 0.41045554652810096
Training loss on iteration 2160 = 0.36663001254200933
Training loss on iteration 2180 = 0.3963943175971508
Training loss on iteration 2200 = 0.34008253887295725
Training loss on iteration 2220 = 0.3370191119611263
Training loss on iteration 2240 = 0.380562799423933
Training loss on iteration 2260 = 0.37884256020188334
Training loss on iteration 2280 = 0.4060167707502842
Training loss on iteration 2300 = 0.3904594458639622
Training loss on iteration 2320 = 0.38452323228120805
Training loss on iteration 2340 = 0.33312867358326914
Training loss on iteration 2360 = 0.40132708102464676
Training loss on iteration 2380 = 0.4733161933720112
Training loss on iteration 2400 = 0.38055375292897226
Training loss on iteration 2420 = 0.29655987918376925
Training loss on iteration 2440 = 0.3786411702632904
Training loss on iteration 2460 = 0.45606458485126494
Training loss on iteration 2480 = 0.37747045755386355
Training loss on iteration 2500 = 0.399370476603508
Training loss on iteration 2520 = 0.35305708423256876
Training loss on iteration 2540 = 0.3648088239133358
Training loss on iteration 2560 = 0.36430688872933387
Training loss on iteration 2580 = 0.3444223165512085
Training loss on iteration 2600 = 0.47581482380628587
Training loss on iteration 2620 = 0.3985245324671268
Training loss on iteration 2640 = 0.40052872970700265
Training loss on iteration 2660 = 0.442375411093235
Training loss on iteration 2680 = 0.4218373581767082
Training loss on iteration 2700 = 0.4348126217722893
Training loss on iteration 2720 = 0.5033542841672898
Training loss on iteration 2740 = 0.3850791536271572
Training loss on iteration 2760 = 0.42007861435413363
Training loss on iteration 2780 = 0.3982965238392353
Training loss on iteration 2800 = 0.4740581981837749
Training loss on iteration 2820 = 0.38435969278216364
Training loss on iteration 2840 = 0.48155215606093404
Training loss on iteration 2860 = 0.3307403489947319
Training loss on iteration 2880 = 0.40946860164403914
Training loss on iteration 2900 = 0.4336497537791729
Training loss on iteration 2920 = 0.40316868871450423
Training loss on iteration 2940 = 0.298991446942091
Training loss on iteration 2960 = 0.44870062842965125
Training loss on iteration 2980 = 0.4540704652667046
Training loss on iteration 3000 = 0.41325335279107095
Training loss on iteration 3020 = 0.3488707900047302
Training loss on iteration 3040 = 0.4081523969769478
Training loss on iteration 3060 = 0.40629143044352534
Training loss on iteration 3080 = 0.39599703773856165
Training loss on iteration 3100 = 0.35216507539153097
Training loss on iteration 3120 = 0.37464517578482626
Training loss on iteration 3140 = 0.3695464327931404
Training loss on iteration 3160 = 0.50990309715271
Training loss on iteration 3180 = 0.3651869513094425
Training loss on iteration 3200 = 0.3738228902220726
Training loss on iteration 3220 = 0.40290987491607666
Training loss on iteration 3240 = 0.41197079196572306
Training loss on iteration 3260 = 0.34670254290103913
Training loss on iteration 3280 = 0.36231379210948944
Training loss on iteration 3300 = 0.3864645943045616
Training loss on iteration 3320 = 0.4282299347221851
Training loss on iteration 3340 = 0.4010847441852093
Training loss on iteration 3360 = 0.3576825611293316
Training loss on iteration 3380 = 0.31537975892424586
Training loss on iteration 3400 = 0.3081232950091362
Training loss on iteration 3420 = 0.349820239841938
Training loss on iteration 3440 = 0.39154334738850594
Training loss on iteration 3460 = 0.3160744085907936
Training loss on iteration 3480 = 0.4286148428916931
Training loss on iteration 3500 = 0.2847689833492041
Training loss on iteration 3520 = 0.3580495685338974
Training loss on iteration 3540 = 0.44553581699728967
Training loss on iteration 3560 = 0.41707841232419013
Training loss on iteration 3580 = 0.3901612274348736
Training loss on iteration 3600 = 0.41107024922966956
Training loss on iteration 3620 = 0.41695827096700666
Training loss on iteration 3640 = 0.4208239935338497
Training loss on iteration 3660 = 0.41786789447069167
Training loss on iteration 3680 = 0.27399200275540353
Training loss on iteration 3700 = 0.4201416470110416
Training loss on iteration 3720 = 0.5238650254905224
Training loss on iteration 3740 = 0.4836470663547516
Training loss on iteration 3760 = 0.3826954230666161
Training loss on iteration 3780 = 0.42693609148263933
Training loss on iteration 3800 = 0.4637755319476128
Training loss on iteration 3820 = 0.39949899837374686
Training loss on iteration 3840 = 0.3813564576208591
Training loss on iteration 3860 = 0.3923475831747055
Training loss on iteration 3880 = 0.37774667218327523
Training loss on iteration 3900 = 0.46855574995279314
Training loss on iteration 3920 = 0.5642909593880177
Training loss on iteration 3940 = 0.35990722849965096
Training loss on iteration 3960 = 0.33745033890008924
Training loss on iteration 3980 = 0.6707737661898137
Training loss on iteration 4000 = 0.40365786105394363
Training loss on iteration 4020 = 0.44482149332761767
Training loss on iteration 4040 = 0.42003854364156723
Training loss on iteration 4060 = 0.4165721941739321
Training loss on iteration 4080 = 0.4574463255703449
Training loss on iteration 4100 = 0.4256150141358376
Training loss on iteration 4120 = 0.31799573376774787
Training loss on iteration 4140 = 0.5403446212410927
Training loss on iteration 4160 = 0.32369401678442955
Training loss on iteration 4180 = 0.3739045947790146
Training loss on iteration 4200 = 0.381552617251873
Training loss on iteration 4220 = 0.41527914479374883
Training loss on iteration 4240 = 0.31528513580560685
Training loss on iteration 4260 = 0.38200977593660357
Training loss on iteration 4280 = 0.41924070864915847
Training loss on iteration 4300 = 0.46860525757074356
Training loss on iteration 4320 = 0.320303862541914
Training loss on iteration 4340 = 0.4048510119318962
Training loss on iteration 4360 = 0.45005501210689547
Training loss on iteration 4380 = 0.3792289696633816
Training loss on iteration 4400 = 0.4483170576393604
Training loss on iteration 4420 = 0.40486322417855264
Training loss on iteration 4440 = 0.3717369318008423
Training loss on iteration 4460 = 0.38928478956222534
Training loss on iteration 4480 = 0.41361251324415205
Training loss on iteration 4500 = 0.4395380288362503
Training loss on iteration 4520 = 0.35498958081007004
Training loss on iteration 4540 = 0.4583701856434345
Training loss on iteration 4560 = 0.44497392550110815
Training loss on iteration 4580 = 0.34322891011834145
Training loss on iteration 4600 = 0.2993272073566914
Training loss on iteration 4620 = 0.4106394685804844
Training loss on iteration 4640 = 0.39552195817232133
Training loss on iteration 4660 = 0.3895877942442894
Training loss on iteration 4680 = 0.3241435378789902
Training loss on iteration 4700 = 0.42648340612649915
Training loss on iteration 4720 = 0.396482914686203
Training loss on iteration 4740 = 0.36115862503647805
Training loss on iteration 4760 = 0.4369289897382259
Training loss on iteration 4780 = 0.343319595605135
Training loss on iteration 4800 = 0.49131845831871035
Training loss on iteration 4820 = 0.38553328178822993
Training loss on iteration 4840 = 0.3583454988896847
Training loss on iteration 4860 = 0.344697218388319
Training loss on iteration 4880 = 0.42125493958592414
Training loss on iteration 4900 = 0.397980822622776
Training loss on iteration 4920 = 0.39706254154443743
Training loss on iteration 4940 = 0.3936957087367773
Training loss on iteration 4960 = 0.4423848822712898
Training loss on iteration 4980 = 0.3710649911314249
Training loss on iteration 5000 = 0.3315661050379276
Training loss on iteration 5020 = 0.4960598200559616
Training loss on iteration 5040 = 0.49755008071660994
Training loss on iteration 5060 = 0.4361025407910347
Training loss on iteration 5080 = 0.4854356124997139
Training loss on iteration 5100 = 0.4570985622704029
Training loss on iteration 5120 = 0.4340156152844429
Training loss on iteration 5140 = 0.5234049893915653
Training loss on iteration 5160 = 0.4337848074734211
Training loss on iteration 5180 = 0.42486227229237555
Training loss on iteration 5200 = 0.4165045492351055
Training loss on iteration 5220 = 0.44076277911663053
Training loss on iteration 5240 = 0.49300264194607735
Training loss on iteration 5260 = 0.46241433918476105
Training loss on iteration 5280 = 0.48531539067626
Training loss on iteration 5300 = 0.5126600049436092
Training loss on iteration 5320 = 0.3375546269118786
Training loss on iteration 5340 = 0.34109501615166665
Training loss on iteration 5360 = 0.4305287256836891
Training loss on iteration 5380 = 0.3471092917025089
Training loss on iteration 5400 = 0.39260612353682517
Training loss on iteration 5420 = 0.39801825657486917
Training loss on iteration 5440 = 0.4008613437414169
Training loss on iteration 5460 = 0.37041559144854547
Training loss on iteration 5480 = 0.584633482247591
Training loss on iteration 5500 = 0.47377025336027145
Training loss on iteration 5520 = 0.34798362478613853
Training loss on iteration 5540 = 0.40562771037220957
Training loss on iteration 5560 = 0.4211359277367592
Training loss on iteration 5580 = 0.4675918109714985
Training loss on iteration 5600 = 0.445023675262928
Training loss on iteration 5620 = 0.39512469843029974
Training loss on iteration 5640 = 0.47681081518530843
Training loss on iteration 5660 = 0.43074181452393534
Training loss on iteration 5680 = 0.35792876929044726
Training loss on iteration 5700 = 0.40901841036975384
Training loss on iteration 5720 = 0.46706980019807814
Training loss on iteration 5740 = 0.4504247948527336
Training loss on iteration 5760 = 0.43047356866300107
Training loss on iteration 5780 = 0.4588415794074535
Training loss on iteration 5800 = 0.45379275381565093
Training loss on iteration 5820 = 0.4221365861594677
Training loss on iteration 5840 = 0.40337780490517616
Training loss on iteration 5860 = 0.4418139047920704
Training loss on iteration 5880 = 0.43700429052114487
Training loss on iteration 5900 = 0.3488607294857502
Training loss on iteration 5920 = 0.46055494248867035
Training loss on iteration 5940 = 0.46185857951641085
Training loss on iteration 5960 = 0.3935974672436714
Training loss on iteration 5980 = 0.40336690843105316
Training loss on iteration 6000 = 0.555447231978178
Training loss on iteration 6020 = 0.5100641824305058
Training loss on iteration 6040 = 0.47179553732275964
Training loss on iteration 6060 = 0.4153020404279232
Training loss on iteration 6080 = 0.3759453296661377
Training loss on iteration 6100 = 0.4059096068143845
Training loss on iteration 6120 = 0.517545560002327
Training loss on iteration 6140 = 0.3927491992712021
Training loss on iteration 6160 = 0.4337224498391151
Training loss on iteration 6180 = 0.3462041199207306
Training loss on iteration 6200 = 0.36564775109291076
Training loss on iteration 6220 = 0.334625069051981
Training loss on iteration 6240 = 0.38586087003350256
Training loss on iteration 6260 = 0.5298525646328927
Training loss on iteration 6280 = 0.5116538599133491
Training loss on iteration 6300 = 0.3925435200333595
Training loss on iteration 6320 = 0.4099257156252861
Training loss on iteration 6340 = 0.36126026809215545
Training loss on iteration 6360 = 0.5209806486964226
Training loss on iteration 6380 = 0.4437248967587948
Training loss on iteration 0 = 0.1837722659111023
Training loss on iteration 20 = 0.37326462268829347
Training loss on iteration 40 = 0.3545023426413536
Training loss on iteration 60 = 0.4200972855091095
Training loss on iteration 80 = 0.3445213496685028
Training loss on iteration 100 = 0.3624719850718975
Training loss on iteration 120 = 0.4241869542747736
Training loss on iteration 140 = 0.36328298300504686
Training loss on iteration 160 = 0.35775372833013536
Training loss on iteration 180 = 0.34744413644075395
Training loss on iteration 200 = 0.37994458004832266
Training loss on iteration 220 = 0.3587737772613764
Training loss on iteration 240 = 0.37153018787503245
Training loss on iteration 260 = 0.3734756842255592
Training loss on iteration 280 = 0.3181824214756489
Training loss on iteration 300 = 0.32993135154247283
Training loss on iteration 320 = 0.3381501868367195
Training loss on iteration 340 = 0.4810904748737812
Training loss on iteration 360 = 0.3304357185959816
Training loss on iteration 380 = 0.49535402432084086
Training loss on iteration 400 = 0.40751542523503304
Training loss on iteration 420 = 0.3908680222928524
Training loss on iteration 440 = 0.33987054526805877
Training loss on iteration 460 = 0.4328679755330086
Training loss on iteration 480 = 0.40847652703523635
Training loss on iteration 500 = 0.35515484437346456
Training loss on iteration 520 = 0.39043945670127866
Training loss on iteration 540 = 0.3900696761906147
Training loss on iteration 560 = 0.364032506942749
Training loss on iteration 580 = 0.34357171058654784
Training loss on iteration 600 = 0.43109680637717246
Training loss on iteration 620 = 0.36926100254058836
Training loss on iteration 640 = 0.41176988631486894
Training loss on iteration 660 = 0.351215223968029
Training loss on iteration 680 = 0.3470470532774925
Training loss on iteration 700 = 0.32890396788716314
Training loss on iteration 720 = 0.35556128397583964
Training loss on iteration 740 = 0.36087692007422445
Training loss on iteration 760 = 0.37682989090681074
Training loss on iteration 780 = 0.3302661143243313
Training loss on iteration 800 = 0.3011172845959663
Training loss on iteration 820 = 0.3659771181643009
Training loss on iteration 840 = 0.39152262806892396
Training loss on iteration 860 = 0.38449906557798386
Training loss on iteration 880 = 0.36284670457243917
Training loss on iteration 900 = 0.44184808656573293
Training loss on iteration 920 = 0.3769989416003227
Training loss on iteration 940 = 0.3923223026096821
Training loss on iteration 960 = 0.33186983317136765
Training loss on iteration 980 = 0.3327149011194706
Training loss on iteration 1000 = 0.47771636098623277
Training loss on iteration 1020 = 0.3333566226065159
Training loss on iteration 1040 = 0.40623208731412885
Training loss on iteration 1060 = 0.43293034061789515
Training loss on iteration 1080 = 0.34636395797133446
Training loss on iteration 1100 = 0.3883934885263443
Training loss on iteration 1120 = 0.4187994323670864
Training loss on iteration 1140 = 0.35085555985569955
Training loss on iteration 1160 = 0.46095692217350004
Training loss on iteration 1180 = 0.3779838740825653
Training loss on iteration 1200 = 0.37695747092366216
Training loss on iteration 1220 = 0.40614357590675354
Training loss on iteration 1240 = 0.37124947607517245
Training loss on iteration 1260 = 0.4059028930962086
Training loss on iteration 1280 = 0.3689619615674019
Training loss on iteration 1300 = 0.3264336444437504
Training loss on iteration 1320 = 0.39289188385009766
Training loss on iteration 1340 = 0.3588995888829231
Training loss on iteration 1360 = 0.43587288409471514
Training loss on iteration 1380 = 0.4397948823869228
Training loss on iteration 1400 = 0.38397749438881873
Training loss on iteration 1420 = 0.3220581956207752
Training loss on iteration 1440 = 0.28054841980338097
Training loss on iteration 1460 = 0.36080931276082995
Training loss on iteration 1480 = 0.34599770307540895
Training loss on iteration 1500 = 0.3585386209189892
Training loss on iteration 1520 = 0.4092317782342434
Training loss on iteration 1540 = 0.41347631216049197
Training loss on iteration 1560 = 0.5123606435954571
Training loss on iteration 1580 = 0.3836005136370659
Training loss on iteration 1600 = 0.38199962228536605
Training loss on iteration 1620 = 0.4279530942440033
Training loss on iteration 1640 = 0.3272860459983349
Training loss on iteration 1660 = 0.4347505487501621
Training loss on iteration 1680 = 0.3351467467844486
Training loss on iteration 1700 = 0.3315292280167341
Training loss on iteration 1720 = 0.4133896067738533
Training loss on iteration 1740 = 0.37894321158528327
Training loss on iteration 1760 = 0.38201976791024206
Training loss on iteration 1780 = 0.35061720870435237
Training loss on iteration 1800 = 0.33309553042054174
Training loss on iteration 1820 = 0.4385484792292118
Training loss on iteration 1840 = 0.36525977924466135
Training loss on iteration 1860 = 0.3462293244898319
Training loss on iteration 1880 = 0.31546336188912394
Training loss on iteration 1900 = 0.3662741303443909
Training loss on iteration 1920 = 0.35488287955522535
Training loss on iteration 1940 = 0.29651067964732647
Training loss on iteration 1960 = 0.3316456936299801
Training loss on iteration 1980 = 0.43065037205815315
Training loss on iteration 2000 = 0.3922430768609047
Training loss on iteration 2020 = 0.38731411695480344
Training loss on iteration 2040 = 0.3941027235239744
Training loss on iteration 2060 = 0.3381186485290527
Training loss on iteration 2080 = 0.48061477243900297
Training loss on iteration 2100 = 0.3943868070840836
Training loss on iteration 2120 = 0.38980427756905556
Training loss on iteration 2140 = 0.38033815324306486
Training loss on iteration 2160 = 0.3620535619556904
Training loss on iteration 2180 = 0.4564489468932152
Training loss on iteration 2200 = 0.3669817067682743
Training loss on iteration 2220 = 0.3827407009899616
Training loss on iteration 2240 = 0.5466729499399662
Training loss on iteration 2260 = 0.45867724567651746
Training loss on iteration 2280 = 0.40376962050795556
Training loss on iteration 2300 = 0.38957315757870675
Training loss on iteration 2320 = 0.3288467820733786
Training loss on iteration 2340 = 0.5145991154015064
Training loss on iteration 2360 = 0.4386214703321457
Training loss on iteration 2380 = 0.4984163768589497
Training loss on iteration 2400 = 0.3389178872108459
Training loss on iteration 2420 = 0.32045608535408976
Training loss on iteration 2440 = 0.3809500128030777
Training loss on iteration 2460 = 0.6100663177669048
Training loss on iteration 2480 = 0.5210741177201271
Training loss on iteration 2500 = 0.4093011498451233
Training loss on iteration 2520 = 0.41711425557732584
Training loss on iteration 2540 = 0.40150943025946617
Training loss on iteration 2560 = 0.4581126272678375
Training loss on iteration 2580 = 0.3714975632727146
Training loss on iteration 2600 = 0.48342556431889533
Training loss on iteration 2620 = 0.43144311383366585
Training loss on iteration 2640 = 0.31407287046313287
Training loss on iteration 2660 = 0.34625554755330085
Training loss on iteration 2680 = 0.40898769721388817
Training loss on iteration 2700 = 0.406304781883955
Training loss on iteration 2720 = 0.33188762292265894
Training loss on iteration 2740 = 0.38413708955049514
Training loss on iteration 2760 = 0.3514695532619953
Training loss on iteration 2780 = 0.4099994882941246
Training loss on iteration 2800 = 0.3764492131769657
Training loss on iteration 2820 = 0.3800865612924099
Training loss on iteration 2840 = 0.39438943564891815
Training loss on iteration 2860 = 0.4905143089592457
Training loss on iteration 2880 = 0.3565587721765041
Training loss on iteration 2900 = 0.39071288257837294
Training loss on iteration 2920 = 0.34393327608704566
Training loss on iteration 2940 = 0.4087655909359455
Training loss on iteration 2960 = 0.41088215857744215
Training loss on iteration 2980 = 0.4309405893087387
Training loss on iteration 3000 = 0.37679994776844977
Training loss on iteration 3020 = 0.38654199689626695
Training loss on iteration 3040 = 0.4674469605088234
Training loss on iteration 3060 = 0.3866225898265839
Training loss on iteration 3080 = 0.48203272745013237
Training loss on iteration 3100 = 0.34326229616999626
Training loss on iteration 3120 = 0.43689668327569964
Training loss on iteration 3140 = 0.8614524818956852
Training loss on iteration 3160 = 0.39870255514979364
Training loss on iteration 3180 = 0.44308020994067193
Training loss on iteration 3200 = 0.41934647858142854
Training loss on iteration 3220 = 0.3919821843504906
Training loss on iteration 3240 = 0.43858196660876275
Training loss on iteration 3260 = 0.3819492533802986
Training loss on iteration 3280 = 0.3420387148857117
Training loss on iteration 3300 = 0.42953683957457545
Training loss on iteration 3320 = 0.3133153893053532
Training loss on iteration 3340 = 0.3411966130137444
Training loss on iteration 3360 = 0.36754599809646604
Training loss on iteration 3380 = 0.41711951792240143
Training loss on iteration 3400 = 0.39990444779396056
Training loss on iteration 3420 = 0.3481392256915569
Training loss on iteration 3440 = 0.43582920357584953
Training loss on iteration 3460 = 0.3496722847223282
Training loss on iteration 3480 = 0.3760960131883621
Training loss on iteration 3500 = 0.41445583924651147
Training loss on iteration 3520 = 0.35853273794054985
Training loss on iteration 3540 = 0.3330305255949497
Training loss on iteration 3560 = 0.3804195947945118
Training loss on iteration 3580 = 0.3670561954379082
Training loss on iteration 3600 = 0.4607603192329407
Training loss on iteration 3620 = 0.38299925848841665
Training loss on iteration 3640 = 0.4130350098013878
Training loss on iteration 3660 = 0.36060362607240676
Training loss on iteration 3680 = 0.53228540122509
Training loss on iteration 3700 = 0.46135311350226405
Training loss on iteration 3720 = 0.3859308548271656
Training loss on iteration 3740 = 0.4626072108745575
Training loss on iteration 3760 = 0.38828623853623867
Training loss on iteration 3780 = 0.3199222944676876
Training loss on iteration 3800 = 0.41667494401335714
Training loss on iteration 3820 = 0.38428172618150713
Training loss on iteration 3840 = 0.34527208656072617
Training loss on iteration 3860 = 0.42950909286737443
Training loss on iteration 3880 = 0.4653941571712494
Training loss on iteration 3900 = 0.45543102025985716
Training loss on iteration 3920 = 0.3368978202342987
Training loss on iteration 3940 = 0.48248521611094475
Training loss on iteration 3960 = 0.4132736690342426
Training loss on iteration 3980 = 0.34564301148056986
Training loss on iteration 4000 = 0.4022887133061886
Training loss on iteration 4020 = 0.49715299606323243
Training loss on iteration 4040 = 0.4659191831946373
Training loss on iteration 4060 = 0.35393626019358637
Training loss on iteration 4080 = 0.41638059690594675
Training loss on iteration 4100 = 0.37650416120886804
Training loss on iteration 4120 = 0.38459803983569146
Training loss on iteration 4140 = 0.33938524723052976
Training loss on iteration 4160 = 0.3647116418927908
Training loss on iteration 4180 = 0.4104753121733665
Training loss on iteration 4200 = 0.3954512909054756
Training loss on iteration 4220 = 0.41256927847862246
Training loss on iteration 4240 = 0.3274043411016464
Training loss on iteration 4260 = 0.33129130899906156
Training loss on iteration 4280 = 0.3025237798690796
Training loss on iteration 4300 = 0.3117459736764431
Training loss on iteration 4320 = 0.30231705158948896
Training loss on iteration 4340 = 0.420329499989748
Training loss on iteration 4360 = 0.3538860470056534
Training loss on iteration 4380 = 0.3795096352696419
Training loss on iteration 4400 = 0.4188274085521698
Training loss on iteration 4420 = 0.370999151468277
Training loss on iteration 4440 = 0.3738218158483505
Training loss on iteration 4460 = 0.38797622844576835
Training loss on iteration 4480 = 0.33248035684227945
Training loss on iteration 4500 = 0.38510640487074854
Training loss on iteration 4520 = 0.39649724438786504
Training loss on iteration 4540 = 0.3403373405337334
Training loss on iteration 4560 = 0.3210494384169579
Training loss on iteration 4580 = 0.34082350209355355
Training loss on iteration 4600 = 0.3208597034215927
Training loss on iteration 4620 = 0.35415381863713263
Training loss on iteration 4640 = 0.37143988683819773
Training loss on iteration 4660 = 0.39489141702651975
Training loss on iteration 4680 = 0.4157919354736805
Training loss on iteration 4700 = 0.36841225251555443
Training loss on iteration 4720 = 0.37468315958976744
Training loss on iteration 4740 = 0.38740630000829696
Training loss on iteration 4760 = 0.371685079485178
Training loss on iteration 4780 = 0.384197112172842
Training loss on iteration 4800 = 0.36699688658118246
Training loss on iteration 4820 = 0.38147637620568275
Training loss on iteration 4840 = 0.41899676620960236
Training loss on iteration 4860 = 0.43065514490008355
Training loss on iteration 4880 = 0.38056383207440375
Training loss on iteration 4900 = 0.3487078003585339
Training loss on iteration 4920 = 0.4118858382105827
Training loss on iteration 4940 = 0.38666598945856095
Training loss on iteration 4960 = 0.49721677601337433
Training loss on iteration 4980 = 0.4139224201440811
Training loss on iteration 5000 = 0.4498079061508179
Training loss on iteration 5020 = 0.4311088420450687
Training loss on iteration 5040 = 0.519968082010746
Training loss on iteration 5060 = 0.4422423802316189
Training loss on iteration 5080 = 0.40296235457062723
Training loss on iteration 5100 = 0.41418649405241015
Training loss on iteration 5120 = 0.4439681738615036
Training loss on iteration 5140 = 0.34528938978910445
Training loss on iteration 5160 = 0.3291391171514988
Training loss on iteration 5180 = 0.4045556303113699
Training loss on iteration 5200 = 0.33766604885458945
Training loss on iteration 5220 = 0.44611898362636565
Training loss on iteration 5240 = 0.3561599228531122
Training loss on iteration 5260 = 0.31457325369119643
Training loss on iteration 5280 = 0.4305760309100151
Training loss on iteration 5300 = 0.4092935137450695
Training loss on iteration 5320 = 0.37237570285797117
Training loss on iteration 5340 = 0.43533283844590187
Training loss on iteration 5360 = 0.3471527971327305
Training loss on iteration 5380 = 0.35017627775669097
Training loss on iteration 5400 = 0.35345879644155503
Training loss on iteration 5420 = 0.40767173990607264
Training loss on iteration 5440 = 0.4815662458539009
Training loss on iteration 5460 = 0.3829823687672615
Training loss on iteration 5480 = 0.46723487339913844
Training loss on iteration 5500 = 0.3658493921160698
Training loss on iteration 5520 = 0.46558455675840377
Training loss on iteration 5540 = 0.3683472529053688
Training loss on iteration 5560 = 0.41494219079613687
Training loss on iteration 5580 = 0.4211864612996578
Training loss on iteration 5600 = 0.4118566259741783
Training loss on iteration 5620 = 0.38781175687909125
Training loss on iteration 5640 = 0.49198843389749525
Training loss on iteration 5660 = 0.45532828718423846
Training loss on iteration 5680 = 0.4181778386235237
Training loss on iteration 5700 = 0.4037186034023762
Training loss on iteration 5720 = 0.49837197214365003
Training loss on iteration 5740 = 0.435910764336586
Training loss on iteration 5760 = 0.4122877448797226
Training loss on iteration 5780 = 0.5070576570928097
Training loss on iteration 5800 = 0.35776954889297485
Training loss on iteration 5820 = 0.43226292356848717
Training loss on iteration 5840 = 0.34936183840036394
Training loss on iteration 5860 = 0.4619521602988243
Training loss on iteration 5880 = 0.41049022004008295
Training loss on iteration 5900 = 0.41056115105748175
Training loss on iteration 5920 = 0.3507986806333065
Training loss on iteration 5940 = 0.3579899474978447
Training loss on iteration 5960 = 0.37975558936595916
Training loss on iteration 5980 = 0.5478253535926342
Training loss on iteration 6000 = 0.38303585946559904
Training loss on iteration 6020 = 0.39513796120882033
Training loss on iteration 6040 = 0.3768692992627621
Training loss on iteration 6060 = 0.4087808676064014
Training loss on iteration 6080 = 0.4617974527180195
Training loss on iteration 6100 = 0.46594717279076575
Training loss on iteration 6120 = 0.6521293990314007
Training loss on iteration 6140 = 0.396170886605978
Training loss on iteration 6160 = 0.39329869821667673
Training loss on iteration 6180 = 0.38294066712260244
Training loss on iteration 6200 = 0.3965570442378521
Training loss on iteration 6220 = 0.4661324582993984
Training loss on iteration 6240 = 0.46839464604854586
Training loss on iteration 6260 = 0.4231425806879997
Training loss on iteration 6280 = 0.39228648841381075
Training loss on iteration 6300 = 0.4557589113712311
Training loss on iteration 6320 = 0.40081537514925003
Training loss on iteration 6340 = 0.36213022097945213
Training loss on iteration 6360 = 0.4145097255706787
Training loss on iteration 6380 = 0.4042483799159527
Training loss on iteration 0 = 0.31931331753730774
Training loss on iteration 20 = 0.3965834639966488
Training loss on iteration 40 = 0.25921146273612977
Training loss on iteration 60 = 0.39708838909864425
Training loss on iteration 80 = 0.37976721823215487
Training loss on iteration 100 = 0.4063466787338257
Training loss on iteration 120 = 0.3004251796752214
Training loss on iteration 140 = 0.31613998785614966
Training loss on iteration 160 = 0.3762617640197277
Training loss on iteration 180 = 0.3726915009319782
Training loss on iteration 200 = 0.33543865010142326
Training loss on iteration 220 = 0.29516229405999184
Training loss on iteration 240 = 0.4165997788310051
Training loss on iteration 260 = 0.3408576674759388
Training loss on iteration 280 = 0.3783882461488247
Training loss on iteration 300 = 0.3734608627855778
Training loss on iteration 320 = 0.347331989556551
Training loss on iteration 340 = 0.298123924061656
Training loss on iteration 360 = 0.33060276284813883
Training loss on iteration 380 = 0.3592198118567467
Training loss on iteration 400 = 0.3522627778351307
Training loss on iteration 420 = 0.32074172124266626
Training loss on iteration 440 = 0.3593037538230419
Training loss on iteration 460 = 0.3428800217807293
Training loss on iteration 480 = 0.34940817430615423
Training loss on iteration 500 = 0.33044207394123076
Training loss on iteration 520 = 0.2763846844434738
Training loss on iteration 540 = 0.30355094745755196
Training loss on iteration 560 = 0.3425441704690456
Training loss on iteration 580 = 0.33579942733049395
Training loss on iteration 600 = 0.3274662889540195
Training loss on iteration 620 = 0.3868412524461746
Training loss on iteration 640 = 0.3210643894970417
Training loss on iteration 660 = 0.32918279618024826
Training loss on iteration 680 = 0.30959508419036863
Training loss on iteration 700 = 0.3158841170370579
Training loss on iteration 720 = 0.3209430515766144
Training loss on iteration 740 = 0.3452629573643208
Training loss on iteration 760 = 0.3258867710828781
Training loss on iteration 780 = 0.3562570631504059
Training loss on iteration 800 = 0.3757560282945633
Training loss on iteration 820 = 0.34039142802357675
Training loss on iteration 840 = 0.33481917083263396
Training loss on iteration 860 = 0.2869512900710106
Training loss on iteration 880 = 0.3594612434506416
Training loss on iteration 900 = 0.43066731840372086
Training loss on iteration 920 = 0.3843767672777176
Training loss on iteration 940 = 0.3648155748844147
Training loss on iteration 960 = 0.4671450324356556
Training loss on iteration 980 = 0.3921761967241764
Training loss on iteration 1000 = 0.36723900511860846
Training loss on iteration 1020 = 0.3482986122369766
Training loss on iteration 1040 = 0.5276393510401249
Training loss on iteration 1060 = 0.3367317058146
Training loss on iteration 1080 = 0.47639446556568144
Training loss on iteration 1100 = 0.35573878660798075
Training loss on iteration 1120 = 0.45179393514990807
Training loss on iteration 1140 = 0.33239716216921805
Training loss on iteration 1160 = 0.3604030258953571
Training loss on iteration 1180 = 0.32266422286629676
Training loss on iteration 1200 = 0.3446737177670002
Training loss on iteration 1220 = 0.41502115577459336
Training loss on iteration 1240 = 0.3670711174607277
Training loss on iteration 1260 = 0.3356992296874523
Training loss on iteration 1280 = 0.3546234928071499
Training loss on iteration 1300 = 0.3160960666835308
Training loss on iteration 1320 = 0.35978278145194054
Training loss on iteration 1340 = 0.41570105478167535
Training loss on iteration 1360 = 0.3149037197232246
Training loss on iteration 1380 = 0.3643091544508934
Training loss on iteration 1400 = 0.40594933554530144
Training loss on iteration 1420 = 0.3759929608553648
Training loss on iteration 1440 = 0.35705302357673646
Training loss on iteration 1460 = 0.39839865267276764
Training loss on iteration 1480 = 0.3878438644111156
Training loss on iteration 1500 = 0.32318950071930885
Training loss on iteration 1520 = 0.36129470095038413
Training loss on iteration 1540 = 0.361254620552063
Training loss on iteration 1560 = 0.3295501582324505
Training loss on iteration 1580 = 0.3973788470029831
Training loss on iteration 1600 = 0.35355493277311323
Training loss on iteration 1620 = 0.31668036952614786
Training loss on iteration 1640 = 0.37367906123399736
Training loss on iteration 1660 = 0.3868758663535118
Training loss on iteration 1680 = 0.3420900352299213
Training loss on iteration 1700 = 0.373805820196867
Training loss on iteration 1720 = 0.3896171376109123
Training loss on iteration 1740 = 0.36088949665427206
Training loss on iteration 1760 = 0.3369633570313454
Training loss on iteration 1780 = 0.3273056961596012
Training loss on iteration 1800 = 0.3542126096785069
Training loss on iteration 1820 = 0.3783727802336216
Training loss on iteration 1840 = 0.4671633832156658
Training loss on iteration 1860 = 0.4015292190015316
Training loss on iteration 1880 = 0.30367733240127565
Training loss on iteration 1900 = 0.3706740066409111
Training loss on iteration 1920 = 0.41436819806694986
Training loss on iteration 1940 = 0.35607800632715225
Training loss on iteration 1960 = 0.36435716599226
Training loss on iteration 1980 = 0.4204896785318851
Training loss on iteration 2000 = 0.3829013258218765
Training loss on iteration 2020 = 0.41813725754618647
Training loss on iteration 2040 = 0.3418735794723034
Training loss on iteration 2060 = 0.3369473792612553
Training loss on iteration 2080 = 0.37114037573337555
Training loss on iteration 2100 = 0.44734028801321984
Training loss on iteration 2120 = 0.3904297068715096
Training loss on iteration 2140 = 0.34199323430657386
Training loss on iteration 2160 = 0.4964031293988228
Training loss on iteration 2180 = 0.3132236525416374
Training loss on iteration 2200 = 0.3554804861545563
Training loss on iteration 2220 = 0.4691127993166447
Training loss on iteration 2240 = 0.2979138784110546
Training loss on iteration 2260 = 0.3655299454927444
Training loss on iteration 2280 = 0.37047911509871484
Training loss on iteration 2300 = 0.4092498540878296
Training loss on iteration 2320 = 0.5589688926935196
Training loss on iteration 2340 = 0.3275753229856491
Training loss on iteration 2360 = 0.33646848425269127
Training loss on iteration 2380 = 0.4164220534265041
Training loss on iteration 2400 = 0.38141838982701304
Training loss on iteration 2420 = 0.3575244635343552
Training loss on iteration 2440 = 0.3600096508860588
Training loss on iteration 2460 = 0.3639911822974682
Training loss on iteration 2480 = 0.34010003730654714
Training loss on iteration 2500 = 0.48466319069266317
Training loss on iteration 2520 = 0.35381118953227997
Training loss on iteration 2540 = 0.38043741285800936
Training loss on iteration 2560 = 0.3248604439198971
Training loss on iteration 2580 = 0.3307911567389965
Training loss on iteration 2600 = 0.3205707333981991
Training loss on iteration 2620 = 0.3734773211181164
Training loss on iteration 2640 = 0.2859291672706604
Training loss on iteration 2660 = 0.3446590229868889
Training loss on iteration 2680 = 0.34924826249480245
Training loss on iteration 2700 = 0.3477401189506054
Training loss on iteration 2720 = 0.4396441139280796
Training loss on iteration 2740 = 0.3866798013448715
Training loss on iteration 2760 = 0.36034930795431136
Training loss on iteration 2780 = 0.375265508890152
Training loss on iteration 2800 = 0.32754885107278825
Training loss on iteration 2820 = 0.2938473366200924
Training loss on iteration 2840 = 0.342447143048048
Training loss on iteration 2860 = 0.31405204385519025
Training loss on iteration 2880 = 0.41659492179751395
Training loss on iteration 2900 = 0.34307337552309036
Training loss on iteration 2920 = 0.38815709128975867
Training loss on iteration 2940 = 0.3562026612460613
Training loss on iteration 2960 = 0.4321591086685658
Training loss on iteration 2980 = 0.3419798485934734
Training loss on iteration 3000 = 0.4486086778342724
Training loss on iteration 3020 = 0.330231798440218
Training loss on iteration 3040 = 0.36080312356352806
Training loss on iteration 3060 = 0.3308187063783407
Training loss on iteration 3080 = 0.3787452861666679
Training loss on iteration 3100 = 0.428483909368515
Training loss on iteration 3120 = 0.29146624319255354
Training loss on iteration 3140 = 0.3688997343182564
Training loss on iteration 3160 = 0.32603573277592657
Training loss on iteration 3180 = 0.3563453681766987
Training loss on iteration 3200 = 0.3608145765960217
Training loss on iteration 3220 = 0.38491724133491517
Training loss on iteration 3240 = 0.331305780261755
Training loss on iteration 3260 = 0.32783240228891375
Training loss on iteration 3280 = 0.5665942505002022
Training loss on iteration 3300 = 0.4314223058521748
Training loss on iteration 3320 = 0.34159738197922707
Training loss on iteration 3340 = 0.36905689239501954
Training loss on iteration 3360 = 0.2882876291871071
Training loss on iteration 3380 = 0.35622367486357687
Training loss on iteration 3400 = 0.38174944967031477
Training loss on iteration 3420 = 0.30592472925782205
Training loss on iteration 3440 = 0.40186175480484965
Training loss on iteration 3460 = 0.2891122177243233
Training loss on iteration 3480 = 0.34509369507431986
Training loss on iteration 3500 = 0.4679636962711811
Training loss on iteration 3520 = 0.3686491459608078
Training loss on iteration 3540 = 0.35740424022078515
Training loss on iteration 3560 = 0.4098889298737049
Training loss on iteration 3580 = 0.33552609384059906
Training loss on iteration 3600 = 0.46722026392817495
Training loss on iteration 3620 = 0.4472952999174595
Training loss on iteration 3640 = 0.40121401026844977
Training loss on iteration 3660 = 0.4662053018808365
Training loss on iteration 3680 = 0.46864375844597816
Training loss on iteration 3700 = 0.4288652136921883
Training loss on iteration 3720 = 0.3996489211916924
Training loss on iteration 3740 = 0.40626747384667394
Training loss on iteration 3760 = 0.34265039563179017
Training loss on iteration 3780 = 0.3789813697338104
Training loss on iteration 3800 = 0.38905227556824684
Training loss on iteration 3820 = 0.38324325531721115
Training loss on iteration 3840 = 0.4271529570221901
Training loss on iteration 3860 = 0.36249395459890366
Training loss on iteration 3880 = 0.3437149614095688
Training loss on iteration 3900 = 0.3707479417324066
Training loss on iteration 3920 = 0.3511230990290642
Training loss on iteration 3940 = 0.4641084671020508
Training loss on iteration 3960 = 0.34770928844809534
Training loss on iteration 3980 = 0.35988384634256365
Training loss on iteration 4000 = 0.3119892619550228
Training loss on iteration 4020 = 0.30485830567777156
Training loss on iteration 4040 = 0.40951952412724496
Training loss on iteration 4060 = 0.38572708815336226
Training loss on iteration 4080 = 0.39285373464226725
Training loss on iteration 4100 = 0.4067249096930027
Training loss on iteration 4120 = 0.3590075083076954
Training loss on iteration 4140 = 0.4199266880750656
Training loss on iteration 4160 = 0.30846252739429475
Training loss on iteration 4180 = 0.3437651954591274
Training loss on iteration 4200 = 0.4576086260378361
Training loss on iteration 4220 = 0.3382442735135555
Training loss on iteration 4240 = 0.3212029717862606
Training loss on iteration 4260 = 0.3045457750558853
Training loss on iteration 4280 = 0.3899653233587742
Training loss on iteration 4300 = 0.37491517066955565
Training loss on iteration 4320 = 0.3942668899893761
Training loss on iteration 4340 = 0.35267643332481385
Training loss on iteration 4360 = 0.36136820539832115
Training loss on iteration 4380 = 0.3145618550479412
Training loss on iteration 4400 = 0.372885013371706
Training loss on iteration 4420 = 0.3426592580974102
Training loss on iteration 4440 = 0.4456604830920696
Training loss on iteration 4460 = 0.3329988427460194
Training loss on iteration 4480 = 0.35093908533453944
Training loss on iteration 4500 = 0.3211293451488018
Training loss on iteration 4520 = 0.36744009628891944
Training loss on iteration 4540 = 0.44233507588505744
Training loss on iteration 4560 = 0.35999068319797517
Training loss on iteration 4580 = 0.3112929105758667
Training loss on iteration 4600 = 0.3365393426269293
Training loss on iteration 4620 = 0.36893065720796586
Training loss on iteration 4640 = 0.3729795880615711
Training loss on iteration 4660 = 0.3177425540983677
Training loss on iteration 4680 = 0.5103005588054657
Training loss on iteration 4700 = 0.3181326322257519
Training loss on iteration 4720 = 0.40106009021401406
Training loss on iteration 4740 = 0.34330255538225174
Training loss on iteration 4760 = 0.36333743706345556
Training loss on iteration 4780 = 0.3729771487414837
Training loss on iteration 4800 = 0.34809146970510485
Training loss on iteration 4820 = 0.32509856298565865
Training loss on iteration 4840 = 0.3894331842660904
Training loss on iteration 4860 = 0.3441959023475647
Training loss on iteration 4880 = 0.42431532591581345
Training loss on iteration 4900 = 0.3534435279667377
Training loss on iteration 4920 = 0.41012985408306124
Training loss on iteration 4940 = 0.4274287484586239
Training loss on iteration 4960 = 0.4406589411199093
Training loss on iteration 4980 = 0.3940532818436623
Training loss on iteration 5000 = 0.5005822107195854
Training loss on iteration 5020 = 0.41472740322351453
Training loss on iteration 5040 = 0.4223111286759377
Training loss on iteration 5060 = 0.3620657674968243
Training loss on iteration 5080 = 0.3461219802498817
Training loss on iteration 5100 = 0.33152466639876366
Training loss on iteration 5120 = 0.4383117571473122
Training loss on iteration 5140 = 0.3227074399590492
Training loss on iteration 5160 = 0.33815132044255736
Training loss on iteration 5180 = 0.416204809397459
Training loss on iteration 5200 = 0.4323693558573723
Training loss on iteration 5220 = 0.3742089867591858
Training loss on iteration 5240 = 0.4116946332156658
Training loss on iteration 5260 = 0.6087110720574855
Training loss on iteration 5280 = 0.3529693029820919
Training loss on iteration 5300 = 0.40394195094704627
Training loss on iteration 5320 = 0.4699213571846485
Training loss on iteration 5340 = 0.41039586886763574
Training loss on iteration 5360 = 0.3987828142940998
Training loss on iteration 5380 = 0.43863228559494016
Training loss on iteration 5400 = 0.47043163552880285
Training loss on iteration 5420 = 0.35838588178157804
Training loss on iteration 5440 = 0.2828631617128849
Training loss on iteration 5460 = 0.33807395249605177
Training loss on iteration 5480 = 0.4593380741775036
Training loss on iteration 5500 = 0.3614423036575317
Training loss on iteration 5520 = 0.3630666647106409
Training loss on iteration 5540 = 0.43065731003880503
Training loss on iteration 5560 = 0.4089896731078625
Training loss on iteration 5580 = 0.4105429485440254
Training loss on iteration 5600 = 0.3647784575819969
Training loss on iteration 5620 = 0.3525352254509926
Training loss on iteration 5640 = 0.2979562021791935
Training loss on iteration 5660 = 0.31622705459594724
Training loss on iteration 5680 = 0.40490511655807493
Training loss on iteration 5700 = 0.4386181429028511
Training loss on iteration 5720 = 0.40852833539247513
Training loss on iteration 5740 = 0.34603624492883683
Training loss on iteration 5760 = 0.3484134986996651
Training loss on iteration 5780 = 0.4026568666100502
Training loss on iteration 5800 = 0.3528487928211689
Training loss on iteration 5820 = 0.3433085046708584
Training loss on iteration 5840 = 0.3476149275898933
Training loss on iteration 5860 = 0.3741879478096962
Training loss on iteration 5880 = 0.34457658752799036
Training loss on iteration 5900 = 0.3579694963991642
Training loss on iteration 5920 = 0.3801059789955616
Training loss on iteration 5940 = 0.44447800889611244
Training loss on iteration 5960 = 0.4882187619805336
Training loss on iteration 5980 = 0.36673392429947854
Training loss on iteration 6000 = 0.3328695334494114
Training loss on iteration 6020 = 0.33195445910096166
Training loss on iteration 6040 = 0.4006500579416752
Training loss on iteration 6060 = 0.3797667272388935
Training loss on iteration 6080 = 0.6120048321783542
Training loss on iteration 6100 = 0.3253737784922123
Training loss on iteration 6120 = 0.32574866712093353
Training loss on iteration 6140 = 0.482867369055748
Training loss on iteration 6160 = 0.4940043371170759
Training loss on iteration 6180 = 0.3349011532962322
Training loss on iteration 6200 = 0.42686565667390824
Training loss on iteration 6220 = 0.35160548239946365
Training loss on iteration 6240 = 0.3181774243712425
Training loss on iteration 6260 = 0.44951689168810843
Training loss on iteration 6280 = 0.4560732081532478
Training loss on iteration 6300 = 0.48877929002046583
Training loss on iteration 6320 = 0.40910653546452524
Training loss on iteration 6340 = 0.3677053712308407
Training loss on iteration 6360 = 0.44366143718361856
Training loss on iteration 6380 = 0.4258683383464813
Training loss on iteration 0 = 0.16873230040073395
Training loss on iteration 20 = 0.36734602972865105
Training loss on iteration 40 = 0.3836401544511318
Training loss on iteration 60 = 0.3222588449716568
Training loss on iteration 80 = 0.3414221428334713
Training loss on iteration 100 = 0.3484880290925503
Training loss on iteration 120 = 0.3441555485129356
Training loss on iteration 140 = 0.31461967527866364
Training loss on iteration 160 = 0.312696547806263
Training loss on iteration 180 = 0.300624306499958
Training loss on iteration 200 = 0.29853803664445877
Training loss on iteration 220 = 0.3314515739679337
Training loss on iteration 240 = 0.347373229265213
Training loss on iteration 260 = 0.3564428463578224
Training loss on iteration 280 = 0.4262527942657471
Training loss on iteration 300 = 0.43109015598893163
Training loss on iteration 320 = 0.3609782412648201
Training loss on iteration 340 = 0.30559240765869616
Training loss on iteration 360 = 0.4044145539402962
Training loss on iteration 380 = 0.33153819665312767
Training loss on iteration 400 = 0.30742026418447493
Training loss on iteration 420 = 0.29636878073215484
Training loss on iteration 440 = 0.37190375179052354
Training loss on iteration 460 = 0.359231298416853
Training loss on iteration 480 = 0.2678325280547142
Training loss on iteration 500 = 0.291527434438467
Training loss on iteration 520 = 0.30456712320446966
Training loss on iteration 540 = 0.32190717086195947
Training loss on iteration 560 = 0.32939367592334745
Training loss on iteration 580 = 0.4022070698440075
Training loss on iteration 600 = 0.33256633058190344
Training loss on iteration 620 = 0.2757100027054548
Training loss on iteration 640 = 0.27971640378236773
Training loss on iteration 660 = 0.2680653862655163
Training loss on iteration 680 = 0.2962728630751371
Training loss on iteration 700 = 0.3301975764334202
Training loss on iteration 720 = 0.3440826665610075
Training loss on iteration 740 = 0.33763279020786285
Training loss on iteration 760 = 0.27428452149033544
Training loss on iteration 780 = 0.37365338057279585
Training loss on iteration 800 = 0.3220395430922508
Training loss on iteration 820 = 0.38849802538752554
Training loss on iteration 840 = 0.3997983492910862
Training loss on iteration 860 = 0.3595898851752281
Training loss on iteration 880 = 0.32207616455852983
Training loss on iteration 900 = 0.3803203977644444
Training loss on iteration 920 = 0.28817712515592575
Training loss on iteration 940 = 0.28997691348195076
Training loss on iteration 960 = 0.2612238131463528
Training loss on iteration 980 = 0.41626378819346427
Training loss on iteration 1000 = 0.34139672443270686
Training loss on iteration 1020 = 0.2828159719705582
Training loss on iteration 1040 = 0.3486666604876518
Training loss on iteration 1060 = 0.26995620839297774
Training loss on iteration 1080 = 0.34291603341698645
Training loss on iteration 1100 = 0.2582148611545563
Training loss on iteration 1120 = 0.36841145604848863
Training loss on iteration 1140 = 0.32990691214799883
Training loss on iteration 1160 = 0.34262642115354536
Training loss on iteration 1180 = 0.30868818759918215
Training loss on iteration 1200 = 0.31375099569559095
Training loss on iteration 1220 = 0.3521096110343933
Training loss on iteration 1240 = 0.38264888525009155
Training loss on iteration 1260 = 0.3757530231028795
Training loss on iteration 1280 = 0.2780328311026096
Training loss on iteration 1300 = 0.40774881318211553
Training loss on iteration 1320 = 0.35626559183001516
Training loss on iteration 1340 = 0.393941055983305
Training loss on iteration 1360 = 0.35824542567133905
Training loss on iteration 1380 = 0.35555226877331736
Training loss on iteration 1400 = 0.3507832266390324
Training loss on iteration 1420 = 0.31708023101091387
Training loss on iteration 1440 = 0.3004779599606991
Training loss on iteration 1460 = 0.33749323710799217
Training loss on iteration 1480 = 0.35614143013954164
Training loss on iteration 1500 = 0.3028621688485146
Training loss on iteration 1520 = 0.3257190905511379
Training loss on iteration 1540 = 0.34287034124135973
Training loss on iteration 1560 = 0.33072693534195424
Training loss on iteration 1580 = 0.3667112596333027
Training loss on iteration 1600 = 0.395259515196085
Training loss on iteration 1620 = 0.410929936170578
Training loss on iteration 1640 = 0.38599068522453306
Training loss on iteration 1660 = 0.3432221174240112
Training loss on iteration 1680 = 0.3089229851961136
Training loss on iteration 1700 = 0.3468594714999199
Training loss on iteration 1720 = 0.32743241898715497
Training loss on iteration 1740 = 0.3318967655301094
Training loss on iteration 1760 = 0.303716991096735
Training loss on iteration 1780 = 0.36214897632598875
Training loss on iteration 1800 = 0.38758117854595187
Training loss on iteration 1820 = 0.30030245184898374
Training loss on iteration 1840 = 0.3471539460122585
Training loss on iteration 1860 = 0.34114803224802015
Training loss on iteration 1880 = 0.3217305265367031
Training loss on iteration 1900 = 0.37726669311523436
Training loss on iteration 1920 = 0.3659004747867584
Training loss on iteration 1940 = 0.3855260074138641
Training loss on iteration 1960 = 0.3190686099231243
Training loss on iteration 1980 = 0.31959849745035174
Training loss on iteration 2000 = 0.3731807015836239
Training loss on iteration 2020 = 0.5091549806296826
Training loss on iteration 2040 = 0.3200873173773289
Training loss on iteration 2060 = 0.35617735609412193
Training loss on iteration 2080 = 0.35953047648072245
Training loss on iteration 2100 = 0.516867883503437
Training loss on iteration 2120 = 0.32925340011715887
Training loss on iteration 2140 = 0.3315760001540184
Training loss on iteration 2160 = 0.28112324997782706
Training loss on iteration 2180 = 0.3426563121378422
Training loss on iteration 2200 = 0.3802469663321972
Training loss on iteration 2220 = 0.40268600136041643
Training loss on iteration 2240 = 0.3808698259294033
Training loss on iteration 2260 = 0.36980768516659734
Training loss on iteration 2280 = 0.2876841627061367
Training loss on iteration 2300 = 0.41594326198101045
Training loss on iteration 2320 = 0.38329455330967904
Training loss on iteration 2340 = 0.36828441023826597
Training loss on iteration 2360 = 0.30361072197556493
Training loss on iteration 2380 = 0.3369319014251232
Training loss on iteration 2400 = 0.4373572126030922
Training loss on iteration 2420 = 0.3061130106449127
Training loss on iteration 2440 = 0.29667133539915086
Training loss on iteration 2460 = 0.365992184728384
Training loss on iteration 2480 = 0.41932970136404035
Training loss on iteration 2500 = 0.319040858745575
Training loss on iteration 2520 = 0.302527017891407
Training loss on iteration 2540 = 0.2960657738149166
Training loss on iteration 2560 = 0.3717867024242878
Training loss on iteration 2580 = 0.3674112752079964
Training loss on iteration 2600 = 0.40065461695194243
Training loss on iteration 2620 = 0.3989319533109665
Training loss on iteration 2640 = 0.4297982037067413
Training loss on iteration 2660 = 0.35761901214718816
Training loss on iteration 2680 = 0.3552353959530592
Training loss on iteration 2700 = 0.3950033761560917
Training loss on iteration 2720 = 0.3304249495267868
Training loss on iteration 2740 = 0.3298543609678745
Training loss on iteration 2760 = 0.3769934944808483
Training loss on iteration 2780 = 0.345734953135252
Training loss on iteration 2800 = 0.3809956476092339
Training loss on iteration 2820 = 0.3772466108202934
Training loss on iteration 2840 = 0.3307711061090231
Training loss on iteration 2860 = 0.3771896243095398
Training loss on iteration 2880 = 0.29032898843288424
Training loss on iteration 2900 = 0.31067447178065777
Training loss on iteration 2920 = 0.352021187543869
Training loss on iteration 2940 = 0.2999303761869669
Training loss on iteration 2960 = 0.38169015049934385
Training loss on iteration 2980 = 0.2980596493929625
Training loss on iteration 3000 = 0.36296581849455833
Training loss on iteration 3020 = 0.4277766734361649
Training loss on iteration 3040 = 0.30000588297843933
Training loss on iteration 3060 = 0.39850332364439967
Training loss on iteration 3080 = 0.4914295054972172
Training loss on iteration 3100 = 0.42452202215790746
Training loss on iteration 3120 = 0.31975661516189574
Training loss on iteration 3140 = 0.41846941113471986
Training loss on iteration 3160 = 0.45964266657829284
Training loss on iteration 3180 = 0.3503532387316227
Training loss on iteration 3200 = 0.3480908632278442
Training loss on iteration 3220 = 0.274792692437768
Training loss on iteration 3240 = 0.46270688623189926
Training loss on iteration 3260 = 0.37119568660855295
Training loss on iteration 3280 = 0.3527073264122009
Training loss on iteration 3300 = 0.3812399931252003
Training loss on iteration 3320 = 0.3378820367157459
Training loss on iteration 3340 = 0.36832789033651353
Training loss on iteration 3360 = 0.3712963551282883
Training loss on iteration 3380 = 0.3559981606900692
Training loss on iteration 3400 = 0.4122101925313473
Training loss on iteration 3420 = 0.3529506340622902
Training loss on iteration 3440 = 0.35532170459628104
Training loss on iteration 3460 = 0.3611024335026741
Training loss on iteration 3480 = 0.3557180680334568
Training loss on iteration 3500 = 0.42354071140289307
Training loss on iteration 3520 = 0.36250345408916473
Training loss on iteration 3540 = 0.33503591939806937
Training loss on iteration 3560 = 0.3735391430556774
Training loss on iteration 3580 = 0.40835339948534966
Training loss on iteration 3600 = 0.38183506205677986
Training loss on iteration 3620 = 0.2953877955675125
Training loss on iteration 3640 = 0.33433364033699037
Training loss on iteration 3660 = 0.32526499032974243
Training loss on iteration 3680 = 0.33088148236274717
Training loss on iteration 3700 = 0.3770333305001259
Training loss on iteration 3720 = 0.3637869693338871
Training loss on iteration 3740 = 0.33301362991333006
Training loss on iteration 3760 = 0.2734720692038536
Training loss on iteration 3780 = 0.35176667794585226
Training loss on iteration 3800 = 0.3449093632400036
Training loss on iteration 3820 = 0.30708530992269517
Training loss on iteration 3840 = 0.3257486928254366
Training loss on iteration 3860 = 0.33221342638134954
Training loss on iteration 3880 = 0.3645301893353462
Training loss on iteration 3900 = 0.46189224943518636
Training loss on iteration 3920 = 0.4006055906414986
Training loss on iteration 3940 = 0.3367291778326035
Training loss on iteration 3960 = 0.3070357874035835
Training loss on iteration 3980 = 0.3624535582959652
Training loss on iteration 4000 = 0.37321794405579567
Training loss on iteration 4020 = 0.33949879482388495
Training loss on iteration 4040 = 0.3399344965815544
Training loss on iteration 4060 = 0.3633508838713169
Training loss on iteration 4080 = 0.38021816238760947
Training loss on iteration 4100 = 0.34148015081882477
Training loss on iteration 4120 = 0.31747172474861146
Training loss on iteration 4140 = 0.39801277965307236
Training loss on iteration 4160 = 0.3676234409213066
Training loss on iteration 4180 = 0.3327110517770052
Training loss on iteration 4200 = 0.3581681985408068
Training loss on iteration 4220 = 0.35786372497677804
Training loss on iteration 4240 = 0.3432293750345707
Training loss on iteration 4260 = 0.3384319365024567
Training loss on iteration 4280 = 0.32581140473484993
Training loss on iteration 4300 = 0.3873681746423244
Training loss on iteration 4320 = 0.35971401557326316
Training loss on iteration 4340 = 0.3600576683878899
Training loss on iteration 4360 = 0.37825786918401716
Training loss on iteration 4380 = 0.42263584285974504
Training loss on iteration 4400 = 0.3932352840900421
Training loss on iteration 4420 = 0.3405544959008694
Training loss on iteration 4440 = 0.3171250447630882
Training loss on iteration 4460 = 0.29349870458245275
Training loss on iteration 4480 = 0.3315675169229507
Training loss on iteration 4500 = 0.3615923322737217
Training loss on iteration 4520 = 0.44310700073838233
Training loss on iteration 4540 = 0.3126556985080242
Training loss on iteration 4560 = 0.3985258489847183
Training loss on iteration 4580 = 0.33421847224235535
Training loss on iteration 4600 = 0.3361588418483734
Training loss on iteration 4620 = 0.5220056280493737
Training loss on iteration 4640 = 0.3767164692282677
Training loss on iteration 4660 = 0.34820796847343444
Training loss on iteration 4680 = 0.37638340815901755
Training loss on iteration 4700 = 0.4068943656980991
Training loss on iteration 4720 = 0.4114310950040817
Training loss on iteration 4740 = 0.37115992680191995
Training loss on iteration 4760 = 0.37237286418676374
Training loss on iteration 4780 = 0.3870195113122463
Training loss on iteration 4800 = 0.34603581503033637
Training loss on iteration 4820 = 0.32559179067611693
Training loss on iteration 4840 = 0.33863279670476915
Training loss on iteration 4860 = 0.3913047663867474
Training loss on iteration 4880 = 0.27570202872157096
Training loss on iteration 4900 = 0.3496681671589613
Training loss on iteration 4920 = 0.3471753790974617
Training loss on iteration 4940 = 0.33102733045816424
Training loss on iteration 4960 = 0.34198797941207887
Training loss on iteration 4980 = 0.3860551483929157
Training loss on iteration 5000 = 0.38327626287937167
Training loss on iteration 5020 = 0.33988975957036016
Training loss on iteration 5040 = 0.3994309179484844
Training loss on iteration 5060 = 0.28677496537566183
Training loss on iteration 5080 = 0.3211230427026749
Training loss on iteration 5100 = 0.33177663311362265
Training loss on iteration 5120 = 0.3315868727862835
Training loss on iteration 5140 = 0.34197995252907276
Training loss on iteration 5160 = 0.3054430015385151
Training loss on iteration 5180 = 0.2721222072839737
Training loss on iteration 5200 = 0.38030579388141633
Training loss on iteration 5220 = 0.4104652300477028
Training loss on iteration 5240 = 0.340466246008873
Training loss on iteration 5260 = 0.3188977435231209
Training loss on iteration 5280 = 0.35535538047552107
Training loss on iteration 5300 = 0.3519568260759115
Training loss on iteration 5320 = 0.3269068919122219
Training loss on iteration 5340 = 0.3503885708749294
Training loss on iteration 5360 = 0.4302362747490406
Training loss on iteration 5380 = 0.3552343510091305
Training loss on iteration 5400 = 0.4036742977797985
Training loss on iteration 5420 = 0.34769670441746714
Training loss on iteration 5440 = 0.33339869529008864
Training loss on iteration 5460 = 0.39272486642003057
Training loss on iteration 5480 = 0.4180883921682835
Training loss on iteration 5500 = 0.37211313247680666
Training loss on iteration 5520 = 0.30828983783721925
Training loss on iteration 5540 = 0.33004236221313477
Training loss on iteration 5560 = 0.3700877595692873
Training loss on iteration 5580 = 0.35522379353642464
Training loss on iteration 5600 = 0.3091366820037365
Training loss on iteration 5620 = 0.3654098302125931
Training loss on iteration 5640 = 0.4078019991517067
Training loss on iteration 5660 = 0.34996571838855745
Training loss on iteration 5680 = 0.38649428710341455
Training loss on iteration 5700 = 0.3166850656270981
Training loss on iteration 5720 = 0.39970802888274193
Training loss on iteration 5740 = 0.3946880824863911
Training loss on iteration 5760 = 0.3632976055145264
Training loss on iteration 5780 = 0.42097851559519767
Training loss on iteration 5800 = 0.3171994097530842
Training loss on iteration 5820 = 0.35043226554989815
Training loss on iteration 5840 = 0.3485800839960575
Training loss on iteration 5860 = 0.3829209849238396
Training loss on iteration 5880 = 0.3863720506429672
Training loss on iteration 5900 = 0.30528045147657396
Training loss on iteration 5920 = 0.3373840242624283
Training loss on iteration 5940 = 0.3773970976471901
Training loss on iteration 5960 = 0.3192512944340706
Training loss on iteration 5980 = 0.3833909295499325
Training loss on iteration 6000 = 0.3275535374879837
Training loss on iteration 6020 = 0.38462238013744354
Training loss on iteration 6040 = 0.35462028756737707
Training loss on iteration 6060 = 0.3147315762937069
Training loss on iteration 6080 = 0.36009190455079076
Training loss on iteration 6100 = 0.4036617361009121
Training loss on iteration 6120 = 0.3808045729994774
Training loss on iteration 6140 = 0.43699107840657236
Training loss on iteration 6160 = 0.3908180147409439
Training loss on iteration 6180 = 0.3155721604824066
Training loss on iteration 6200 = 0.3565093897283077
Training loss on iteration 6220 = 0.3494312450289726
Training loss on iteration 6240 = 0.3974797762930393
Training loss on iteration 6260 = 0.4124601997435093
Training loss on iteration 6280 = 0.37597621977329254
Training loss on iteration 6300 = 0.39178583547472956
Training loss on iteration 6320 = 0.3585675355046988
Training loss on iteration 6340 = 0.41604899913072585
Training loss on iteration 6360 = 0.3646592505276203
Training loss on iteration 6380 = 0.39142758771777153
Training loss on iteration 0 = 0.3765076696872711
Training loss on iteration 20 = 0.37387405037879945
Training loss on iteration 40 = 0.35685707703232766
Training loss on iteration 60 = 0.3911393485963345
Training loss on iteration 80 = 0.36352775767445566
Training loss on iteration 100 = 0.3038491316139698
Training loss on iteration 120 = 0.30439760684967043
Training loss on iteration 140 = 0.34809622168540955
Training loss on iteration 160 = 0.31182474866509435
Training loss on iteration 180 = 0.2629393719136715
Training loss on iteration 200 = 0.3859705038368702
Training loss on iteration 220 = 0.3876232370734215
Training loss on iteration 240 = 0.31951012909412385
Training loss on iteration 260 = 0.30064943209290507
Training loss on iteration 280 = 0.320702051743865
Training loss on iteration 300 = 0.28698575496673584
Training loss on iteration 320 = 0.3345420427620411
Training loss on iteration 340 = 0.3151050455868244
Training loss on iteration 360 = 0.32591941952705383
Training loss on iteration 380 = 0.33621000945568086
Training loss on iteration 400 = 0.33037380278110506
Training loss on iteration 420 = 0.4134604953229427
Training loss on iteration 440 = 0.39027283787727357
Training loss on iteration 460 = 0.40546980649232867
Training loss on iteration 480 = 0.3268205881118774
Training loss on iteration 500 = 0.33755463287234305
Training loss on iteration 520 = 0.4411702536046505
Training loss on iteration 540 = 0.3342364221811295
Training loss on iteration 560 = 0.3450843207538128
Training loss on iteration 580 = 0.3329691231250763
Training loss on iteration 600 = 0.2949088729918003
Training loss on iteration 620 = 0.30175540782511234
Training loss on iteration 640 = 0.3577959813177586
Training loss on iteration 660 = 0.3386312685906887
Training loss on iteration 680 = 0.4035810027271509
Training loss on iteration 700 = 0.3128614105284214
Training loss on iteration 720 = 0.3179021693766117
Training loss on iteration 740 = 0.31902674660086633
Training loss on iteration 760 = 0.3361824795603752
Training loss on iteration 780 = 0.3110534965991974
Training loss on iteration 800 = 0.35071630105376245
Training loss on iteration 820 = 0.28346300721168516
Training loss on iteration 840 = 0.2961478304117918
Training loss on iteration 860 = 0.2913193367421627
Training loss on iteration 880 = 0.2873061530292034
Training loss on iteration 900 = 0.37211712300777433
Training loss on iteration 920 = 0.5339619122445584
Training loss on iteration 940 = 0.32928706258535384
Training loss on iteration 960 = 0.3182736046612263
Training loss on iteration 980 = 0.3099457070231438
Training loss on iteration 1000 = 0.299732918292284
Training loss on iteration 1020 = 0.30276861488819123
Training loss on iteration 1040 = 0.2801223322749138
Training loss on iteration 1060 = 0.3413858748972416
Training loss on iteration 1080 = 0.3034103251993656
Training loss on iteration 1100 = 0.2706745781004429
Training loss on iteration 1120 = 0.2937482759356499
Training loss on iteration 1140 = 0.3201879866421223
Training loss on iteration 1160 = 0.3230848781764507
Training loss on iteration 1180 = 0.3691681370139122
Training loss on iteration 1200 = 0.25989550054073335
Training loss on iteration 1220 = 0.3051712244749069
Training loss on iteration 1240 = 0.35360570475459097
Training loss on iteration 1260 = 0.3302358351647854
Training loss on iteration 1280 = 0.3207226499915123
Training loss on iteration 1300 = 0.33134338855743406
Training loss on iteration 1320 = 0.36419728174805643
Training loss on iteration 1340 = 0.35623309910297396
Training loss on iteration 1360 = 0.2883722990751266
Training loss on iteration 1380 = 0.3146078288555145
Training loss on iteration 1400 = 0.26111009791493417
Training loss on iteration 1420 = 0.311658938229084
Training loss on iteration 1440 = 0.33948022201657296
Training loss on iteration 1460 = 0.34107316508889196
Training loss on iteration 1480 = 0.4089703619480133
Training loss on iteration 1500 = 0.3444939874112606
Training loss on iteration 1520 = 0.3181571327149868
Training loss on iteration 1540 = 0.35126682817935945
Training loss on iteration 1560 = 0.29783554673194884
Training loss on iteration 1580 = 0.2532549023628235
Training loss on iteration 1600 = 0.5193017855286598
Training loss on iteration 1620 = 0.31685121953487394
Training loss on iteration 1640 = 0.2877391904592514
Training loss on iteration 1660 = 0.3287952460348606
Training loss on iteration 1680 = 0.2928111791610718
Training loss on iteration 1700 = 0.371852184087038
Training loss on iteration 1720 = 0.2968797214329243
Training loss on iteration 1740 = 0.33157323077321055
Training loss on iteration 1760 = 0.36045926064252853
Training loss on iteration 1780 = 0.29763606414198873
Training loss on iteration 1800 = 0.32474159449338913
Training loss on iteration 1820 = 0.38050502315163615
Training loss on iteration 1840 = 0.3362544983625412
Training loss on iteration 1860 = 0.3008858561515808
Training loss on iteration 1880 = 0.270859819278121
Training loss on iteration 1900 = 0.2683848697692156
Training loss on iteration 1920 = 0.3658449433743954
Training loss on iteration 1940 = 0.3329931512475014
Training loss on iteration 1960 = 0.30839733630418775
Training loss on iteration 1980 = 0.357721496373415
Training loss on iteration 2000 = 0.312700392305851
Training loss on iteration 2020 = 0.3135494694113731
Training loss on iteration 2040 = 0.37028127908706665
Training loss on iteration 2060 = 0.3695222981274128
Training loss on iteration 2080 = 0.3164356291294098
Training loss on iteration 2100 = 0.3057780500501394
Training loss on iteration 2120 = 0.31428709253668785
Training loss on iteration 2140 = 0.3544788330793381
Training loss on iteration 2160 = 0.3261317916214466
Training loss on iteration 2180 = 0.34338559955358505
Training loss on iteration 2200 = 0.31003578156232836
Training loss on iteration 2220 = 0.2703627489507198
Training loss on iteration 2240 = 0.3373053163290024
Training loss on iteration 2260 = 0.37576776146888735
Training loss on iteration 2280 = 0.31409292221069335
Training loss on iteration 2300 = 0.26744901686906813
Training loss on iteration 2320 = 0.3350864753127098
Training loss on iteration 2340 = 0.3744105964899063
Training loss on iteration 2360 = 0.3016415536403656
Training loss on iteration 2380 = 0.3689434215426445
Training loss on iteration 2400 = 0.36866806969046595
Training loss on iteration 2420 = 0.3822506971657276
Training loss on iteration 2440 = 0.4294944770634174
Training loss on iteration 2460 = 0.36349577009677886
Training loss on iteration 2480 = 0.3118037708103657
Training loss on iteration 2500 = 0.3604346804320812
Training loss on iteration 2520 = 0.3245979882776737
Training loss on iteration 2540 = 0.28281965777277945
Training loss on iteration 2560 = 0.3111648984253407
Training loss on iteration 2580 = 0.4087548442184925
Training loss on iteration 2600 = 0.3255276717245579
Training loss on iteration 2620 = 0.3662234306335449
Training loss on iteration 2640 = 0.34907386228442194
Training loss on iteration 2660 = 0.28548497185111044
Training loss on iteration 2680 = 0.34334871284663676
Training loss on iteration 2700 = 0.3264945439994335
Training loss on iteration 2720 = 0.4000396229326725
Training loss on iteration 2740 = 0.3608152575790882
Training loss on iteration 2760 = 0.36141619011759757
Training loss on iteration 2780 = 0.346174731105566
Training loss on iteration 2800 = 0.35003590434789655
Training loss on iteration 2820 = 0.34480746909976007
Training loss on iteration 2840 = 0.27736031636595726
Training loss on iteration 2860 = 0.3103918179869652
Training loss on iteration 2880 = 0.31931849867105483
Training loss on iteration 2900 = 0.29727867990732193
Training loss on iteration 2920 = 0.27850169837474825
Training loss on iteration 2940 = 0.26897567585110665
Training loss on iteration 2960 = 0.3388500578701496
Training loss on iteration 2980 = 0.3592465780675411
Training loss on iteration 3000 = 0.3583786584436893
Training loss on iteration 3020 = 0.28671934083104134
Training loss on iteration 3040 = 0.32417728677392005
Training loss on iteration 3060 = 0.30984588488936426
Training loss on iteration 3080 = 0.329233019053936
Training loss on iteration 3100 = 0.4620875731110573
Training loss on iteration 3120 = 0.3355634868144989
Training loss on iteration 3140 = 0.3858783431351185
Training loss on iteration 3160 = 0.32431840524077415
Training loss on iteration 3180 = 0.33662383928894996
Training loss on iteration 3200 = 0.3241205833852291
Training loss on iteration 3220 = 0.2948782451450825
Training loss on iteration 3240 = 0.27583171278238294
Training loss on iteration 3260 = 0.3343009248375893
Training loss on iteration 3280 = 0.2976127564907074
Training loss on iteration 3300 = 0.37878466621041296
Training loss on iteration 3320 = 0.3903594933450222
Training loss on iteration 3340 = 0.339762382209301
Training loss on iteration 3360 = 0.35852388888597486
Training loss on iteration 3380 = 0.2975471757352352
Training loss on iteration 3400 = 0.2793115064501762
Training loss on iteration 3420 = 0.35567205771803856
Training loss on iteration 3440 = 0.34179812148213384
Training loss on iteration 3460 = 0.31193266697227956
Training loss on iteration 3480 = 0.30106965750455855
Training loss on iteration 3500 = 0.35133014991879463
Training loss on iteration 3520 = 0.29157488234341145
Training loss on iteration 3540 = 0.41759161874651907
Training loss on iteration 3560 = 0.3720769539475441
Training loss on iteration 3580 = 0.37302461341023446
Training loss on iteration 3600 = 0.3593530021607876
Training loss on iteration 3620 = 0.2796779774129391
Training loss on iteration 3640 = 0.38832392245531083
Training loss on iteration 3660 = 0.31405442878603934
Training loss on iteration 3680 = 0.2688629388809204
Training loss on iteration 3700 = 0.3469515539705753
Training loss on iteration 3720 = 0.430599058419466
Training loss on iteration 3740 = 0.31851551681756973
Training loss on iteration 3760 = 0.3512902081012726
Training loss on iteration 3780 = 0.30622653439641
Training loss on iteration 3800 = 0.3646550141274929
Training loss on iteration 3820 = 0.2986421775072813
Training loss on iteration 3840 = 0.31645662561058996
Training loss on iteration 3860 = 0.3292678356170654
Training loss on iteration 3880 = 0.29891393408179284
Training loss on iteration 3900 = 0.3885609723627567
Training loss on iteration 3920 = 0.3578786991536617
Training loss on iteration 3940 = 0.33557692617177964
Training loss on iteration 3960 = 0.24586354792118073
Training loss on iteration 3980 = 0.4174002446234226
Training loss on iteration 4000 = 0.34972579404711723
Training loss on iteration 4020 = 0.3625472471117973
Training loss on iteration 4040 = 0.30308995991945265
Training loss on iteration 4060 = 0.3381902664899826
Training loss on iteration 4080 = 0.41390422098338603
Training loss on iteration 4100 = 0.31661994233727453
Training loss on iteration 4120 = 0.3281434401869774
Training loss on iteration 4140 = 0.29001866206526755
Training loss on iteration 4160 = 0.34105798602104187
Training loss on iteration 4180 = 0.32711296565830705
Training loss on iteration 4200 = 0.35763185024261473
Training loss on iteration 4220 = 0.3503643684089184
Training loss on iteration 4240 = 0.3790707506239414
Training loss on iteration 4260 = 0.3361304014921188
Training loss on iteration 4280 = 0.32167147696018217
Training loss on iteration 4300 = 0.32808022722601893
Training loss on iteration 4320 = 0.2923442587256432
Training loss on iteration 4340 = 0.2903459154069424
Training loss on iteration 4360 = 0.34204253256320954
Training loss on iteration 4380 = 0.30217941477894783
Training loss on iteration 4400 = 0.28516371697187426
Training loss on iteration 4420 = 0.28937597796320913
Training loss on iteration 4440 = 0.30367124229669573
Training loss on iteration 4460 = 0.33832032755017283
Training loss on iteration 4480 = 0.3264355257153511
Training loss on iteration 4500 = 0.280849925801158
Training loss on iteration 4520 = 0.3065428599715233
Training loss on iteration 4540 = 0.32609976679086683
Training loss on iteration 4560 = 0.3857786521315575
Training loss on iteration 4580 = 0.38419310599565504
Training loss on iteration 4600 = 0.3787889629602432
Training loss on iteration 4620 = 0.4375509679317474
Training loss on iteration 4640 = 0.3910030201077461
Training loss on iteration 4660 = 0.4355623111128807
Training loss on iteration 4680 = 0.35954973474144936
Training loss on iteration 4700 = 0.2927079536020756
Training loss on iteration 4720 = 0.35464646071195605
Training loss on iteration 4740 = 0.32611485868692397
Training loss on iteration 4760 = 0.33964590504765513
Training loss on iteration 4780 = 0.3823399871587753
Training loss on iteration 4800 = 0.30883799195289613
Training loss on iteration 4820 = 0.2693923942744732
Training loss on iteration 4840 = 0.3365653619170189
Training loss on iteration 4860 = 0.36491772532463074
Training loss on iteration 4880 = 0.31177637055516244
Training loss on iteration 4900 = 0.2768990110605955
Training loss on iteration 4920 = 0.3083453603088856
Training loss on iteration 4940 = 0.31697814986109735
Training loss on iteration 4960 = 0.3130107294768095
Training loss on iteration 4980 = 0.2708046086132526
Training loss on iteration 5000 = 0.3271595545113087
Training loss on iteration 5020 = 0.4803133592009544
Training loss on iteration 5040 = 0.3129720576107502
Training loss on iteration 5060 = 0.2957208279520273
Training loss on iteration 5080 = 0.4233379952609539
Training loss on iteration 5100 = 0.3626460686326027
Training loss on iteration 5120 = 0.3568588890135288
Training loss on iteration 5140 = 0.3274030670523643
Training loss on iteration 5160 = 1.2035198122262956
Training loss on iteration 5180 = 0.49031926691532135
Training loss on iteration 5200 = 0.3789949335157871
Training loss on iteration 5220 = 0.3699482947587967
Training loss on iteration 5240 = 0.3667908273637295
Training loss on iteration 5260 = 0.38299923837184907
Training loss on iteration 5280 = 0.42869134321808816
Training loss on iteration 5300 = 0.4272129155695438
Training loss on iteration 5320 = 0.33184765577316283
Training loss on iteration 5340 = 0.34403168931603434
Training loss on iteration 5360 = 0.46158352568745614
Training loss on iteration 5380 = 0.4129063956439495
Training loss on iteration 5400 = 0.3426264010369778
Training loss on iteration 5420 = 0.321157418936491
Training loss on iteration 5440 = 0.3628985084593296
Training loss on iteration 5460 = 0.3660008139908314
Training loss on iteration 5480 = 0.43754215389490125
Training loss on iteration 5500 = 0.3490429401397705
Training loss on iteration 5520 = 0.3740987882018089
Training loss on iteration 5540 = 0.3153079241514206
Training loss on iteration 5560 = 0.36463338434696196
Training loss on iteration 5580 = 0.3145030055195093
Training loss on iteration 5600 = 0.34425423443317416
Training loss on iteration 5620 = 0.3932221971452236
Training loss on iteration 5640 = 0.3497632689774036
Training loss on iteration 5660 = 0.3701066434383392
Training loss on iteration 5680 = 0.3031088922172785
Training loss on iteration 5700 = 0.4078929923474789
Training loss on iteration 5720 = 0.34673078134655955
Training loss on iteration 5740 = 0.346563509106636
Training loss on iteration 5760 = 0.37099579349160194
Training loss on iteration 5780 = 0.4475643090903759
Training loss on iteration 5800 = 0.3033179171383381
Training loss on iteration 5820 = 0.33284441456198693
Training loss on iteration 5840 = 0.34486701637506484
Training loss on iteration 5860 = 0.28524808064103124
Training loss on iteration 5880 = 0.3331503916531801
Training loss on iteration 5900 = 0.29385719299316404
Training loss on iteration 5920 = 0.3469857282936573
Training loss on iteration 5940 = 0.3052956096827984
Training loss on iteration 5960 = 0.2600089155137539
Training loss on iteration 5980 = 0.33254273012280466
Training loss on iteration 6000 = 0.3572832018136978
Training loss on iteration 6020 = 0.3617834411561489
Training loss on iteration 6040 = 0.3906091690063477
Training loss on iteration 6060 = 0.2943260625004768
Training loss on iteration 6080 = 0.3238856039941311
Training loss on iteration 6100 = 0.4166844829916954
Training loss on iteration 6120 = 0.3305260106921196
Training loss on iteration 6140 = 0.2891926035284996
Training loss on iteration 6160 = 0.3337668739259243
Training loss on iteration 6180 = 0.29078282713890075
Training loss on iteration 6200 = 0.3169105812907219
Training loss on iteration 6220 = 0.39655746817588805
Training loss on iteration 6240 = 0.2831907946616411
Training loss on iteration 6260 = 0.3704308047890663
Training loss on iteration 6280 = 0.3852641187608242
Training loss on iteration 6300 = 0.6017860107123851
Training loss on iteration 6320 = 0.3023760937154293
Training loss on iteration 6340 = 0.29375638961791994
Training loss on iteration 6360 = 0.32905348315835
Training loss on iteration 6380 = 0.3268666245043278
Training loss on iteration 0 = 0.166785329580307
Training loss on iteration 20 = 0.30660426393151285
Training loss on iteration 40 = 0.3256233140826225
Training loss on iteration 60 = 0.45793619081377984
Training loss on iteration 80 = 0.2540636293590069
Training loss on iteration 100 = 0.30714993849396705
Training loss on iteration 120 = 0.2454301979392767
Training loss on iteration 140 = 0.31020034924149514
Training loss on iteration 160 = 0.27189239524304865
Training loss on iteration 180 = 0.2663956850767136
Training loss on iteration 200 = 0.302313981205225
Training loss on iteration 220 = 0.25681373104453087
Training loss on iteration 240 = 0.33298349380493164
Training loss on iteration 260 = 0.2946739159524441
Training loss on iteration 280 = 0.2813080459833145
Training loss on iteration 300 = 0.32910302430391314
Training loss on iteration 320 = 0.2626605711877346
Training loss on iteration 340 = 0.3256878428161144
Training loss on iteration 360 = 0.296427833288908
Training loss on iteration 380 = 0.3296678803861141
Training loss on iteration 400 = 0.2948412820696831
Training loss on iteration 420 = 0.2949971750378609
Training loss on iteration 440 = 0.2684934262186289
Training loss on iteration 460 = 0.28628339543938636
Training loss on iteration 480 = 0.30503764301538466
Training loss on iteration 500 = 0.30460174307227134
Training loss on iteration 520 = 0.322663651406765
Training loss on iteration 540 = 0.2601132314652205
Training loss on iteration 560 = 0.2945972681045532
Training loss on iteration 580 = 0.2884629309177399
Training loss on iteration 600 = 0.3109516091644764
Training loss on iteration 620 = 0.30639129430055617
Training loss on iteration 640 = 0.3258179619908333
Training loss on iteration 660 = 0.3586334280669689
Training loss on iteration 680 = 0.3073719941079617
Training loss on iteration 700 = 0.2900288291275501
Training loss on iteration 720 = 0.2918634556233883
Training loss on iteration 740 = 0.27890328839421274
Training loss on iteration 760 = 0.34670685715973376
Training loss on iteration 780 = 0.2545172683894634
Training loss on iteration 800 = 0.3441055864095688
Training loss on iteration 820 = 0.279145710170269
Training loss on iteration 840 = 0.3408291064202785
Training loss on iteration 860 = 0.3564365614205599
Training loss on iteration 880 = 0.35298525243997575
Training loss on iteration 900 = 0.28974607214331627
Training loss on iteration 920 = 0.3496453255414963
Training loss on iteration 940 = 0.31944766119122503
Training loss on iteration 960 = 0.3213993698358536
Training loss on iteration 980 = 0.3725788608193398
Training loss on iteration 1000 = 0.3253208942711353
Training loss on iteration 1020 = 0.3761378385126591
Training loss on iteration 1040 = 0.2818645521998405
Training loss on iteration 1060 = 0.2764064081013203
Training loss on iteration 1080 = 0.28027293235063555
Training loss on iteration 1100 = 0.3625018108636141
Training loss on iteration 1120 = 0.2505106132477522
Training loss on iteration 1140 = 0.3061161793768406
Training loss on iteration 1160 = 0.33894639313220976
Training loss on iteration 1180 = 0.2922330267727375
Training loss on iteration 1200 = 0.35863435864448545
Training loss on iteration 1220 = 0.31466552838683126
Training loss on iteration 1240 = 0.36933187320828437
Training loss on iteration 1260 = 0.32450651153922083
Training loss on iteration 1280 = 0.26711057871580124
Training loss on iteration 1300 = 0.34133263379335405
Training loss on iteration 1320 = 0.2963162504136562
Training loss on iteration 1340 = 0.2980640947818756
Training loss on iteration 1360 = 0.24164483249187468
Training loss on iteration 1380 = 0.27128597944974897
Training loss on iteration 1400 = 0.30790734738111497
Training loss on iteration 1420 = 0.3220015585422516
Training loss on iteration 1440 = 0.32463912591338157
Training loss on iteration 1460 = 0.38889230638742445
Training loss on iteration 1480 = 0.3218483664095402
Training loss on iteration 1500 = 0.31328844726085664
Training loss on iteration 1520 = 0.27445204854011535
Training loss on iteration 1540 = 0.2711889658123255
Training loss on iteration 1560 = 0.29913428574800494
Training loss on iteration 1580 = 0.3233150541782379
Training loss on iteration 1600 = 0.2854015350341797
Training loss on iteration 1620 = 0.39160981103777887
Training loss on iteration 1640 = 0.313570586591959
Training loss on iteration 1660 = 0.3834034502506256
Training loss on iteration 1680 = 0.2384822614490986
Training loss on iteration 1700 = 0.30289447233080863
Training loss on iteration 1720 = 0.2782918609678745
Training loss on iteration 1740 = 0.3398475393652916
Training loss on iteration 1760 = 0.2544807031750679
Training loss on iteration 1780 = 0.322817762196064
Training loss on iteration 1800 = 0.3111179396510124
Training loss on iteration 1820 = 0.3301411956548691
Training loss on iteration 1840 = 0.29715118929743767
Training loss on iteration 1860 = 0.311481612175703
Training loss on iteration 1880 = 0.3169391743838787
Training loss on iteration 1900 = 0.3135412886738777
Training loss on iteration 1920 = 0.38984056785702703
Training loss on iteration 1940 = 0.2769713122397661
Training loss on iteration 1960 = 0.31783949583768845
Training loss on iteration 1980 = 0.27513209655880927
Training loss on iteration 2000 = 0.33280378840863706
Training loss on iteration 2020 = 0.3140993721783161
Training loss on iteration 2040 = 0.28035039007663726
Training loss on iteration 2060 = 0.2998430758714676
Training loss on iteration 2080 = 0.30031599253416064
Training loss on iteration 2100 = 0.3087862506508827
Training loss on iteration 2120 = 0.2786136828362942
Training loss on iteration 2140 = 0.2833718739449978
Training loss on iteration 2160 = 0.2896170973777771
Training loss on iteration 2180 = 0.29006330445408823
Training loss on iteration 2200 = 0.32932736054062844
Training loss on iteration 2220 = 0.2757549859583378
Training loss on iteration 2240 = 0.3154498368501663
Training loss on iteration 2260 = 0.33576508387923243
Training loss on iteration 2280 = 0.32057988792657854
Training loss on iteration 2300 = 0.3198838487267494
Training loss on iteration 2320 = 0.29239348545670507
Training loss on iteration 2340 = 0.24045871533453464
Training loss on iteration 2360 = 0.32191338017582893
Training loss on iteration 2380 = 0.29914812222123144
Training loss on iteration 2400 = 0.3109307445585728
Training loss on iteration 2420 = 0.36654667034745214
Training loss on iteration 2440 = 0.34012898728251456
Training loss on iteration 2460 = 0.3078537054359913
Training loss on iteration 2480 = 0.4048614799976349
Training loss on iteration 2500 = 0.3043948419392109
Training loss on iteration 2520 = 0.3217625141143799
Training loss on iteration 2540 = 0.258143462985754
Training loss on iteration 2560 = 0.31830400228500366
Training loss on iteration 2580 = 0.3306654836982489
Training loss on iteration 2600 = 0.2966249890625477
Training loss on iteration 2620 = 0.3380020294338465
Training loss on iteration 2640 = 0.31194853857159616
Training loss on iteration 2660 = 0.2972974471747875
Training loss on iteration 2680 = 0.31129222773015497
Training loss on iteration 2700 = 0.27934583872556684
Training loss on iteration 2720 = 0.32665527760982516
Training loss on iteration 2740 = 0.3390272717922926
Training loss on iteration 2760 = 0.2787380665540695
Training loss on iteration 2780 = 0.3273474916815758
Training loss on iteration 2800 = 0.32875872403383255
Training loss on iteration 2820 = 0.2802557796239853
Training loss on iteration 2840 = 0.30834106281399726
Training loss on iteration 2860 = 0.3517025426030159
Training loss on iteration 2880 = 0.319225949048996
Training loss on iteration 2900 = 0.33245252519845964
Training loss on iteration 2920 = 0.4065053716301918
Training loss on iteration 2940 = 0.34277346059679986
Training loss on iteration 2960 = 0.3243459723889828
Training loss on iteration 2980 = 0.3655179537832737
Training loss on iteration 3000 = 0.2942624375224113
Training loss on iteration 3020 = 0.29227553606033324
Training loss on iteration 3040 = 0.33523854315280915
Training loss on iteration 3060 = 0.323682714253664
Training loss on iteration 3080 = 0.2901919513940811
Training loss on iteration 3100 = 0.3469190925359726
Training loss on iteration 3120 = 0.39524415507912636
Training loss on iteration 3140 = 0.27701141908764837
Training loss on iteration 3160 = 0.3295178547501564
Training loss on iteration 3180 = 0.31054071709513664
Training loss on iteration 3200 = 0.40281484425067904
Training loss on iteration 3220 = 0.4083653040230274
Training loss on iteration 3240 = 0.4299275830388069
Training loss on iteration 3260 = 0.2899046324193478
Training loss on iteration 3280 = 0.2530238352715969
Training loss on iteration 3300 = 0.3153263874351978
Training loss on iteration 3320 = 0.317672622948885
Training loss on iteration 3340 = 0.2812483936548233
Training loss on iteration 3360 = 0.35343287140130997
Training loss on iteration 3380 = 0.2829193748533726
Training loss on iteration 3400 = 0.34168321937322615
Training loss on iteration 3420 = 0.3321975737810135
Training loss on iteration 3440 = 0.2874602645635605
Training loss on iteration 3460 = 0.351747539639473
Training loss on iteration 3480 = 0.3156595569103956
Training loss on iteration 3500 = 0.3204359419643879
Training loss on iteration 3520 = 0.34457223415374755
Training loss on iteration 3540 = 0.31325890496373177
Training loss on iteration 3560 = 0.3601524755358696
Training loss on iteration 3580 = 0.5282701425254345
Training loss on iteration 3600 = 0.3267062582075596
Training loss on iteration 3620 = 0.3106433428823948
Training loss on iteration 3640 = 0.2721310079097748
Training loss on iteration 3660 = 0.31335781291127207
Training loss on iteration 3680 = 0.29628586694598197
Training loss on iteration 3700 = 0.28661084175109863
Training loss on iteration 3720 = 0.27906077802181245
Training loss on iteration 3740 = 0.3333618208765984
Training loss on iteration 3760 = 0.27698405310511587
Training loss on iteration 3780 = 0.2983923636376858
Training loss on iteration 3800 = 0.3430023223161697
Training loss on iteration 3820 = 0.25020434856414797
Training loss on iteration 3840 = 0.2919948101043701
Training loss on iteration 3860 = 0.3296234130859375
Training loss on iteration 3880 = 0.28468247428536414
Training loss on iteration 3900 = 0.3370214208960533
Training loss on iteration 3920 = 0.33129342198371886
Training loss on iteration 3940 = 0.29346988573670385
Training loss on iteration 3960 = 0.28072510920464994
Training loss on iteration 3980 = 0.3340094588696957
Training loss on iteration 4000 = 0.3470104120671749
Training loss on iteration 4020 = 0.26285869926214217
Training loss on iteration 4040 = 0.30440926253795625
Training loss on iteration 4060 = 0.3364158645272255
Training loss on iteration 4080 = 0.31330891996622084
Training loss on iteration 4100 = 0.3039459496736526
Training loss on iteration 4120 = 0.3503609776496887
Training loss on iteration 4140 = 0.4190972581505775
Training loss on iteration 4160 = 0.35956900417804716
Training loss on iteration 4180 = 0.2911288537085056
Training loss on iteration 4200 = 0.33493315130472184
Training loss on iteration 4220 = 0.30939554497599603
Training loss on iteration 4240 = 0.3862119477242231
Training loss on iteration 4260 = 0.3429233506321907
Training loss on iteration 4280 = 0.30299880653619765
Training loss on iteration 4300 = 0.2701157622039318
Training loss on iteration 4320 = 0.28174990266561506
Training loss on iteration 4340 = 0.3167438417673111
Training loss on iteration 4360 = 0.32025465592741964
Training loss on iteration 4380 = 0.2856751210987568
Training loss on iteration 4400 = 0.33115082830190656
Training loss on iteration 4420 = 0.35462666526436804
Training loss on iteration 4440 = 0.31860324889421465
Training loss on iteration 4460 = 0.31872354447841644
Training loss on iteration 4480 = 0.31091574430465696
Training loss on iteration 4500 = 0.3479767329990864
Training loss on iteration 4520 = 0.3167592525482178
Training loss on iteration 4540 = 0.38576795160770416
Training loss on iteration 4560 = 0.3580417066812515
Training loss on iteration 4580 = 0.3325080618262291
Training loss on iteration 4600 = 0.3959790006279945
Training loss on iteration 4620 = 0.31203325614333155
Training loss on iteration 4640 = 0.3767059452831745
Training loss on iteration 4660 = 0.31330482065677645
Training loss on iteration 4680 = 0.36680073142051695
Training loss on iteration 4700 = 0.3019811656326056
Training loss on iteration 4720 = 0.3096939422190189
Training loss on iteration 4740 = 0.3403995469212532
Training loss on iteration 4760 = 0.26963753141462804
Training loss on iteration 4780 = 0.33499194905161855
Training loss on iteration 4800 = 0.3254824958741665
Training loss on iteration 4820 = 0.36435612887144087
Training loss on iteration 4840 = 0.30740371979773046
Training loss on iteration 4860 = 0.3005125232040882
Training loss on iteration 4880 = 0.29706998616456987
Training loss on iteration 4900 = 0.37801278978586195
Training loss on iteration 4920 = 0.28029644563794137
Training loss on iteration 4940 = 0.2643086418509483
Training loss on iteration 4960 = 0.2760937988758087
Training loss on iteration 4980 = 0.3091648079454899
Training loss on iteration 5000 = 0.30871639475226403
Training loss on iteration 5020 = 0.39979317784309387
Training loss on iteration 5040 = 0.31006488054990766
Training loss on iteration 5060 = 0.33470297083258627
Training loss on iteration 5080 = 0.3557304099202156
Training loss on iteration 5100 = 0.3848717652261257
Training loss on iteration 5120 = 0.2759645603597164
Training loss on iteration 5140 = 0.3281625933945179
Training loss on iteration 5160 = 0.3222689516842365
Training loss on iteration 5180 = 0.301690524071455
Training loss on iteration 5200 = 0.32074864879250525
Training loss on iteration 5220 = 0.3381999596953392
Training loss on iteration 5240 = 0.29964782670140266
Training loss on iteration 5260 = 0.29087780341506003
Training loss on iteration 5280 = 0.3399487160146236
Training loss on iteration 5300 = 0.33029706999659536
Training loss on iteration 5320 = 0.3390029989182949
Training loss on iteration 5340 = 0.3709542483091354
Training loss on iteration 5360 = 0.34332425370812414
Training loss on iteration 5380 = 0.2932380385696888
Training loss on iteration 5400 = 0.3659570336341858
Training loss on iteration 5420 = 0.4623305030167103
Training loss on iteration 5440 = 0.33824345618486407
Training loss on iteration 5460 = 0.26344964019954203
Training loss on iteration 5480 = 0.36578951254487035
Training loss on iteration 5500 = 0.33391943722963335
Training loss on iteration 5520 = 0.344754908233881
Training loss on iteration 5540 = 0.34640502855181693
Training loss on iteration 5560 = 0.3001590706408024
Training loss on iteration 5580 = 0.24272151291370392
Training loss on iteration 5600 = 0.2892150156199932
Training loss on iteration 5620 = 0.30512823462486266
Training loss on iteration 5640 = 0.2895647682249546
Training loss on iteration 5660 = 0.32812992930412294
Training loss on iteration 5680 = 0.317175667732954
Training loss on iteration 5700 = 0.38170956522226335
Training loss on iteration 5720 = 0.301936886459589
Training loss on iteration 5740 = 0.3533540315926075
Training loss on iteration 5760 = 0.3733272053301334
Training loss on iteration 5780 = 0.36621673256158827
Training loss on iteration 5800 = 0.3567190505564213
Training loss on iteration 5820 = 0.3668589599430561
Training loss on iteration 5840 = 0.2998978726565838
Training loss on iteration 5860 = 0.34564163237810136
Training loss on iteration 5880 = 0.2742541186511517
Training loss on iteration 5900 = 0.3270600840449333
Training loss on iteration 5920 = 0.3126684702932835
Training loss on iteration 5940 = 0.29066129922866824
Training loss on iteration 5960 = 0.28733880147337915
Training loss on iteration 5980 = 0.3254571009427309
Training loss on iteration 6000 = 0.3454425953328609
Training loss on iteration 6020 = 0.3537192836403847
Training loss on iteration 6040 = 0.38708514645695685
Training loss on iteration 6060 = 0.3590336114168167
Training loss on iteration 6080 = 0.3327327936887741
Training loss on iteration 6100 = 0.3264772292226553
Training loss on iteration 6120 = 0.35418443754315376
Training loss on iteration 6140 = 0.2902941547334194
Training loss on iteration 6160 = 0.3327547572553158
Training loss on iteration 6180 = 0.28642527721822264
Training loss on iteration 6200 = 0.38455267027020457
Training loss on iteration 6220 = 0.33833588734269143
Training loss on iteration 6240 = 0.33875386938452723
Training loss on iteration 6260 = 0.3618837758898735
Training loss on iteration 6280 = 0.3553819768130779
Training loss on iteration 6300 = 0.29598111733794213
Training loss on iteration 6320 = 0.3996809981763363
Training loss on iteration 6340 = 0.3003373026847839
Training loss on iteration 6360 = 0.2874334201216698
Training loss on iteration 6380 = 0.3311543479561806
Training loss on iteration 0 = 0.2348959743976593
Training loss on iteration 20 = 0.23779814317822456
Training loss on iteration 40 = 0.2971033029258251
Training loss on iteration 60 = 0.3007357187569141
Training loss on iteration 80 = 0.29829267859458924
Training loss on iteration 100 = 0.34674557819962504
Training loss on iteration 120 = 0.30811988934874535
Training loss on iteration 140 = 0.2893322303891182
Training loss on iteration 160 = 0.30061252415180206
Training loss on iteration 180 = 0.31976431533694266
Training loss on iteration 200 = 0.30663339197635653
Training loss on iteration 220 = 0.2920746698975563
Training loss on iteration 240 = 0.2582035556435585
Training loss on iteration 260 = 0.2944653108716011
Training loss on iteration 280 = 0.26585094854235647
Training loss on iteration 300 = 0.2870532348752022
Training loss on iteration 320 = 0.32642394602298735
Training loss on iteration 340 = 0.3135606862604618
Training loss on iteration 360 = 0.26845428720116615
Training loss on iteration 380 = 0.28762712590396405
Training loss on iteration 400 = 0.29463231898844244
Training loss on iteration 420 = 0.2945053346455097
Training loss on iteration 440 = 0.27593150176107883
Training loss on iteration 460 = 0.2945215940475464
Training loss on iteration 480 = 0.34806864708662033
Training loss on iteration 500 = 0.2929045207798481
Training loss on iteration 520 = 0.24466062784194947
Training loss on iteration 540 = 0.28701326623559
Training loss on iteration 560 = 0.28243680894374845
Training loss on iteration 580 = 0.2968440756201744
Training loss on iteration 600 = 0.3080104887485504
Training loss on iteration 620 = 0.28853316232562065
Training loss on iteration 640 = 0.2798173263669014
Training loss on iteration 660 = 0.26074982434511185
Training loss on iteration 680 = 0.26910231858491895
Training loss on iteration 700 = 0.2868889331817627
Training loss on iteration 720 = 0.25797003582119943
Training loss on iteration 740 = 0.2842250347137451
Training loss on iteration 760 = 0.2721047677099705
Training loss on iteration 780 = 0.3467600718140602
Training loss on iteration 800 = 0.3075244091451168
Training loss on iteration 820 = 0.2557590637356043
Training loss on iteration 840 = 0.32427735179662703
Training loss on iteration 860 = 0.3032170131802559
Training loss on iteration 880 = 0.34867310598492623
Training loss on iteration 900 = 0.3550421752035618
Training loss on iteration 920 = 0.3553417004644871
Training loss on iteration 940 = 0.3068403549492359
Training loss on iteration 960 = 0.26734871342778205
Training loss on iteration 980 = 0.24560330361127852
Training loss on iteration 1000 = 0.344574985653162
Training loss on iteration 1020 = 0.30118981227278707
Training loss on iteration 1040 = 0.35606495290994644
Training loss on iteration 1060 = 0.4239706553518772
Training loss on iteration 1080 = 0.2977126508951187
Training loss on iteration 1100 = 0.31032962799072267
Training loss on iteration 1120 = 0.3129252105951309
Training loss on iteration 1140 = 0.37475484907627105
Training loss on iteration 1160 = 0.2456641413271427
Training loss on iteration 1180 = 0.294094642996788
Training loss on iteration 1200 = 0.27064423710107804
Training loss on iteration 1220 = 0.2673026107251644
Training loss on iteration 1240 = 0.28956863954663276
Training loss on iteration 1260 = 0.29951489306986334
Training loss on iteration 1280 = 0.2825311444699764
Training loss on iteration 1300 = 0.3061421558260918
Training loss on iteration 1320 = 0.28223170414566995
Training loss on iteration 1340 = 0.29079833477735517
Training loss on iteration 1360 = 0.2663515582680702
Training loss on iteration 1380 = 0.27270656228065493
Training loss on iteration 1400 = 0.3190222829580307
Training loss on iteration 1420 = 0.28789953589439393
Training loss on iteration 1440 = 0.33149092122912405
Training loss on iteration 1460 = 0.34177993014454844
Training loss on iteration 1480 = 0.3413071297109127
Training loss on iteration 1500 = 0.37885804772377013
Training loss on iteration 1520 = 0.25079321675002575
Training loss on iteration 1540 = 0.2683632031083107
Training loss on iteration 1560 = 0.28000567108392715
Training loss on iteration 1580 = 0.2854128792881966
Training loss on iteration 1600 = 0.28940420746803286
Training loss on iteration 1620 = 0.28090567141771317
Training loss on iteration 1640 = 0.27811602875590324
Training loss on iteration 1660 = 0.288433650135994
Training loss on iteration 1680 = 0.3316041611135006
Training loss on iteration 1700 = 0.3056019403040409
Training loss on iteration 1720 = 0.27306420654058455
Training loss on iteration 1740 = 0.2686320684850216
Training loss on iteration 1760 = 0.3022871442139149
Training loss on iteration 1780 = 0.3199173897504807
Training loss on iteration 1800 = 0.31268942579627035
Training loss on iteration 1820 = 0.3424105033278465
Training loss on iteration 1840 = 0.2999340891838074
Training loss on iteration 1860 = 0.2894319906830788
Training loss on iteration 1880 = 0.25390123277902604
Training loss on iteration 1900 = 0.25835082232952117
Training loss on iteration 1920 = 0.32614932358264925
Training loss on iteration 1940 = 0.34259838089346883
Training loss on iteration 1960 = 0.30365467965602877
Training loss on iteration 1980 = 0.25383994951844213
Training loss on iteration 2000 = 0.2854002803564072
Training loss on iteration 2020 = 0.3506671294569969
Training loss on iteration 2040 = 0.27350089624524115
Training loss on iteration 2060 = 0.24031604528427125
Training loss on iteration 2080 = 0.2827720679342747
Training loss on iteration 2100 = 0.2868466842919588
Training loss on iteration 2120 = 0.30452210903167726
Training loss on iteration 2140 = 0.2621841195970774
Training loss on iteration 2160 = 0.3084276683628559
Training loss on iteration 2180 = 0.26492692679166796
Training loss on iteration 2200 = 0.30019870325922965
Training loss on iteration 2220 = 0.2685971610248089
Training loss on iteration 2240 = 0.30612715147435665
Training loss on iteration 2260 = 0.3141501918435097
Training loss on iteration 2280 = 0.32795797735452653
Training loss on iteration 2300 = 0.2631509892642498
Training loss on iteration 2320 = 0.34075886756181717
Training loss on iteration 2340 = 0.29238464385271073
Training loss on iteration 2360 = 0.2650491900742054
Training loss on iteration 2380 = 0.31138789355754853
Training loss on iteration 2400 = 0.2160108931362629
Training loss on iteration 2420 = 0.29376793578267096
Training loss on iteration 2440 = 0.27265016958117483
Training loss on iteration 2460 = 0.316711837053299
Training loss on iteration 2480 = 0.2738596245646477
Training loss on iteration 2500 = 0.32322938218712804
Training loss on iteration 2520 = 0.32931316643953323
Training loss on iteration 2540 = 0.3025780975818634
Training loss on iteration 2560 = 0.28834934495389464
Training loss on iteration 2580 = 0.2986744873225689
Training loss on iteration 2600 = 0.3568705372512341
Training loss on iteration 2620 = 0.3180368900299072
Training loss on iteration 2640 = 0.3302285440266132
Training loss on iteration 2660 = 0.2597810637205839
Training loss on iteration 2680 = 0.3162978582084179
Training loss on iteration 2700 = 0.3143711619079113
Training loss on iteration 2720 = 0.33411222249269484
Training loss on iteration 2740 = 0.3239018991589546
Training loss on iteration 2760 = 0.28215430304408073
Training loss on iteration 2780 = 0.2749753125011921
Training loss on iteration 2800 = 0.3375017926096916
Training loss on iteration 2820 = 0.30870282277464867
Training loss on iteration 2840 = 0.30693056881427766
Training loss on iteration 2860 = 0.3295496851205826
Training loss on iteration 2880 = 0.3143659047782421
Training loss on iteration 2900 = 0.331766127794981
Training loss on iteration 2920 = 0.3010162070393562
Training loss on iteration 2940 = 0.25832444578409197
Training loss on iteration 2960 = 0.3059051789343357
Training loss on iteration 2980 = 0.40987381860613825
Training loss on iteration 3000 = 0.3000919781625271
Training loss on iteration 3020 = 0.3021618895232677
Training loss on iteration 3040 = 0.3010298013687134
Training loss on iteration 3060 = 0.30110040977597236
Training loss on iteration 3080 = 0.33499990329146384
Training loss on iteration 3100 = 0.3104968756437302
Training loss on iteration 3120 = 0.28338707983493805
Training loss on iteration 3140 = 0.33083443678915503
Training loss on iteration 3160 = 0.340166250616312
Training loss on iteration 3180 = 0.30162454321980475
Training loss on iteration 3200 = 0.2874566908925772
Training loss on iteration 3220 = 0.33322315812110903
Training loss on iteration 3240 = 0.2981960289180279
Training loss on iteration 3260 = 0.32514193654060364
Training loss on iteration 3280 = 0.2951808854937553
Training loss on iteration 3300 = 0.27461940310895444
Training loss on iteration 3320 = 0.33348713666200636
Training loss on iteration 3340 = 0.3171728603541851
Training loss on iteration 3360 = 0.342881341278553
Training loss on iteration 3380 = 0.30545140951871874
Training loss on iteration 3400 = 0.31722072288393977
Training loss on iteration 3420 = 0.3187411528080702
Training loss on iteration 3440 = 0.3791138667613268
Training loss on iteration 3460 = 0.35928198248147963
Training loss on iteration 3480 = 0.34591910019516947
Training loss on iteration 3500 = 0.3663424335420132
Training loss on iteration 3520 = 0.30516566187143324
Training loss on iteration 3540 = 0.24860531315207482
Training loss on iteration 3560 = 0.32258242592215536
Training loss on iteration 3580 = 0.3071119226515293
Training loss on iteration 3600 = 0.34606156274676325
Training loss on iteration 3620 = 0.30132397562265395
Training loss on iteration 3640 = 0.35621644034981725
Training loss on iteration 3660 = 0.3190812386572361
Training loss on iteration 3680 = 0.27769320979714396
Training loss on iteration 3700 = 0.3098966605961323
Training loss on iteration 3720 = 0.3260225407779217
Training loss on iteration 3740 = 0.3107699666172266
Training loss on iteration 3760 = 0.3662652313709259
Training loss on iteration 3780 = 0.30432764887809755
Training loss on iteration 3800 = 0.29618481621146203
Training loss on iteration 3820 = 0.35479429736733437
Training loss on iteration 3840 = 0.3015632100403309
Training loss on iteration 3860 = 0.3623323764652014
Training loss on iteration 3880 = 0.2898196090012789
Training loss on iteration 3900 = 0.41217595934867857
Training loss on iteration 3920 = 0.25731769278645517
Training loss on iteration 3940 = 0.29395634680986404
Training loss on iteration 3960 = 0.29057302251458167
Training loss on iteration 3980 = 0.3035029523074627
Training loss on iteration 4000 = 0.31255042776465414
Training loss on iteration 4020 = 0.32927193865180016
Training loss on iteration 4040 = 0.31809976547956464
Training loss on iteration 4060 = 0.3402210347354412
Training loss on iteration 4080 = 0.34324174374341965
Training loss on iteration 4100 = 0.3502186544239521
Training loss on iteration 4120 = 0.3012721002101898
Training loss on iteration 4140 = 0.2950764998793602
Training loss on iteration 4160 = 0.3467109836637974
Training loss on iteration 4180 = 0.31851102560758593
Training loss on iteration 4200 = 0.2954908058047295
Training loss on iteration 4220 = 0.2822614062577486
Training loss on iteration 4240 = 0.2927368953824043
Training loss on iteration 4260 = 0.3015192478895187
Training loss on iteration 4280 = 0.5309887312352657
Training loss on iteration 4300 = 0.2753912638872862
Training loss on iteration 4320 = 0.3111483231186867
Training loss on iteration 4340 = 0.31245668008923533
Training loss on iteration 4360 = 0.3260823965072632
Training loss on iteration 4380 = 0.3028940610587597
Training loss on iteration 4400 = 0.3306107044219971
Training loss on iteration 4420 = 0.2962053157389164
Training loss on iteration 4440 = 0.24780107140541077
Training loss on iteration 4460 = 0.3220526024699211
Training loss on iteration 4480 = 0.34887411817908287
Training loss on iteration 4500 = 0.28050092458724973
Training loss on iteration 4520 = 0.40362631976604463
Training loss on iteration 4540 = 0.30865550637245176
Training loss on iteration 4560 = 0.3289917975664139
Training loss on iteration 4580 = 0.33080998808145523
Training loss on iteration 4600 = 0.26511452384293077
Training loss on iteration 4620 = 0.40279466956853865
Training loss on iteration 4640 = 0.37584956735372543
Training loss on iteration 4660 = 0.34768021628260615
Training loss on iteration 4680 = 0.3343650154769421
Training loss on iteration 4700 = 0.3006561227142811
Training loss on iteration 4720 = 0.3636495992541313
Training loss on iteration 4740 = 0.29677013903856275
Training loss on iteration 4760 = 0.5468791380524636
Training loss on iteration 4780 = 0.29960011392831803
Training loss on iteration 4800 = 0.3319714151322842
Training loss on iteration 4820 = 0.25542773455381396
Training loss on iteration 4840 = 0.29265225902199743
Training loss on iteration 4860 = 0.3330058977007866
Training loss on iteration 4880 = 0.29615996591746807
Training loss on iteration 4900 = 0.3452008955180645
Training loss on iteration 4920 = 0.2748536176979542
Training loss on iteration 4940 = 0.30377961322665215
Training loss on iteration 4960 = 0.2648913186043501
Training loss on iteration 4980 = 0.26019044741988184
Training loss on iteration 5000 = 0.25907809734344484
Training loss on iteration 5020 = 0.327105962485075
Training loss on iteration 5040 = 0.36373087391257286
Training loss on iteration 5060 = 0.3415678970515728
Training loss on iteration 5080 = 0.2635440468788147
Training loss on iteration 5100 = 0.2549008332192898
Training loss on iteration 5120 = 0.34547040052711964
Training loss on iteration 5140 = 0.2863271594047546
Training loss on iteration 5160 = 0.2699901148676872
Training loss on iteration 5180 = 0.2741552941501141
Training loss on iteration 5200 = 0.2766762040555477
Training loss on iteration 5220 = 0.274151111766696
Training loss on iteration 5240 = 0.3210338167846203
Training loss on iteration 5260 = 0.2799288980662823
Training loss on iteration 5280 = 0.38223606050014497
Training loss on iteration 5300 = 0.31578237414360044
Training loss on iteration 5320 = 0.3056190587580204
Training loss on iteration 5340 = 0.33849030509591105
Training loss on iteration 5360 = 0.30912312120199203
Training loss on iteration 5380 = 0.310536690056324
Training loss on iteration 5400 = 0.3142357930541039
Training loss on iteration 5420 = 0.37722147107124326
Training loss on iteration 5440 = 0.3134076625108719
Training loss on iteration 5460 = 0.3016584426164627
Training loss on iteration 5480 = 0.3144783705472946
Training loss on iteration 5500 = 0.2884743519127369
Training loss on iteration 5520 = 0.3174950048327446
Training loss on iteration 5540 = 0.3257281482219696
Training loss on iteration 5560 = 0.306752547621727
Training loss on iteration 5580 = 0.29071111381053927
Training loss on iteration 5600 = 0.33384267836809156
Training loss on iteration 5620 = 0.26103727519512177
Training loss on iteration 5640 = 0.30281697660684587
Training loss on iteration 5660 = 0.35816736370325086
Training loss on iteration 5680 = 0.3488542966544628
Training loss on iteration 5700 = 0.31748290210962293
Training loss on iteration 5720 = 0.27278190329670904
Training loss on iteration 5740 = 0.35450350120663643
Training loss on iteration 5760 = 0.32715722247958184
Training loss on iteration 5780 = 0.29136854112148286
Training loss on iteration 5800 = 0.3769843392074108
Training loss on iteration 5820 = 0.3113758027553558
Training loss on iteration 5840 = 0.32761046290397644
Training loss on iteration 5860 = 0.34591950923204423
Training loss on iteration 5880 = 0.3151135466992855
Training loss on iteration 5900 = 0.3448758013546467
Training loss on iteration 5920 = 0.3890921398997307
Training loss on iteration 5940 = 0.31871747598052025
Training loss on iteration 5960 = 0.3326494351029396
Training loss on iteration 5980 = 0.31681995913386346
Training loss on iteration 6000 = 0.3208563171327114
Training loss on iteration 6020 = 0.37063094675540925
Training loss on iteration 6040 = 0.2671504393219948
Training loss on iteration 6060 = 0.36623184382915497
Training loss on iteration 6080 = 0.3557779997587204
Training loss on iteration 6100 = 0.31794925928115847
Training loss on iteration 6120 = 0.28378090634942055
Training loss on iteration 6140 = 0.32842121422290804
Training loss on iteration 6160 = 0.34596272706985476
Training loss on iteration 6180 = 0.3090982407331467
Training loss on iteration 6200 = 0.305802658200264
Training loss on iteration 6220 = 0.3193131387233734
Training loss on iteration 6240 = 0.31567443907260895
Training loss on iteration 6260 = 0.34720435962080953
Training loss on iteration 6280 = 0.3526532217860222
Training loss on iteration 6300 = 0.2874531224370003
Training loss on iteration 6320 = 0.34124783650040624
Training loss on iteration 6340 = 0.2881441943347454
Training loss on iteration 6360 = 0.3125766046345234
Training loss on iteration 6380 = 0.2777970638126135
Training loss on iteration 0 = 0.28234806656837463
Training loss on iteration 20 = 0.2698280647397041
Training loss on iteration 40 = 0.2548309188336134
Training loss on iteration 60 = 0.2658479459583759
Training loss on iteration 80 = 0.2857206605374813
Training loss on iteration 100 = 0.25099234133958814
Training loss on iteration 120 = 0.2905052475631237
Training loss on iteration 140 = 0.2510645881295204
Training loss on iteration 160 = 0.26053750663995745
Training loss on iteration 180 = 0.32443917617201806
Training loss on iteration 200 = 0.2693036932498217
Training loss on iteration 220 = 0.32292027361691
Training loss on iteration 240 = 0.27582391276955603
Training loss on iteration 260 = 0.2605944238603115
Training loss on iteration 280 = 0.26332092359662057
Training loss on iteration 300 = 0.21368342228233814
Training loss on iteration 320 = 0.2509902823716402
Training loss on iteration 340 = 0.2661160662770271
Training loss on iteration 360 = 0.22107581347227095
Training loss on iteration 380 = 0.2564778670668602
Training loss on iteration 400 = 0.2915881425142288
Training loss on iteration 420 = 0.25798669308423994
Training loss on iteration 440 = 0.28457655385136604
Training loss on iteration 460 = 0.2746555544435978
Training loss on iteration 480 = 0.3087952047586441
Training loss on iteration 500 = 0.3049963891506195
Training loss on iteration 520 = 0.24964683391153814
Training loss on iteration 540 = 0.30776896253228186
Training loss on iteration 560 = 0.27449347004294394
Training loss on iteration 580 = 0.27033401653170586
Training loss on iteration 600 = 0.248036377876997
Training loss on iteration 620 = 0.2954145263880491
Training loss on iteration 640 = 0.2647293426096439
Training loss on iteration 660 = 0.2680441368371248
Training loss on iteration 680 = 0.23957714699208738
Training loss on iteration 700 = 0.24286606647074221
Training loss on iteration 720 = 0.25011911019682886
Training loss on iteration 740 = 0.27283603996038436
Training loss on iteration 760 = 0.30425492078065874
Training loss on iteration 780 = 0.2648114860057831
Training loss on iteration 800 = 0.36919731423258784
Training loss on iteration 820 = 0.2804649643599987
Training loss on iteration 840 = 0.29444228038191794
Training loss on iteration 860 = 0.25990753173828124
Training loss on iteration 880 = 0.2751231599599123
Training loss on iteration 900 = 0.2685612242668867
Training loss on iteration 920 = 0.3231181539595127
Training loss on iteration 940 = 0.33100361451506616
Training loss on iteration 960 = 0.332476818561554
Training loss on iteration 980 = 0.286984047293663
Training loss on iteration 1000 = 0.2716206155717373
Training loss on iteration 1020 = 0.2588398076593876
Training loss on iteration 1040 = 0.2588587425649166
Training loss on iteration 1060 = 0.3685392953455448
Training loss on iteration 1080 = 0.3079036340117455
Training loss on iteration 1100 = 0.3286672256886959
Training loss on iteration 1120 = 0.3002457723021507
Training loss on iteration 1140 = 0.24213764443993568
Training loss on iteration 1160 = 0.27049027793109415
Training loss on iteration 1180 = 0.34053120836615564
Training loss on iteration 1200 = 0.28276884704828265
Training loss on iteration 1220 = 0.2670086398720741
Training loss on iteration 1240 = 0.2898468367755413
Training loss on iteration 1260 = 0.31637797579169274
Training loss on iteration 1280 = 0.3127818077802658
Training loss on iteration 1300 = 0.2737174317240715
Training loss on iteration 1320 = 0.3123320259153843
Training loss on iteration 1340 = 0.2887812189757824
Training loss on iteration 1360 = 0.25403265431523325
Training loss on iteration 1380 = 0.2771784007549286
Training loss on iteration 1400 = 0.2552490871399641
Training loss on iteration 1420 = 0.3457348011434078
Training loss on iteration 1440 = 0.23599321618676186
Training loss on iteration 1460 = 0.30111502185463906
Training loss on iteration 1480 = 0.2873336598277092
Training loss on iteration 1500 = 0.2851617582142353
Training loss on iteration 1520 = 0.22713663130998613
Training loss on iteration 1540 = 0.4153601549565792
Training loss on iteration 1560 = 0.2411763846874237
Training loss on iteration 1580 = 0.2588761545717716
Training loss on iteration 1600 = 0.26336604803800584
Training loss on iteration 1620 = 0.2812690794467926
Training loss on iteration 1640 = 0.2969494305551052
Training loss on iteration 1660 = 0.28007003366947175
Training loss on iteration 1680 = 0.2830104038119316
Training loss on iteration 1700 = 0.26656641960144045
Training loss on iteration 1720 = 0.26466407738626
Training loss on iteration 1740 = 0.2673095494508743
Training loss on iteration 1760 = 0.29309103786945345
Training loss on iteration 1780 = 0.3114327937364578
Training loss on iteration 1800 = 0.28628641813993455
Training loss on iteration 1820 = 0.2591107789427042
Training loss on iteration 1840 = 0.24438613951206206
Training loss on iteration 1860 = 0.2752406939864159
Training loss on iteration 1880 = 0.3428588330745697
Training loss on iteration 1900 = 0.2619923137128353
Training loss on iteration 1920 = 0.34808280766010286
Training loss on iteration 1940 = 0.2459486972540617
Training loss on iteration 1960 = 0.25855053812265394
Training loss on iteration 1980 = 0.30127112492918967
Training loss on iteration 2000 = 0.313238100707531
Training loss on iteration 2020 = 0.30648434087634085
Training loss on iteration 2040 = 0.2776229865849018
Training loss on iteration 2060 = 0.24224861152470112
Training loss on iteration 2080 = 0.25385374277830125
Training loss on iteration 2100 = 0.3594955913722515
Training loss on iteration 2120 = 0.30681399405002596
Training loss on iteration 2140 = 0.31532696411013605
Training loss on iteration 2160 = 0.31565858498215676
Training loss on iteration 2180 = 0.3023629978299141
Training loss on iteration 2200 = 0.341425222158432
Training loss on iteration 2220 = 0.3240559041500092
Training loss on iteration 2240 = 0.348361698538065
Training loss on iteration 2260 = 0.29058772996068
Training loss on iteration 2280 = 0.3195939943194389
Training loss on iteration 2300 = 0.29351866617798805
Training loss on iteration 2320 = 0.2627155639231205
Training loss on iteration 2340 = 0.2826287649571896
Training loss on iteration 2360 = 0.27006336450576784
Training loss on iteration 2380 = 0.2777941606938839
Training loss on iteration 2400 = 0.2781436577439308
Training loss on iteration 2420 = 0.2636366493999958
Training loss on iteration 2440 = 0.2637455314397812
Training loss on iteration 2460 = 0.2533283941447735
Training loss on iteration 2480 = 0.2818705029785633
Training loss on iteration 2500 = 0.2835909932851791
Training loss on iteration 2520 = 0.31020281091332436
Training loss on iteration 2540 = 0.2995523285120726
Training loss on iteration 2560 = 0.2805909294635057
Training loss on iteration 2580 = 0.2562480714172125
Training loss on iteration 2600 = 0.25135585889220236
Training loss on iteration 2620 = 0.3214525580406189
Training loss on iteration 2640 = 0.26012971103191374
Training loss on iteration 2660 = 0.2944469764828682
Training loss on iteration 2680 = 0.2799055319279432
Training loss on iteration 2700 = 0.294492045044899
Training loss on iteration 2720 = 0.2944116622209549
Training loss on iteration 2740 = 0.31510994136333464
Training loss on iteration 2760 = 0.2613446287810802
Training loss on iteration 2780 = 0.34472133666276933
Training loss on iteration 2800 = 0.29704329967498777
Training loss on iteration 2820 = 0.2764959461987019
Training loss on iteration 2840 = 0.25511872842907907
Training loss on iteration 2860 = 0.2727348007261753
Training loss on iteration 2880 = 0.2847024157643318
Training loss on iteration 2900 = 0.2800935998558998
Training loss on iteration 2920 = 0.27400205805897715
Training loss on iteration 2940 = 0.2815090324729681
Training loss on iteration 2960 = 0.33695425391197203
Training loss on iteration 2980 = 0.27154474928975103
Training loss on iteration 3000 = 0.2403675302863121
Training loss on iteration 3020 = 0.28303859755396843
Training loss on iteration 3040 = 0.28365700915455816
Training loss on iteration 3060 = 0.2418496824800968
Training loss on iteration 3080 = 0.2917526103556156
Training loss on iteration 3100 = 0.25019084066152575
Training loss on iteration 3120 = 0.32173703908920287
Training loss on iteration 3140 = 0.27208472788333893
Training loss on iteration 3160 = 0.2852586515247822
Training loss on iteration 3180 = 0.351447793841362
Training loss on iteration 3200 = 0.2982748568058014
Training loss on iteration 3220 = 0.3529608748853207
Training loss on iteration 3240 = 0.34141058698296545
Training loss on iteration 3260 = 0.2961312770843506
Training loss on iteration 3280 = 0.325173432379961
Training loss on iteration 3300 = 0.24695478305220603
Training loss on iteration 3320 = 0.22759374231100082
Training loss on iteration 3340 = 0.2865529224276543
Training loss on iteration 3360 = 0.2429620899260044
Training loss on iteration 3380 = 0.280027461796999
Training loss on iteration 3400 = 0.26528589576482775
Training loss on iteration 3420 = 0.2537285380065441
Training loss on iteration 3440 = 0.30073130056262015
Training loss on iteration 3460 = 0.31188594177365303
Training loss on iteration 3480 = 0.258120347186923
Training loss on iteration 3500 = 0.3334687266498804
Training loss on iteration 3520 = 0.3138536438345909
Training loss on iteration 3540 = 0.3130095511674881
Training loss on iteration 3560 = 0.29449338018894194
Training loss on iteration 3580 = 0.2975961990654469
Training loss on iteration 3600 = 0.33869114741683004
Training loss on iteration 3620 = 0.3152186006307602
Training loss on iteration 3640 = 0.28659623600542544
Training loss on iteration 3660 = 0.30305335149168966
Training loss on iteration 3680 = 0.3426031731069088
Training loss on iteration 3700 = 0.2657513149082661
Training loss on iteration 3720 = 0.3644016616046429
Training loss on iteration 3740 = 0.30724083408713343
Training loss on iteration 3760 = 0.3065964922308922
Training loss on iteration 3780 = 0.28706908971071243
Training loss on iteration 3800 = 0.34542191326618193
Training loss on iteration 3820 = 0.33269289657473566
Training loss on iteration 3840 = 0.3066772609949112
Training loss on iteration 3860 = 0.3233097575604916
Training loss on iteration 3880 = 0.33558357805013656
Training loss on iteration 3900 = 0.33404875695705416
Training loss on iteration 3920 = 0.3524116173386574
Training loss on iteration 3940 = 0.2506652496755123
Training loss on iteration 3960 = 0.27739463560283184
Training loss on iteration 3980 = 0.29626881033182145
Training loss on iteration 4000 = 0.39781485572457315
Training loss on iteration 4020 = 0.3155773676931858
Training loss on iteration 4040 = 0.5544009871780873
Training loss on iteration 4060 = 0.29655229561030866
Training loss on iteration 4080 = 0.3546776495873928
Training loss on iteration 4100 = 0.2926092237234116
Training loss on iteration 4120 = 0.24850652888417243
Training loss on iteration 4140 = 0.34210952594876287
Training loss on iteration 4160 = 0.28652751445770264
Training loss on iteration 4180 = 0.28461083471775056
Training loss on iteration 4200 = 0.32420076578855517
Training loss on iteration 4220 = 0.27246176898479463
Training loss on iteration 4240 = 0.3097895696759224
Training loss on iteration 4260 = 0.3196959555149078
Training loss on iteration 4280 = 0.31281445994973184
Training loss on iteration 4300 = 0.37903441339731214
Training loss on iteration 4320 = 0.2814989648759365
Training loss on iteration 4340 = 0.2572670951485634
Training loss on iteration 4360 = 0.27120981439948083
Training loss on iteration 4380 = 0.2608843259513378
Training loss on iteration 4400 = 0.3301960818469524
Training loss on iteration 4420 = 0.2882365621626377
Training loss on iteration 4440 = 0.2989584505558014
Training loss on iteration 4460 = 0.3008268393576145
Training loss on iteration 4480 = 0.255859400331974
Training loss on iteration 4500 = 0.3403330765664577
Training loss on iteration 4520 = 0.26431486643850804
Training loss on iteration 4540 = 0.2935736455023289
Training loss on iteration 4560 = 0.3221366800367832
Training loss on iteration 4580 = 0.2828186422586441
Training loss on iteration 4600 = 0.2584706757217646
Training loss on iteration 4620 = 0.2668301410973072
Training loss on iteration 4640 = 0.3013368032872677
Training loss on iteration 4660 = 0.27168643549084665
Training loss on iteration 4680 = 0.3217226304113865
Training loss on iteration 4700 = 0.3274483993649483
Training loss on iteration 4720 = 0.27526435554027556
Training loss on iteration 4740 = 0.3018998049199581
Training loss on iteration 4760 = 0.3652716867625713
Training loss on iteration 4780 = 0.259178401902318
Training loss on iteration 4800 = 0.2796688497066498
Training loss on iteration 4820 = 0.3856434792280197
Training loss on iteration 4840 = 0.25022543147206305
Training loss on iteration 4860 = 0.2800465412437916
Training loss on iteration 4880 = 0.3087821565568447
Training loss on iteration 4900 = 0.2569902762770653
Training loss on iteration 4920 = 0.30961862169206145
Training loss on iteration 4940 = 0.27349837459623816
Training loss on iteration 4960 = 0.2713129386305809
Training loss on iteration 4980 = 0.35470094755291937
Training loss on iteration 5000 = 0.325788763910532
Training loss on iteration 5020 = 0.30060323476791384
Training loss on iteration 5040 = 0.31554101780056953
Training loss on iteration 5060 = 0.29462515413761137
Training loss on iteration 5080 = 0.3549666963517666
Training loss on iteration 5100 = 0.2687979839742184
Training loss on iteration 5120 = 0.2713680498301983
Training loss on iteration 5140 = 0.2998963460326195
Training loss on iteration 5160 = 0.30267255529761317
Training loss on iteration 5180 = 0.3566101811826229
Training loss on iteration 5200 = 0.31431642472743987
Training loss on iteration 5220 = 0.26177863627672193
Training loss on iteration 5240 = 0.30201013684272765
Training loss on iteration 5260 = 0.32632182687520983
Training loss on iteration 5280 = 0.31772494204342366
Training loss on iteration 5300 = 0.37180459275841715
Training loss on iteration 5320 = 0.32776765078306197
Training loss on iteration 5340 = 0.26042834520339964
Training loss on iteration 5360 = 0.25122176930308343
Training loss on iteration 5380 = 0.29851530864834785
Training loss on iteration 5400 = 0.30425201952457426
Training loss on iteration 5420 = 0.2782774068415165
Training loss on iteration 5440 = 0.2556022390723228
Training loss on iteration 5460 = 0.241730397939682
Training loss on iteration 5480 = 0.28824779242277143
Training loss on iteration 5500 = 0.3544891647994518
Training loss on iteration 5520 = 0.30758204236626624
Training loss on iteration 5540 = 0.31906188540160657
Training loss on iteration 5560 = 0.3098595730960369
Training loss on iteration 5580 = 0.3336685188114643
Training loss on iteration 5600 = 0.3322394788265228
Training loss on iteration 5620 = 0.2720457136631012
Training loss on iteration 5640 = 0.3574866086244583
Training loss on iteration 5660 = 0.2914795055985451
Training loss on iteration 5680 = 0.26919778287410734
Training loss on iteration 5700 = 0.33960835114121435
Training loss on iteration 5720 = 0.2778993099927902
Training loss on iteration 5740 = 0.33016879931092263
Training loss on iteration 5760 = 0.3488039620220661
Training loss on iteration 5780 = 0.25691493451595304
Training loss on iteration 5800 = 0.33306899927556516
Training loss on iteration 5820 = 0.2735541220754385
Training loss on iteration 5840 = 0.29597091898322103
Training loss on iteration 5860 = 0.28327280580997466
Training loss on iteration 5880 = 0.2849857844412327
Training loss on iteration 5900 = 0.26236883625388147
Training loss on iteration 5920 = 0.2500817447900772
Training loss on iteration 5940 = 0.5033629216253758
Training loss on iteration 5960 = 0.24104608669877053
Training loss on iteration 5980 = 0.24789569526910782
Training loss on iteration 6000 = 0.2956554587930441
Training loss on iteration 6020 = 0.29721891060471534
Training loss on iteration 6040 = 0.3279210597276688
Training loss on iteration 6060 = 0.2926384754478931
Training loss on iteration 6080 = 0.2678967647254467
Training loss on iteration 6100 = 0.2783104248344898
Training loss on iteration 6120 = 0.2657755486667156
Training loss on iteration 6140 = 0.34291435927152636
Training loss on iteration 6160 = 0.3501041531562805
Training loss on iteration 6180 = 0.2771468795835972
Training loss on iteration 6200 = 0.30129328966140745
Training loss on iteration 6220 = 0.33410049304366113
Training loss on iteration 6240 = 0.31501781940460205
Training loss on iteration 6260 = 0.26453643664717674
Training loss on iteration 6280 = 0.299876644462347
Training loss on iteration 6300 = 0.2643847346305847
Training loss on iteration 6320 = 0.37473211660981176
Training loss on iteration 6340 = 0.3131877303123474
Training loss on iteration 6360 = 0.30735466331243516
Training loss on iteration 6380 = 0.2784053564071655
Training loss on iteration 0 = 0.2929445207118988
Training loss on iteration 20 = 0.2832010336220264
Training loss on iteration 40 = 0.24963564425706863
Training loss on iteration 60 = 0.24075398296117784
Training loss on iteration 80 = 0.23834567815065383
Training loss on iteration 100 = 0.22748205363750457
Training loss on iteration 120 = 0.2817852724343538
Training loss on iteration 140 = 0.23303318135440348
Training loss on iteration 160 = 0.23652669936418533
Training loss on iteration 180 = 0.2642381839454174
Training loss on iteration 200 = 0.22832026258111
Training loss on iteration 220 = 0.21375482976436616
Training loss on iteration 240 = 0.22448710836470126
Training loss on iteration 260 = 0.21663155145943164
Training loss on iteration 280 = 0.22742206640541554
Training loss on iteration 300 = 0.23202979154884815
Training loss on iteration 320 = 0.23929850459098817
Training loss on iteration 340 = 0.22089444622397422
Training loss on iteration 360 = 0.3744836401194334
Training loss on iteration 380 = 0.23897455334663392
Training loss on iteration 400 = 0.23762052245438098
Training loss on iteration 420 = 0.20506184175610542
Training loss on iteration 440 = 0.22435103207826615
Training loss on iteration 460 = 0.2389650397002697
Training loss on iteration 480 = 0.2021590754389763
Training loss on iteration 500 = 0.21809292249381543
Training loss on iteration 520 = 0.2346827734261751
Training loss on iteration 540 = 0.26675936952233315
Training loss on iteration 560 = 0.24849622547626496
Training loss on iteration 580 = 0.19691190905869008
Training loss on iteration 600 = 0.2573468409478664
Training loss on iteration 620 = 0.23134587928652764
Training loss on iteration 640 = 0.21932592131197454
Training loss on iteration 660 = 0.19734970033168792
Training loss on iteration 680 = 0.22832900770008563
Training loss on iteration 700 = 0.21421869918704034
Training loss on iteration 720 = 0.21356433369219302
Training loss on iteration 740 = 0.21230108253657817
Training loss on iteration 760 = 0.2247275523841381
Training loss on iteration 780 = 0.2492874175310135
Training loss on iteration 800 = 0.2073143221437931
Training loss on iteration 820 = 0.24032105207443238
Training loss on iteration 840 = 0.20318407714366912
Training loss on iteration 860 = 0.2576212130486965
Training loss on iteration 880 = 0.21285397596657277
Training loss on iteration 900 = 0.4253104738891125
Training loss on iteration 920 = 0.24042785167694092
Training loss on iteration 940 = 0.23988011926412584
Training loss on iteration 960 = 0.22651312574744226
Training loss on iteration 980 = 0.24828048013150691
Training loss on iteration 1000 = 0.2374945290386677
Training loss on iteration 1020 = 0.20880511738359928
Training loss on iteration 1040 = 0.2483206495642662
Training loss on iteration 1060 = 0.2320053532719612
Training loss on iteration 1080 = 0.2556509807705879
Training loss on iteration 1100 = 0.2880567260086536
Training loss on iteration 1120 = 0.25049388110637666
Training loss on iteration 1140 = 0.2533955559134483
Training loss on iteration 1160 = 0.2429348722100258
Training loss on iteration 1180 = 0.22463947720825672
Training loss on iteration 1200 = 0.20879369489848615
Training loss on iteration 1220 = 0.27371947579085826
Training loss on iteration 1240 = 0.22293675281107425
Training loss on iteration 1260 = 0.22983878143131733
Training loss on iteration 1280 = 0.30766360387206076
Training loss on iteration 1300 = 0.22432919666171075
Training loss on iteration 1320 = 0.2881601557135582
Training loss on iteration 1340 = 0.2580685045570135
Training loss on iteration 1360 = 0.18695065416395665
Training loss on iteration 1380 = 0.19228947423398496
Training loss on iteration 1400 = 0.21304537802934648
Training loss on iteration 1420 = 0.22613376528024673
Training loss on iteration 1440 = 0.21251749992370605
Training loss on iteration 1460 = 0.21931823417544366
Training loss on iteration 1480 = 0.2333005342632532
Training loss on iteration 1500 = 0.2272219218313694
Training loss on iteration 1520 = 0.23913939371705056
Training loss on iteration 1540 = 0.25503490269184115
Training loss on iteration 1560 = 0.21731938943266868
Training loss on iteration 1580 = 0.2146029993891716
Training loss on iteration 1600 = 0.2029855467379093
Training loss on iteration 1620 = 0.2221597969532013
Training loss on iteration 1640 = 0.21478425599634648
Training loss on iteration 1660 = 0.195013590157032
Training loss on iteration 1680 = 0.21604324281215667
Training loss on iteration 1700 = 0.222372355312109
Training loss on iteration 1720 = 0.2328836940228939
Training loss on iteration 1740 = 0.2194991961121559
Training loss on iteration 1760 = 0.21543206535279752
Training loss on iteration 1780 = 0.21704698577523232
Training loss on iteration 1800 = 0.2632225017994642
Training loss on iteration 1820 = 0.21556663513183594
Training loss on iteration 1840 = 0.23820546492934228
Training loss on iteration 1860 = 0.20685894154012202
Training loss on iteration 1880 = 0.23652406707406043
Training loss on iteration 1900 = 0.23846915513277053
Training loss on iteration 1920 = 0.23670292608439922
Training loss on iteration 1940 = 0.24979542680084704
Training loss on iteration 1960 = 0.24185857363045216
Training loss on iteration 1980 = 0.21619592271745205
Training loss on iteration 2000 = 0.21855073049664497
Training loss on iteration 2020 = 0.25015563815832137
Training loss on iteration 2040 = 0.20343064591288568
Training loss on iteration 2060 = 0.22107411324977874
Training loss on iteration 2080 = 0.2490746408700943
Training loss on iteration 2100 = 0.22648541070520878
Training loss on iteration 2120 = 0.23965790756046773
Training loss on iteration 2140 = 0.23245348818600178
Training loss on iteration 2160 = 0.21501582413911818
Training loss on iteration 2180 = 0.2285945735871792
Training loss on iteration 2200 = 0.20430228784680365
Training loss on iteration 2220 = 0.23713673576712607
Training loss on iteration 2240 = 0.2602052327245474
Training loss on iteration 2260 = 0.28736539632081987
Training loss on iteration 2280 = 0.209715024381876
Training loss on iteration 2300 = 0.20703318938612938
Training loss on iteration 2320 = 0.23115460574626923
Training loss on iteration 2340 = 0.2425148706883192
Training loss on iteration 2360 = 0.22319281548261644
Training loss on iteration 2380 = 0.2143389631062746
Training loss on iteration 2400 = 0.24047686234116555
Training loss on iteration 2420 = 0.21920036524534225
Training loss on iteration 2440 = 0.2440227646380663
Training loss on iteration 2460 = 0.2109821829944849
Training loss on iteration 2480 = 0.22432401292026044
Training loss on iteration 2500 = 0.28940129056572916
Training loss on iteration 2520 = 0.22905875630676747
Training loss on iteration 2540 = 0.24237461164593696
Training loss on iteration 2560 = 0.24652475602924823
Training loss on iteration 2580 = 0.22138716503977776
Training loss on iteration 2600 = 0.2385381456464529
Training loss on iteration 2620 = 0.2273085817694664
Training loss on iteration 2640 = 0.2375287875533104
Training loss on iteration 2660 = 0.1963187910616398
Training loss on iteration 2680 = 0.2178778786212206
Training loss on iteration 2700 = 0.24740307927131652
Training loss on iteration 2720 = 0.207338947057724
Training loss on iteration 2740 = 0.1920095682144165
Training loss on iteration 2760 = 0.27850026041269305
Training loss on iteration 2780 = 0.28311497531831264
Training loss on iteration 2800 = 0.2400884997099638
Training loss on iteration 2820 = 0.21650916785001756
Training loss on iteration 2840 = 0.24981724619865417
Training loss on iteration 2860 = 0.23918194845318794
Training loss on iteration 2880 = 0.24227869063615798
Training loss on iteration 2900 = 0.27465309016406536
Training loss on iteration 2920 = 0.2337074287235737
Training loss on iteration 2940 = 0.24668299704790114
Training loss on iteration 2960 = 0.22354208342731
Training loss on iteration 2980 = 0.22227634713053704
Training loss on iteration 3000 = 0.24529803059995176
Training loss on iteration 3020 = 0.24783550724387168
Training loss on iteration 3040 = 0.26626060009002683
Training loss on iteration 3060 = 0.19699268452823163
Training loss on iteration 3080 = 0.2567218765616417
Training loss on iteration 3100 = 0.2581168659031391
Training loss on iteration 3120 = 0.23722383193671703
Training loss on iteration 3140 = 0.23449919670820235
Training loss on iteration 3160 = 0.2824311636388302
Training loss on iteration 3180 = 0.22960377745330335
Training loss on iteration 3200 = 0.22777038365602492
Training loss on iteration 3220 = 0.2394336186349392
Training loss on iteration 3240 = 0.19628413133323191
Training loss on iteration 3260 = 0.221221374720335
Training loss on iteration 3280 = 0.19481807202100754
Training loss on iteration 3300 = 0.2614175919443369
Training loss on iteration 3320 = 0.23730723038315774
Training loss on iteration 3340 = 0.24065422601997852
Training loss on iteration 3360 = 0.22441434748470784
Training loss on iteration 3380 = 0.2522496171295643
Training loss on iteration 3400 = 0.2592266485095024
Training loss on iteration 3420 = 0.2295034758746624
Training loss on iteration 3440 = 0.23355941586196421
Training loss on iteration 3460 = 0.22369084805250167
Training loss on iteration 3480 = 0.20053966380655766
Training loss on iteration 3500 = 0.24789464063942432
Training loss on iteration 3520 = 0.22630350962281226
Training loss on iteration 3540 = 0.23509967215359212
Training loss on iteration 3560 = 0.22014509849250316
Training loss on iteration 3580 = 0.2333546757698059
Training loss on iteration 3600 = 0.25672802701592445
Training loss on iteration 3620 = 0.24558245576918125
Training loss on iteration 3640 = 0.21599050015211105
Training loss on iteration 3660 = 0.21158202812075616
Training loss on iteration 3680 = 0.2771782983094454
Training loss on iteration 3700 = 0.23724755086004734
Training loss on iteration 3720 = 0.24534166865050794
Training loss on iteration 3740 = 0.1956142671406269
Training loss on iteration 3760 = 0.25568194016814233
Training loss on iteration 3780 = 0.22806327193975448
Training loss on iteration 3800 = 0.22600812055170535
Training loss on iteration 3820 = 0.2075204774737358
Training loss on iteration 3840 = 0.23948238119482995
Training loss on iteration 3860 = 0.240471201390028
Training loss on iteration 3880 = 0.23945261128246784
Training loss on iteration 3900 = 0.23361198566854
Training loss on iteration 3920 = 0.2176465917378664
Training loss on iteration 3940 = 0.21701041758060455
Training loss on iteration 3960 = 0.22082161717116833
Training loss on iteration 3980 = 0.24717828631401062
Training loss on iteration 4000 = 0.23034258261322976
Training loss on iteration 4020 = 0.24474538192152978
Training loss on iteration 4040 = 0.22779683396220207
Training loss on iteration 4060 = 0.23971584141254426
Training loss on iteration 4080 = 0.24826700761914253
Training loss on iteration 4100 = 0.23141804039478303
Training loss on iteration 4120 = 0.23278987370431423
Training loss on iteration 4140 = 0.21337325647473335
Training loss on iteration 4160 = 0.24874096363782883
Training loss on iteration 4180 = 0.2554421566426754
Training loss on iteration 4200 = 0.25135951712727544
Training loss on iteration 4220 = 0.23700916171073913
Training loss on iteration 4240 = 0.26403337754309175
Training loss on iteration 4260 = 0.21682274304330348
Training loss on iteration 4280 = 0.23157839030027388
Training loss on iteration 4300 = 0.25130783542990687
Training loss on iteration 4320 = 0.26019725054502485
Training loss on iteration 4340 = 0.2653192788362503
Training loss on iteration 4360 = 0.28150211498141287
Training loss on iteration 4380 = 0.21463763937354088
Training loss on iteration 4400 = 0.22449310012161733
Training loss on iteration 4420 = 0.22624268978834153
Training loss on iteration 4440 = 0.24981125444173813
Training loss on iteration 4460 = 0.20932185277342796
Training loss on iteration 4480 = 0.22290402725338937
Training loss on iteration 4500 = 0.2761306300759315
Training loss on iteration 4520 = 0.2179802730679512
Training loss on iteration 4540 = 0.23221452198922635
Training loss on iteration 4560 = 0.19552975781261922
Training loss on iteration 4580 = 0.23016796559095382
Training loss on iteration 4600 = 0.2833064042031765
Training loss on iteration 4620 = 0.24377812594175338
Training loss on iteration 4640 = 0.2010835360735655
Training loss on iteration 4660 = 0.2272963050752878
Training loss on iteration 4680 = 0.2813608653843403
Training loss on iteration 4700 = 0.23271278962492942
Training loss on iteration 4720 = 0.2217158380895853
Training loss on iteration 4740 = 0.20932707227766514
Training loss on iteration 4760 = 0.2826992549002171
Training loss on iteration 4780 = 0.24286431968212127
Training loss on iteration 4800 = 0.25871257819235327
Training loss on iteration 4820 = 0.21913718320429326
Training loss on iteration 4840 = 0.24837216809391977
Training loss on iteration 4860 = 0.24634410440921783
Training loss on iteration 4880 = 0.23223853036761283
Training loss on iteration 4900 = 0.224586883187294
Training loss on iteration 4920 = 0.23294273987412453
Training loss on iteration 4940 = 0.2815443553030491
Training loss on iteration 4960 = 0.2320232827216387
Training loss on iteration 4980 = 0.25236389227211475
Training loss on iteration 5000 = 0.23445924893021583
Training loss on iteration 5020 = 0.2635539874434471
Training loss on iteration 5040 = 0.19764321409165858
Training loss on iteration 5060 = 0.23656439110636712
Training loss on iteration 5080 = 0.24747203327715397
Training loss on iteration 5100 = 0.21425494849681853
Training loss on iteration 5120 = 0.2587031826376915
Training loss on iteration 5140 = 0.2523638464510441
Training loss on iteration 5160 = 0.2705609403550625
Training loss on iteration 5180 = 0.2056680005043745
Training loss on iteration 5200 = 0.254117014631629
Training loss on iteration 5220 = 0.2259646996855736
Training loss on iteration 5240 = 0.20947371646761895
Training loss on iteration 5260 = 0.21701692491769792
Training loss on iteration 5280 = 0.2657851576805115
Training loss on iteration 5300 = 0.2143315315246582
Training loss on iteration 5320 = 0.25011300295591354
Training loss on iteration 5340 = 0.21570111401379108
Training loss on iteration 5360 = 0.218080984801054
Training loss on iteration 5380 = 0.19909018352627755
Training loss on iteration 5400 = 0.21941254436969757
Training loss on iteration 5420 = 0.24974964894354343
Training loss on iteration 5440 = 0.23097249642014503
Training loss on iteration 5460 = 0.2110816951841116
Training loss on iteration 5480 = 0.25478682816028597
Training loss on iteration 5500 = 0.21872668266296386
Training loss on iteration 5520 = 0.2681124985218048
Training loss on iteration 5540 = 0.21925020292401315
Training loss on iteration 5560 = 0.29551512002944946
Training loss on iteration 5580 = 0.2350696176290512
Training loss on iteration 5600 = 0.2204594388604164
Training loss on iteration 5620 = 0.22457306757569312
Training loss on iteration 5640 = 0.22160771042108535
Training loss on iteration 5660 = 0.2252447411417961
Training loss on iteration 5680 = 0.22351485416293143
Training loss on iteration 5700 = 0.21571083627641202
Training loss on iteration 5720 = 0.213023404404521
Training loss on iteration 5740 = 0.2454626474529505
Training loss on iteration 5760 = 0.24897066317498684
Training loss on iteration 5780 = 0.20894445478916168
Training loss on iteration 5800 = 0.2688345044851303
Training loss on iteration 5820 = 0.19249848872423173
Training loss on iteration 5840 = 0.2221930593252182
Training loss on iteration 5860 = 0.24812454134225845
Training loss on iteration 5880 = 0.2526690036058426
Training loss on iteration 5900 = 0.22602663189172745
Training loss on iteration 5920 = 0.22545676603913306
Training loss on iteration 5940 = 0.24491136893630028
Training loss on iteration 5960 = 0.25546961054205897
Training loss on iteration 5980 = 0.22561711855232716
Training loss on iteration 6000 = 0.25790223218500613
Training loss on iteration 6020 = 0.2913616195321083
Training loss on iteration 6040 = 0.21542552039027213
Training loss on iteration 6060 = 0.22006138414144516
Training loss on iteration 6080 = 0.22697677239775657
Training loss on iteration 6100 = 0.21049355193972588
Training loss on iteration 6120 = 0.22193518802523612
Training loss on iteration 6140 = 0.22017371878027917
Training loss on iteration 6160 = 0.2162334904074669
Training loss on iteration 6180 = 0.23834245204925536
Training loss on iteration 6200 = 0.2182411603629589
Training loss on iteration 6220 = 0.22153926640748978
Training loss on iteration 6240 = 0.2247843984514475
Training loss on iteration 6260 = 0.21673378199338914
Training loss on iteration 6280 = 0.23743243403732778
Training loss on iteration 6300 = 0.19627488143742083
Training loss on iteration 6320 = 0.2340249553322792
Training loss on iteration 6340 = 0.2679625809192657
Training loss on iteration 6360 = 0.2275286391377449
Training loss on iteration 6380 = 0.2076530508697033
Training loss on iteration 0 = 0.18413405120372772
Training loss on iteration 20 = 0.19532128274440766
Training loss on iteration 40 = 0.18776394687592984
Training loss on iteration 60 = 0.20134937353432178
Training loss on iteration 80 = 0.19333027601242064
Training loss on iteration 100 = 0.19904184341430664
Training loss on iteration 120 = 0.23254376985132694
Training loss on iteration 140 = 0.21736450642347335
Training loss on iteration 160 = 0.2028331756591797
Training loss on iteration 180 = 0.1840999063104391
Training loss on iteration 200 = 0.19764969013631345
Training loss on iteration 220 = 0.17482618428766727
Training loss on iteration 240 = 0.20682349763810634
Training loss on iteration 260 = 0.2842321343719959
Training loss on iteration 280 = 0.2022601343691349
Training loss on iteration 300 = 0.16587617583572864
Training loss on iteration 320 = 0.2162219114601612
Training loss on iteration 340 = 0.22767772413790227
Training loss on iteration 360 = 0.20412167124450206
Training loss on iteration 380 = 0.2146035622805357
Training loss on iteration 400 = 0.1966031774878502
Training loss on iteration 420 = 0.20693810284137726
Training loss on iteration 440 = 0.2054409932345152
Training loss on iteration 460 = 0.2378264505416155
Training loss on iteration 480 = 0.17824749760329722
Training loss on iteration 500 = 0.18385056033730507
Training loss on iteration 520 = 0.20088201239705086
Training loss on iteration 540 = 0.2030570510774851
Training loss on iteration 560 = 0.20683780461549758
Training loss on iteration 580 = 0.22169731855392455
Training loss on iteration 600 = 0.23603976815938948
Training loss on iteration 620 = 0.1946842897683382
Training loss on iteration 640 = 0.2181076131761074
Training loss on iteration 660 = 0.2725961271673441
Training loss on iteration 680 = 0.2153529893606901
Training loss on iteration 700 = 0.21694483011960983
Training loss on iteration 720 = 0.17039126940071583
Training loss on iteration 740 = 0.2317076951265335
Training loss on iteration 760 = 0.2252287432551384
Training loss on iteration 780 = 0.19817257300019264
Training loss on iteration 800 = 0.21190002635121347
Training loss on iteration 820 = 0.20322609320282936
Training loss on iteration 840 = 0.1966878280043602
Training loss on iteration 860 = 0.20590062625706196
Training loss on iteration 880 = 0.19634013250470161
Training loss on iteration 900 = 0.23632133714854717
Training loss on iteration 920 = 0.2056586228311062
Training loss on iteration 940 = 0.22185680978000163
Training loss on iteration 960 = 0.188900575786829
Training loss on iteration 980 = 0.20618620067834853
Training loss on iteration 1000 = 0.17603919990360736
Training loss on iteration 1020 = 0.19809920266270636
Training loss on iteration 1040 = 0.22649194970726966
Training loss on iteration 1060 = 0.1770383261144161
Training loss on iteration 1080 = 0.1990420814603567
Training loss on iteration 1100 = 0.23860384039580823
Training loss on iteration 1120 = 0.2628468360751867
Training loss on iteration 1140 = 0.20847662091255187
Training loss on iteration 1160 = 0.19584021680057048
Training loss on iteration 1180 = 0.1949744276702404
Training loss on iteration 1200 = 0.20573987439274788
Training loss on iteration 1220 = 0.3779285900294781
Training loss on iteration 1240 = 0.18113469406962396
Training loss on iteration 1260 = 0.20182498544454575
Training loss on iteration 1280 = 0.21989433504641057
Training loss on iteration 1300 = 0.20661657638847827
Training loss on iteration 1320 = 0.23381023071706294
Training loss on iteration 1340 = 0.22401296198368073
Training loss on iteration 1360 = 0.23217489793896676
Training loss on iteration 1380 = 0.19343300946056843
Training loss on iteration 1400 = 0.20572771914303303
Training loss on iteration 1420 = 0.21495847702026366
Training loss on iteration 1440 = 0.22458138167858124
Training loss on iteration 1460 = 0.2602281466126442
Training loss on iteration 1480 = 0.1978216264396906
Training loss on iteration 1500 = 0.21214569136500358
Training loss on iteration 1520 = 0.2062692429870367
Training loss on iteration 1540 = 0.19314191304147243
Training loss on iteration 1560 = 0.20719182044267653
Training loss on iteration 1580 = 0.19409382231533528
Training loss on iteration 1600 = 0.22400151304900645
Training loss on iteration 1620 = 0.2086196154356003
Training loss on iteration 1640 = 0.24677257873117925
Training loss on iteration 1660 = 0.20516631491482257
Training loss on iteration 1680 = 0.23341348208487034
Training loss on iteration 1700 = 0.19653601087629796
Training loss on iteration 1720 = 0.196388952806592
Training loss on iteration 1740 = 0.1912224803119898
Training loss on iteration 1760 = 0.29206612445414065
Training loss on iteration 1780 = 0.24665294736623763
Training loss on iteration 1800 = 0.2106951590627432
Training loss on iteration 1820 = 0.20720519870519638
Training loss on iteration 1840 = 0.2079620473086834
Training loss on iteration 1860 = 0.21384850963950158
Training loss on iteration 1880 = 0.25019886195659635
Training loss on iteration 1900 = 0.2527937900274992
Training loss on iteration 1920 = 0.22261288911104202
Training loss on iteration 1940 = 0.2177414044737816
Training loss on iteration 1960 = 0.21287507116794585
Training loss on iteration 1980 = 0.2248129278421402
Training loss on iteration 2000 = 0.20999160706996917
Training loss on iteration 2020 = 0.20319858491420745
Training loss on iteration 2040 = 0.1959576927125454
Training loss on iteration 2060 = 0.21260228753089905
Training loss on iteration 2080 = 0.20790793001651764
Training loss on iteration 2100 = 0.24144936427474023
Training loss on iteration 2120 = 0.22140831537544728
Training loss on iteration 2140 = 0.21201857626438142
Training loss on iteration 2160 = 0.21225665360689164
Training loss on iteration 2180 = 0.2147676084190607
Training loss on iteration 2200 = 0.44889250248670576
Training loss on iteration 2220 = 0.21171969100832938
Training loss on iteration 2240 = 0.22071319296956063
Training loss on iteration 2260 = 0.2551232688128948
Training loss on iteration 2280 = 0.22033878788352013
Training loss on iteration 2300 = 0.2141769167035818
Training loss on iteration 2320 = 0.20659345649182798
Training loss on iteration 2340 = 0.1940895564854145
Training loss on iteration 2360 = 0.1957670569419861
Training loss on iteration 2380 = 0.21583163663744925
Training loss on iteration 2400 = 0.20440227165818214
Training loss on iteration 2420 = 0.19980682618916035
Training loss on iteration 2440 = 0.1854666206985712
Training loss on iteration 2460 = 0.22561465613543988
Training loss on iteration 2480 = 0.22800428569316863
Training loss on iteration 2500 = 0.22274280935525895
Training loss on iteration 2520 = 0.18846710920333862
Training loss on iteration 2540 = 0.24371000565588474
Training loss on iteration 2560 = 0.1862803462892771
Training loss on iteration 2580 = 0.2346002347767353
Training loss on iteration 2600 = 0.2350151479244232
Training loss on iteration 2620 = 0.20451871305704117
Training loss on iteration 2640 = 0.22589355073869227
Training loss on iteration 2660 = 0.23389331810176373
Training loss on iteration 2680 = 0.26147905364632607
Training loss on iteration 2700 = 0.26846237741410733
Training loss on iteration 2720 = 0.21634067818522454
Training loss on iteration 2740 = 0.2383664794266224
Training loss on iteration 2760 = 0.1730365138500929
Training loss on iteration 2780 = 0.27007356733083726
Training loss on iteration 2800 = 0.19971425980329513
Training loss on iteration 2820 = 0.23471627309918403
Training loss on iteration 2840 = 0.20128024220466614
Training loss on iteration 2860 = 0.220398972928524
Training loss on iteration 2880 = 0.22903054840862752
Training loss on iteration 2900 = 0.1892112225294113
Training loss on iteration 2920 = 0.2715499073266983
Training loss on iteration 2940 = 0.2470135662704706
Training loss on iteration 2960 = 0.2251299362629652
Training loss on iteration 2980 = 0.19535727761685848
Training loss on iteration 3000 = 0.21707952618598939
Training loss on iteration 3020 = 0.220066075026989
Training loss on iteration 3040 = 0.22198766693472863
Training loss on iteration 3060 = 0.20180109851062297
Training loss on iteration 3080 = 0.18850482515990735
Training loss on iteration 3100 = 0.19780173301696777
Training loss on iteration 3120 = 0.19741974547505378
Training loss on iteration 3140 = 0.18350506015121937
Training loss on iteration 3160 = 0.20219183675944805
Training loss on iteration 3180 = 0.23433946445584297
Training loss on iteration 3200 = 0.19985746666789056
Training loss on iteration 3220 = 0.23031972125172614
Training loss on iteration 3240 = 0.22330427430570127
Training loss on iteration 3260 = 0.2347550719976425
Training loss on iteration 3280 = 0.22111119478940963
Training loss on iteration 3300 = 0.21551993638277053
Training loss on iteration 3320 = 0.21219884194433689
Training loss on iteration 3340 = 0.2237744577229023
Training loss on iteration 3360 = 0.22476302944123744
Training loss on iteration 3380 = 0.19289686121046543
Training loss on iteration 3400 = 0.26585972905158994
Training loss on iteration 3420 = 0.23397074826061726
Training loss on iteration 3440 = 0.22336973249912262
Training loss on iteration 3460 = 0.19911246225237847
Training loss on iteration 3480 = 0.20380163379013538
Training loss on iteration 3500 = 0.23238007500767707
Training loss on iteration 3520 = 0.20830201171338558
Training loss on iteration 3540 = 0.21466166228055955
Training loss on iteration 3560 = 0.2227437287569046
Training loss on iteration 3580 = 0.21083141416311263
Training loss on iteration 3600 = 0.25071674063801763
Training loss on iteration 3620 = 0.18646260872483253
Training loss on iteration 3640 = 0.21495024338364602
Training loss on iteration 3660 = 0.19224020317196847
Training loss on iteration 3680 = 0.23141219802200794
Training loss on iteration 3700 = 0.2297903761267662
Training loss on iteration 3720 = 0.2239710234105587
Training loss on iteration 3740 = 0.18982601836323737
Training loss on iteration 3760 = 0.20474336184561254
Training loss on iteration 3780 = 0.2012938566505909
Training loss on iteration 3800 = 0.18615625575184822
Training loss on iteration 3820 = 0.2059012059122324
Training loss on iteration 3840 = 0.22053102180361747
Training loss on iteration 3860 = 0.23970478475093843
Training loss on iteration 3880 = 0.20268096048384904
Training loss on iteration 3900 = 0.22742805816233158
Training loss on iteration 3920 = 0.229696337133646
Training loss on iteration 3940 = 0.22178085669875144
Training loss on iteration 3960 = 0.2226879768073559
Training loss on iteration 3980 = 0.2211963001638651
Training loss on iteration 4000 = 0.20833802036941051
Training loss on iteration 4020 = 0.20055916234850885
Training loss on iteration 4040 = 0.2391988318413496
Training loss on iteration 4060 = 0.22082068622112275
Training loss on iteration 4080 = 0.2359713099896908
Training loss on iteration 4100 = 0.2418134979903698
Training loss on iteration 4120 = 0.192654687538743
Training loss on iteration 4140 = 0.22143584564328195
Training loss on iteration 4160 = 0.2400825224816799
Training loss on iteration 4180 = 0.21172065213322638
Training loss on iteration 4200 = 0.2304138969630003
Training loss on iteration 4220 = 0.20202672854065895
Training loss on iteration 4240 = 0.22802471071481706
Training loss on iteration 4260 = 0.20866590067744256
Training loss on iteration 4280 = 0.18997043147683143
Training loss on iteration 4300 = 0.2244955077767372
Training loss on iteration 4320 = 0.20313089340925217
Training loss on iteration 4340 = 0.19023049511015416
Training loss on iteration 4360 = 0.19559154361486436
Training loss on iteration 4380 = 0.2425265610218048
Training loss on iteration 4400 = 0.24362195804715156
Training loss on iteration 4420 = 0.21486732065677644
Training loss on iteration 4440 = 0.1997123833745718
Training loss on iteration 4460 = 0.2840868704020977
Training loss on iteration 4480 = 0.24717591255903243
Training loss on iteration 4500 = 0.22731125205755234
Training loss on iteration 4520 = 0.18780176527798176
Training loss on iteration 4540 = 0.21895514577627181
Training loss on iteration 4560 = 0.23337127193808554
Training loss on iteration 4580 = 0.21252304576337339
Training loss on iteration 4600 = 0.19603699631989002
Training loss on iteration 4620 = 0.19807639084756373
Training loss on iteration 4640 = 0.20711150169372558
Training loss on iteration 4660 = 0.23240299932658673
Training loss on iteration 4680 = 0.23390343636274338
Training loss on iteration 4700 = 0.22614074535667897
Training loss on iteration 4720 = 0.1943370271474123
Training loss on iteration 4740 = 0.2007567249238491
Training loss on iteration 4760 = 0.20399900749325753
Training loss on iteration 4780 = 0.20085644125938415
Training loss on iteration 4800 = 0.22544400095939637
Training loss on iteration 4820 = 0.25609240680933
Training loss on iteration 4840 = 0.21524756252765656
Training loss on iteration 4860 = 0.22000273130834103
Training loss on iteration 4880 = 0.21218488737940788
Training loss on iteration 4900 = 0.22261857576668262
Training loss on iteration 4920 = 0.24160257391631604
Training loss on iteration 4940 = 0.21521258167922497
Training loss on iteration 4960 = 0.24296324029564859
Training loss on iteration 4980 = 0.26554407030344007
Training loss on iteration 5000 = 0.26576321572065353
Training loss on iteration 5020 = 0.19844141975045204
Training loss on iteration 5040 = 0.2206789717078209
Training loss on iteration 5060 = 0.20794479586184025
Training loss on iteration 5080 = 0.2587360844016075
Training loss on iteration 5100 = 0.22154558673501015
Training loss on iteration 5120 = 0.2147793650627136
Training loss on iteration 5140 = 0.24321744367480277
Training loss on iteration 5160 = 0.20230280682444574
Training loss on iteration 5180 = 0.22470670007169247
Training loss on iteration 5200 = 0.21488063149154185
Training loss on iteration 5220 = 0.19673616625368595
Training loss on iteration 5240 = 0.24493812173604965
Training loss on iteration 5260 = 0.20767858996987343
Training loss on iteration 5280 = 0.20713165104389192
Training loss on iteration 5300 = 0.20674331523478032
Training loss on iteration 5320 = 0.20257000513374807
Training loss on iteration 5340 = 0.2401266060769558
Training loss on iteration 5360 = 0.1991697896271944
Training loss on iteration 5380 = 0.21396246999502183
Training loss on iteration 5400 = 0.2075471043586731
Training loss on iteration 5420 = 0.19728428907692433
Training loss on iteration 5440 = 0.20150739774107934
Training loss on iteration 5460 = 0.19375667832791804
Training loss on iteration 5480 = 0.2318168755620718
Training loss on iteration 5500 = 0.1890481036156416
Training loss on iteration 5520 = 0.23546984903514384
Training loss on iteration 5540 = 0.261086256057024
Training loss on iteration 5560 = 0.26613068133592604
Training loss on iteration 5580 = 0.21234962195158005
Training loss on iteration 5600 = 0.23412489295005798
Training loss on iteration 5620 = 0.26773461103439333
Training loss on iteration 5640 = 0.21878714337944985
Training loss on iteration 5660 = 0.21663147434592248
Training loss on iteration 5680 = 0.21816810443997384
Training loss on iteration 5700 = 0.2346157718449831
Training loss on iteration 5720 = 0.23867054991424083
Training loss on iteration 5740 = 0.20985695943236352
Training loss on iteration 5760 = 0.2191941425204277
Training loss on iteration 5780 = 0.18151582032442093
Training loss on iteration 5800 = 0.23400537371635438
Training loss on iteration 5820 = 0.25496029630303385
Training loss on iteration 5840 = 0.25730987787246706
Training loss on iteration 5860 = 0.24564730152487754
Training loss on iteration 5880 = 0.22152001559734344
Training loss on iteration 5900 = 0.21257941722869872
Training loss on iteration 5920 = 0.21898842826485634
Training loss on iteration 5940 = 0.23797462768852712
Training loss on iteration 5960 = 0.2299743816256523
Training loss on iteration 5980 = 0.2065299551934004
Training loss on iteration 6000 = 0.24959113970398902
Training loss on iteration 6020 = 0.22439503856003284
Training loss on iteration 6040 = 0.22459411062300205
Training loss on iteration 6060 = 0.2222157709300518
Training loss on iteration 6080 = 0.22553934082388877
Training loss on iteration 6100 = 0.24942550510168077
Training loss on iteration 6120 = 0.20678493119776248
Training loss on iteration 6140 = 0.20827850326895714
Training loss on iteration 6160 = 0.20405444428324698
Training loss on iteration 6180 = 0.21188049390912056
Training loss on iteration 6200 = 0.2197214424610138
Training loss on iteration 6220 = 0.23590354062616825
Training loss on iteration 6240 = 0.23156370967626572
Training loss on iteration 6260 = 0.22990643754601478
Training loss on iteration 6280 = 0.20005765780806542
Training loss on iteration 6300 = 0.21310033276677132
Training loss on iteration 6320 = 0.20204957835376264
Training loss on iteration 6340 = 0.23936913460493087
Training loss on iteration 6360 = 0.21528003513813018
Training loss on iteration 6380 = 0.21177717708051205
Training loss on iteration 0 = 0.1507057547569275
Training loss on iteration 20 = 0.19117078743875027
Training loss on iteration 40 = 0.18758254908025265
Training loss on iteration 60 = 0.18492618352174758
Training loss on iteration 80 = 0.19622979164123536
Training loss on iteration 100 = 0.16522015985101463
Training loss on iteration 120 = 0.2089578364044428
Training loss on iteration 140 = 0.2004017312079668
Training loss on iteration 160 = 0.20925631299614905
Training loss on iteration 180 = 0.2128487903624773
Training loss on iteration 200 = 0.20493378005921842
Training loss on iteration 220 = 0.1845183629542589
Training loss on iteration 240 = 0.16431826651096343
Training loss on iteration 260 = 0.21468024663627147
Training loss on iteration 280 = 0.189824827760458
Training loss on iteration 300 = 0.18529417142271995
Training loss on iteration 320 = 0.18393942825496196
Training loss on iteration 340 = 0.2583307929337025
Training loss on iteration 360 = 0.15677756257355213
Training loss on iteration 380 = 0.1875377047806978
Training loss on iteration 400 = 0.21671418845653534
Training loss on iteration 420 = 0.1989562876522541
Training loss on iteration 440 = 0.23879463858902455
Training loss on iteration 460 = 0.21972623504698277
Training loss on iteration 480 = 0.19133739061653615
Training loss on iteration 500 = 0.2314083181321621
Training loss on iteration 520 = 0.19868414849042892
Training loss on iteration 540 = 0.1955864906311035
Training loss on iteration 560 = 0.18439512141048908
Training loss on iteration 580 = 0.2013751070946455
Training loss on iteration 600 = 0.1802081350237131
Training loss on iteration 620 = 0.18785346448421478
Training loss on iteration 640 = 0.157045229524374
Training loss on iteration 660 = 0.22486388608813285
Training loss on iteration 680 = 0.24257634319365023
Training loss on iteration 700 = 0.20475913770496845
Training loss on iteration 720 = 0.19453998543322087
Training loss on iteration 740 = 0.17828630581498145
Training loss on iteration 760 = 0.21845051161944867
Training loss on iteration 780 = 0.18418365493416786
Training loss on iteration 800 = 0.3722317576408386
Training loss on iteration 820 = 0.20084274411201478
Training loss on iteration 840 = 0.19905442222952843
Training loss on iteration 860 = 0.20959224440157415
Training loss on iteration 880 = 0.19866046048700808
Training loss on iteration 900 = 0.18285922519862652
Training loss on iteration 920 = 0.17748397141695021
Training loss on iteration 940 = 0.18757166750729085
Training loss on iteration 960 = 0.18513920158147812
Training loss on iteration 980 = 0.20115062706172465
Training loss on iteration 1000 = 0.1881191346794367
Training loss on iteration 1020 = 0.18125694394111633
Training loss on iteration 1040 = 0.1877567671239376
Training loss on iteration 1060 = 0.19806265980005264
Training loss on iteration 1080 = 0.17137064598500729
Training loss on iteration 1100 = 0.19243764579296113
Training loss on iteration 1120 = 0.1956638429313898
Training loss on iteration 1140 = 0.19977945759892463
Training loss on iteration 1160 = 0.21472071036696433
Training loss on iteration 1180 = 0.19275753684341906
Training loss on iteration 1200 = 0.19237946681678295
Training loss on iteration 1220 = 0.17000772282481194
Training loss on iteration 1240 = 0.17683656625449656
Training loss on iteration 1260 = 0.2019363597035408
Training loss on iteration 1280 = 0.17064650766551495
Training loss on iteration 1300 = 0.20847628079354763
Training loss on iteration 1320 = 0.2266454204916954
Training loss on iteration 1340 = 0.20396655946969985
Training loss on iteration 1360 = 0.22078243419528007
Training loss on iteration 1380 = 0.1834525104612112
Training loss on iteration 1400 = 0.23519411832094192
Training loss on iteration 1420 = 0.18662131875753402
Training loss on iteration 1440 = 0.1871110662817955
Training loss on iteration 1460 = 0.21419700384140014
Training loss on iteration 1480 = 0.19360656216740607
Training loss on iteration 1500 = 0.21021004766225815
Training loss on iteration 1520 = 0.2510777983814478
Training loss on iteration 1540 = 0.19867460019886493
Training loss on iteration 1560 = 0.18478733561933042
Training loss on iteration 1580 = 0.1830080445855856
Training loss on iteration 1600 = 0.22252423092722892
Training loss on iteration 1620 = 0.22353535778820516
Training loss on iteration 1640 = 0.19648981280624866
Training loss on iteration 1660 = 0.1930292896926403
Training loss on iteration 1680 = 0.1673581175506115
Training loss on iteration 1700 = 0.21936590895056723
Training loss on iteration 1720 = 0.2051954485476017
Training loss on iteration 1740 = 0.2489112690091133
Training loss on iteration 1760 = 0.17485777400434016
Training loss on iteration 1780 = 0.19685440175235272
Training loss on iteration 1800 = 0.20541246868669988
Training loss on iteration 1820 = 0.17733571901917458
Training loss on iteration 1840 = 0.21043471321463586
Training loss on iteration 1860 = 0.21529182977974415
Training loss on iteration 1880 = 0.19525230638682842
Training loss on iteration 1900 = 0.1617361329495907
Training loss on iteration 1920 = 0.20976592041552067
Training loss on iteration 1940 = 0.22793954908847808
Training loss on iteration 1960 = 0.24401308335363864
Training loss on iteration 1980 = 0.20282243937253952
Training loss on iteration 2000 = 0.21322204247117044
Training loss on iteration 2020 = 0.21588709391653538
Training loss on iteration 2040 = 0.18685118146240712
Training loss on iteration 2060 = 0.20310676842927933
Training loss on iteration 2080 = 0.21449531316757203
Training loss on iteration 2100 = 0.1884371019899845
Training loss on iteration 2120 = 0.21901863180100917
Training loss on iteration 2140 = 0.21219909824430944
Training loss on iteration 2160 = 0.2095382995903492
Training loss on iteration 2180 = 0.19169332832098007
Training loss on iteration 2200 = 0.20959989987313749
Training loss on iteration 2220 = 0.2395995557308197
Training loss on iteration 2240 = 0.1848460104316473
Training loss on iteration 2260 = 0.2085563100874424
Training loss on iteration 2280 = 0.20055725015699863
Training loss on iteration 2300 = 0.17331454567611218
Training loss on iteration 2320 = 0.1968931645154953
Training loss on iteration 2340 = 0.21496074311435223
Training loss on iteration 2360 = 0.20414049103856086
Training loss on iteration 2380 = 0.20847755409777163
Training loss on iteration 2400 = 0.18090798296034336
Training loss on iteration 2420 = 0.20648222118616105
Training loss on iteration 2440 = 0.21777735017240046
Training loss on iteration 2460 = 0.20643368698656558
Training loss on iteration 2480 = 0.23091654926538469
Training loss on iteration 2500 = 0.17300253883004188
Training loss on iteration 2520 = 0.17839258164167404
Training loss on iteration 2540 = 0.2039253182709217
Training loss on iteration 2560 = 0.2334715373814106
Training loss on iteration 2580 = 0.25971588045358657
Training loss on iteration 2600 = 0.22282696962356568
Training loss on iteration 2620 = 0.23278586007654667
Training loss on iteration 2640 = 0.23349089846014975
Training loss on iteration 2660 = 0.20304575487971305
Training loss on iteration 2680 = 0.204028757289052
Training loss on iteration 2700 = 0.19378518983721732
Training loss on iteration 2720 = 0.20507041737437248
Training loss on iteration 2740 = 0.217528073489666
Training loss on iteration 2760 = 0.1912149589508772
Training loss on iteration 2780 = 0.22968608289957046
Training loss on iteration 2800 = 0.20175938978791236
Training loss on iteration 2820 = 0.20142820999026298
Training loss on iteration 2840 = 0.1871839240193367
Training loss on iteration 2860 = 0.19354076646268367
Training loss on iteration 2880 = 0.2450760431587696
Training loss on iteration 2900 = 0.20068788565695286
Training loss on iteration 2920 = 0.20446366518735887
Training loss on iteration 2940 = 0.18999627344310283
Training loss on iteration 2960 = 0.24437793381512166
Training loss on iteration 2980 = 0.23653495460748672
Training loss on iteration 3000 = 0.20727557949721814
Training loss on iteration 3020 = 0.1978569518774748
Training loss on iteration 3040 = 0.1848929315805435
Training loss on iteration 3060 = 0.22448897399008275
Training loss on iteration 3080 = 0.26835290864109995
Training loss on iteration 3100 = 0.19633495211601257
Training loss on iteration 3120 = 0.20202372819185258
Training loss on iteration 3140 = 0.21730648763477803
Training loss on iteration 3160 = 0.20426614619791508
Training loss on iteration 3180 = 0.19458888843655586
Training loss on iteration 3200 = 0.19405374377965928
Training loss on iteration 3220 = 0.19821131452918053
Training loss on iteration 3240 = 0.1955601755529642
Training loss on iteration 3260 = 0.29814779087901117
Training loss on iteration 3280 = 0.25166141912341117
Training loss on iteration 3300 = 0.26821446903049945
Training loss on iteration 3320 = 0.2304547071456909
Training loss on iteration 3340 = 0.1920428268611431
Training loss on iteration 3360 = 0.20541435666382313
Training loss on iteration 3380 = 0.1993766486644745
Training loss on iteration 3400 = 0.2040706120431423
Training loss on iteration 3420 = 0.20060802698135377
Training loss on iteration 3440 = 0.18690460324287414
Training loss on iteration 3460 = 0.2032772868871689
Training loss on iteration 3480 = 0.24816130995750427
Training loss on iteration 3500 = 0.22184791453182698
Training loss on iteration 3520 = 0.22355220690369607
Training loss on iteration 3540 = 0.23063270673155783
Training loss on iteration 3560 = 0.24115439504384995
Training loss on iteration 3580 = 0.21397668793797492
Training loss on iteration 3600 = 0.19417012929916383
Training loss on iteration 3620 = 0.22047400325536728
Training loss on iteration 3640 = 0.20479343682527543
Training loss on iteration 3660 = 0.17568015791475772
Training loss on iteration 3680 = 0.18746974729001523
Training loss on iteration 3700 = 0.1773732852190733
Training loss on iteration 3720 = 0.1901349902153015
Training loss on iteration 3740 = 0.1865645285695791
Training loss on iteration 3760 = 0.20919312797486783
Training loss on iteration 3780 = 0.20442444123327733
Training loss on iteration 3800 = 0.2074098840355873
Training loss on iteration 3820 = 0.20727130584418774
Training loss on iteration 3840 = 0.22071919031441212
Training loss on iteration 3860 = 0.17093837223947048
Training loss on iteration 3880 = 0.19636622965335845
Training loss on iteration 3900 = 0.21788562275469303
Training loss on iteration 3920 = 0.1889699563384056
Training loss on iteration 3940 = 0.21335790827870368
Training loss on iteration 3960 = 0.2161705195903778
Training loss on iteration 3980 = 0.19275535978376865
Training loss on iteration 4000 = 0.2060464832931757
Training loss on iteration 4020 = 0.23390541970729828
Training loss on iteration 4040 = 0.17552617453038694
Training loss on iteration 4060 = 0.18370512500405312
Training loss on iteration 4080 = 0.22760707065463065
Training loss on iteration 4100 = 0.21398120746016502
Training loss on iteration 4120 = 0.19701781570911409
Training loss on iteration 4140 = 0.21348194144666194
Training loss on iteration 4160 = 0.27746106311678886
Training loss on iteration 4180 = 0.21666454710066319
Training loss on iteration 4200 = 0.20638007670640945
Training loss on iteration 4220 = 0.19790655188262463
Training loss on iteration 4240 = 0.22443058863282203
Training loss on iteration 4260 = 0.20122777111828327
Training loss on iteration 4280 = 0.18348448388278485
Training loss on iteration 4300 = 0.21083224266767503
Training loss on iteration 4320 = 0.19209577552974225
Training loss on iteration 4340 = 0.22987536527216434
Training loss on iteration 4360 = 0.18853048756718635
Training loss on iteration 4380 = 0.2303602982312441
Training loss on iteration 4400 = 0.21386385075747966
Training loss on iteration 4420 = 0.20825038552284242
Training loss on iteration 4440 = 0.18184131719172
Training loss on iteration 4460 = 0.20094394832849502
Training loss on iteration 4480 = 0.18097329437732695
Training loss on iteration 4500 = 0.1963473528623581
Training loss on iteration 4520 = 0.19249528534710408
Training loss on iteration 4540 = 0.1994017343968153
Training loss on iteration 4560 = 0.39358303472399714
Training loss on iteration 4580 = 0.19913225583732128
Training loss on iteration 4600 = 0.21507143527269362
Training loss on iteration 4620 = 0.19391252994537353
Training loss on iteration 4640 = 0.17943931370973587
Training loss on iteration 4660 = 0.23761296272277832
Training loss on iteration 4680 = 0.19961128681898116
Training loss on iteration 4700 = 0.2008657321333885
Training loss on iteration 4720 = 0.2030694644898176
Training loss on iteration 4740 = 0.22953177765011787
Training loss on iteration 4760 = 0.2116649691015482
Training loss on iteration 4780 = 0.19609832502901553
Training loss on iteration 4800 = 0.19090475738048554
Training loss on iteration 4820 = 0.20565626099705697
Training loss on iteration 4840 = 0.20736847966909408
Training loss on iteration 4860 = 0.19343804232776166
Training loss on iteration 4880 = 0.1922417998313904
Training loss on iteration 4900 = 0.22348097525537014
Training loss on iteration 4920 = 0.18845872655510904
Training loss on iteration 4940 = 0.1957257553935051
Training loss on iteration 4960 = 0.18771794624626637
Training loss on iteration 4980 = 0.20124989710748195
Training loss on iteration 5000 = 0.20073199830949306
Training loss on iteration 5020 = 0.20280967392027377
Training loss on iteration 5040 = 0.20813270471990108
Training loss on iteration 5060 = 0.22564486265182496
Training loss on iteration 5080 = 0.2038210488855839
Training loss on iteration 5100 = 0.21669002249836922
Training loss on iteration 5120 = 0.20769452750682832
Training loss on iteration 5140 = 0.17914305441081524
Training loss on iteration 5160 = 0.18277003318071366
Training loss on iteration 5180 = 0.18976398408412934
Training loss on iteration 5200 = 0.18935809694230557
Training loss on iteration 5220 = 0.17676182091236115
Training loss on iteration 5240 = 0.20986383073031903
Training loss on iteration 5260 = 0.20561669878661631
Training loss on iteration 5280 = 0.22346604317426683
Training loss on iteration 5300 = 0.21077539063990117
Training loss on iteration 5320 = 0.20623419620096684
Training loss on iteration 5340 = 0.22612083703279495
Training loss on iteration 5360 = 0.16644657999277115
Training loss on iteration 5380 = 0.21045269295573235
Training loss on iteration 5400 = 0.1947015557438135
Training loss on iteration 5420 = 0.20839787796139717
Training loss on iteration 5440 = 0.20769949853420258
Training loss on iteration 5460 = 0.19410061612725257
Training loss on iteration 5480 = 0.19055448062717914
Training loss on iteration 5500 = 0.17495382837951184
Training loss on iteration 5520 = 0.23781302385032177
Training loss on iteration 5540 = 0.22893988601863385
Training loss on iteration 5560 = 0.22328344844281672
Training loss on iteration 5580 = 0.2145460095256567
Training loss on iteration 5600 = 0.19190559163689613
Training loss on iteration 5620 = 0.2378461517393589
Training loss on iteration 5640 = 0.19472708590328694
Training loss on iteration 5660 = 0.21944847293198108
Training loss on iteration 5680 = 0.188095261156559
Training loss on iteration 5700 = 0.2195308819413185
Training loss on iteration 5720 = 0.2116089679300785
Training loss on iteration 5740 = 0.1927472598850727
Training loss on iteration 5760 = 0.22454328835010529
Training loss on iteration 5780 = 0.19873238317668437
Training loss on iteration 5800 = 0.21341097205877305
Training loss on iteration 5820 = 0.20654211789369584
Training loss on iteration 5840 = 0.19454010725021362
Training loss on iteration 5860 = 0.2216544434428215
Training loss on iteration 5880 = 0.18364267088472844
Training loss on iteration 5900 = 0.1846124306321144
Training loss on iteration 5920 = 0.24088860154151917
Training loss on iteration 5940 = 0.21220201067626476
Training loss on iteration 5960 = 0.2583222836256027
Training loss on iteration 5980 = 0.1651204615831375
Training loss on iteration 6000 = 0.20884326361119748
Training loss on iteration 6020 = 0.24061642549932002
Training loss on iteration 6040 = 0.17780212089419364
Training loss on iteration 6060 = 0.19513027854263781
Training loss on iteration 6080 = 0.24321318604052067
Training loss on iteration 6100 = 0.2516554594039917
Training loss on iteration 6120 = 0.22868561521172523
Training loss on iteration 6140 = 0.2524147253483534
Training loss on iteration 6160 = 0.21042988784611225
Training loss on iteration 6180 = 0.22434651367366315
Training loss on iteration 6200 = 0.18849488012492657
Training loss on iteration 6220 = 0.18909926079213618
Training loss on iteration 6240 = 0.17929369769990444
Training loss on iteration 6260 = 0.20957895629107953
Training loss on iteration 6280 = 0.1777478910982609
Training loss on iteration 6300 = 0.20644329786300658
Training loss on iteration 6320 = 0.1762227900326252
Training loss on iteration 6340 = 0.20686256587505342
Training loss on iteration 6360 = 0.1974465072154999
Training loss on iteration 6380 = 0.20869093835353852
Training loss on iteration 0 = 0.1599183827638626
Training loss on iteration 20 = 0.19735355377197267
Training loss on iteration 40 = 0.22181028500199318
Training loss on iteration 60 = 0.1817033275961876
Training loss on iteration 80 = 0.19567348435521126
Training loss on iteration 100 = 0.16354138702154158
Training loss on iteration 120 = 0.18543799482285978
Training loss on iteration 140 = 0.2108875598758459
Training loss on iteration 160 = 0.16788748539984227
Training loss on iteration 180 = 0.20206139124929906
Training loss on iteration 200 = 0.19772380515933036
Training loss on iteration 220 = 0.1845068007707596
Training loss on iteration 240 = 0.1760210558772087
Training loss on iteration 260 = 0.1586587969213724
Training loss on iteration 280 = 0.17761199548840523
Training loss on iteration 300 = 0.19888386316597462
Training loss on iteration 320 = 0.1803594555705786
Training loss on iteration 340 = 0.17233918718993663
Training loss on iteration 360 = 0.18947002850472927
Training loss on iteration 380 = 0.17479178570210935
Training loss on iteration 400 = 0.18788846768438816
Training loss on iteration 420 = 0.17183457650244235
Training loss on iteration 440 = 0.16821128129959106
Training loss on iteration 460 = 0.16242622286081315
Training loss on iteration 480 = 0.20032321363687516
Training loss on iteration 500 = 0.20563553646206856
Training loss on iteration 520 = 0.17853107303380966
Training loss on iteration 540 = 0.20384061969816686
Training loss on iteration 560 = 0.17227873727679252
Training loss on iteration 580 = 0.16057452335953712
Training loss on iteration 600 = 0.1734689697623253
Training loss on iteration 620 = 0.17031941339373588
Training loss on iteration 640 = 0.17625177949666976
Training loss on iteration 660 = 0.1972898043692112
Training loss on iteration 680 = 0.18340871743857862
Training loss on iteration 700 = 0.19382674917578696
Training loss on iteration 720 = 0.1797338891774416
Training loss on iteration 740 = 0.1713450461626053
Training loss on iteration 760 = 0.18601103350520135
Training loss on iteration 780 = 0.22502183653414248
Training loss on iteration 800 = 0.17031786143779754
Training loss on iteration 820 = 0.15456317253410817
Training loss on iteration 840 = 0.17213362753391265
Training loss on iteration 860 = 0.17291526310145855
Training loss on iteration 880 = 0.15320711620151997
Training loss on iteration 900 = 0.1665078416466713
Training loss on iteration 920 = 0.17153000757098197
Training loss on iteration 940 = 0.17203771583735944
Training loss on iteration 960 = 0.20756042525172233
Training loss on iteration 980 = 0.20184255875647067
Training loss on iteration 1000 = 0.25741956271231176
Training loss on iteration 1020 = 0.17803364917635917
Training loss on iteration 1040 = 0.17218346782028676
Training loss on iteration 1060 = 0.17903085835278035
Training loss on iteration 1080 = 0.19328165389597415
Training loss on iteration 1100 = 0.18909848816692829
Training loss on iteration 1120 = 0.18682506941258908
Training loss on iteration 1140 = 0.2705583583563566
Training loss on iteration 1160 = 0.1589116994291544
Training loss on iteration 1180 = 0.18281396999955177
Training loss on iteration 1200 = 0.18355719596147538
Training loss on iteration 1220 = 0.21294262297451497
Training loss on iteration 1240 = 0.16575728133320808
Training loss on iteration 1260 = 0.24309727177023888
Training loss on iteration 1280 = 0.1991446826606989
Training loss on iteration 1300 = 0.18853839598596095
Training loss on iteration 1320 = 0.18931704126298426
Training loss on iteration 1340 = 0.20649562031030655
Training loss on iteration 1360 = 0.2081065945327282
Training loss on iteration 1380 = 0.17103959619998932
Training loss on iteration 1400 = 0.17206366024911404
Training loss on iteration 1420 = 0.22672392912209033
Training loss on iteration 1440 = 0.21592313647270203
Training loss on iteration 1460 = 0.18182213194668292
Training loss on iteration 1480 = 0.16312608644366264
Training loss on iteration 1500 = 0.24571523815393448
Training loss on iteration 1520 = 0.17963303551077842
Training loss on iteration 1540 = 0.19006723277270793
Training loss on iteration 1560 = 0.19551165774464607
Training loss on iteration 1580 = 0.1904717914760113
Training loss on iteration 1600 = 0.176693457365036
Training loss on iteration 1620 = 0.2064195778220892
Training loss on iteration 1640 = 0.15647480450570583
Training loss on iteration 1660 = 0.22805337533354758
Training loss on iteration 1680 = 0.19101825207471848
Training loss on iteration 1700 = 0.1772052116692066
Training loss on iteration 1720 = 0.19986194223165513
Training loss on iteration 1740 = 0.20612239465117455
Training loss on iteration 1760 = 0.17959627844393253
Training loss on iteration 1780 = 0.2005596227943897
Training loss on iteration 1800 = 0.21822518110275269
Training loss on iteration 1820 = 0.1775624740868807
Training loss on iteration 1840 = 0.22497818768024444
Training loss on iteration 1860 = 0.18669793866574763
Training loss on iteration 1880 = 0.20386158712208272
Training loss on iteration 1900 = 0.17708248645067215
Training loss on iteration 1920 = 0.22953596264123916
Training loss on iteration 1940 = 0.2169996749609709
Training loss on iteration 1960 = 0.1858830850571394
Training loss on iteration 1980 = 0.2782315775752068
Training loss on iteration 2000 = 0.2853786751627922
Training loss on iteration 2020 = 0.1897253856062889
Training loss on iteration 2040 = 0.20473065003752708
Training loss on iteration 2060 = 0.18463392741978168
Training loss on iteration 2080 = 0.2194213353097439
Training loss on iteration 2100 = 0.17032474912703038
Training loss on iteration 2120 = 0.20031172297894956
Training loss on iteration 2140 = 0.18961610160768033
Training loss on iteration 2160 = 0.2046936869621277
Training loss on iteration 2180 = 0.19852332398295403
Training loss on iteration 2200 = 0.1891518533229828
Training loss on iteration 2220 = 0.17491669096052648
Training loss on iteration 2240 = 0.1824352715164423
Training loss on iteration 2260 = 0.1867929369211197
Training loss on iteration 2280 = 0.201400838047266
Training loss on iteration 2300 = 0.20431463196873664
Training loss on iteration 2320 = 0.17544966116547583
Training loss on iteration 2340 = 0.21169934533536433
Training loss on iteration 2360 = 0.18024672605097294
Training loss on iteration 2380 = 0.183498752489686
Training loss on iteration 2400 = 0.21687004938721657
Training loss on iteration 2420 = 0.21975826136767865
Training loss on iteration 2440 = 0.3751190580427647
Training loss on iteration 2460 = 0.1849774621427059
Training loss on iteration 2480 = 0.2174418307840824
Training loss on iteration 2500 = 0.21578689217567443
Training loss on iteration 2520 = 0.19219330847263336
Training loss on iteration 2540 = 0.19935304746031762
Training loss on iteration 2560 = 0.20028549768030643
Training loss on iteration 2580 = 0.1940797794610262
Training loss on iteration 2600 = 0.20613627396523954
Training loss on iteration 2620 = 0.18955099806189538
Training loss on iteration 2640 = 0.2003883793950081
Training loss on iteration 2660 = 0.18908172175288201
Training loss on iteration 2680 = 0.16858888305723668
Training loss on iteration 2700 = 0.24564577527344228
Training loss on iteration 2720 = 0.23258377015590667
Training loss on iteration 2740 = 0.21165354177355766
Training loss on iteration 2760 = 0.1832440011203289
Training loss on iteration 2780 = 0.1792163286358118
Training loss on iteration 2800 = 0.2039081111550331
Training loss on iteration 2820 = 0.21184398904442786
Training loss on iteration 2840 = 0.21736813262104987
Training loss on iteration 2860 = 0.1999737937003374
Training loss on iteration 2880 = 0.20955954305827618
Training loss on iteration 2900 = 0.1934816125780344
Training loss on iteration 2920 = 0.18544939458370208
Training loss on iteration 2940 = 0.21881708800792693
Training loss on iteration 2960 = 0.1931931421160698
Training loss on iteration 2980 = 0.19197795987129213
Training loss on iteration 3000 = 0.18474027290940284
Training loss on iteration 3020 = 0.1967196397483349
Training loss on iteration 3040 = 0.17874683253467083
Training loss on iteration 3060 = 0.19937302283942698
Training loss on iteration 3080 = 0.16870759837329388
Training loss on iteration 3100 = 0.21556698381900788
Training loss on iteration 3120 = 0.17495748475193978
Training loss on iteration 3140 = 0.19648728594183923
Training loss on iteration 3160 = 0.18608632683753967
Training loss on iteration 3180 = 0.1873073659837246
Training loss on iteration 3200 = 0.1915117260068655
Training loss on iteration 3220 = 0.1820422038435936
Training loss on iteration 3240 = 0.17366753481328487
Training loss on iteration 3260 = 0.18901587091386318
Training loss on iteration 3280 = 0.18354325518012046
Training loss on iteration 3300 = 0.2209071535617113
Training loss on iteration 3320 = 0.17408265322446823
Training loss on iteration 3340 = 0.18079960457980632
Training loss on iteration 3360 = 0.18017885759472846
Training loss on iteration 3380 = 0.4606064073741436
Training loss on iteration 3400 = 0.22809801772236823
Training loss on iteration 3420 = 0.21557682752609253
Training loss on iteration 3440 = 0.22353156581521033
Training loss on iteration 3460 = 0.20581618510186672
Training loss on iteration 3480 = 0.17855681143701077
Training loss on iteration 3500 = 0.2173960369080305
Training loss on iteration 3520 = 0.18661897405982017
Training loss on iteration 3540 = 0.20399369932711126
Training loss on iteration 3560 = 0.19104347079992295
Training loss on iteration 3580 = 0.18801613450050353
Training loss on iteration 3600 = 0.1846955217421055
Training loss on iteration 3620 = 0.18730876073241234
Training loss on iteration 3640 = 0.17339820191264152
Training loss on iteration 3660 = 0.18555456697940825
Training loss on iteration 3680 = 0.20634623803198338
Training loss on iteration 3700 = 0.16955946274101735
Training loss on iteration 3720 = 0.1981450855731964
Training loss on iteration 3740 = 0.21423448100686074
Training loss on iteration 3760 = 0.1854508202522993
Training loss on iteration 3780 = 0.18362723737955094
Training loss on iteration 3800 = 0.16384560614824295
Training loss on iteration 3820 = 0.19956893436610698
Training loss on iteration 3840 = 0.18352321684360504
Training loss on iteration 3860 = 0.18066842779517173
Training loss on iteration 3880 = 0.19333118610084057
Training loss on iteration 3900 = 0.23310502097010613
Training loss on iteration 3920 = 0.17171518206596376
Training loss on iteration 3940 = 0.18608626537024975
Training loss on iteration 3960 = 0.21369768679141998
Training loss on iteration 3980 = 0.22088215723633767
Training loss on iteration 4000 = 0.18360156938433647
Training loss on iteration 4020 = 0.2063209518790245
Training loss on iteration 4040 = 0.20383244529366493
Training loss on iteration 4060 = 0.20904764756560326
Training loss on iteration 4080 = 0.23001834824681283
Training loss on iteration 4100 = 0.1911275215446949
Training loss on iteration 4120 = 0.19553064405918122
Training loss on iteration 4140 = 0.2138538934290409
Training loss on iteration 4160 = 0.20684139728546141
Training loss on iteration 4180 = 0.19684899598360062
Training loss on iteration 4200 = 0.19020244106650352
Training loss on iteration 4220 = 0.2045174788683653
Training loss on iteration 4240 = 0.19457997865974902
Training loss on iteration 4260 = 0.1845703948289156
Training loss on iteration 4280 = 0.18626534789800644
Training loss on iteration 4300 = 0.17909370437264444
Training loss on iteration 4320 = 0.1989128790795803
Training loss on iteration 4340 = 0.208584089204669
Training loss on iteration 4360 = 0.18416965678334235
Training loss on iteration 4380 = 0.19245255291461943
Training loss on iteration 4400 = 0.1863088019192219
Training loss on iteration 4420 = 0.1697731088846922
Training loss on iteration 4440 = 0.20327799022197723
Training loss on iteration 4460 = 0.18503153063356875
Training loss on iteration 4480 = 0.2700426630675793
Training loss on iteration 4500 = 0.2143371231853962
Training loss on iteration 4520 = 0.20708580389618875
Training loss on iteration 4540 = 0.19348058700561524
Training loss on iteration 4560 = 0.20604718513786793
Training loss on iteration 4580 = 0.19795484095811844
Training loss on iteration 4600 = 0.2305685505270958
Training loss on iteration 4620 = 0.19943079128861427
Training loss on iteration 4640 = 0.19735761247575284
Training loss on iteration 4660 = 0.17999861091375352
Training loss on iteration 4680 = 0.17271971069276332
Training loss on iteration 4700 = 0.20042432174086572
Training loss on iteration 4720 = 0.21010406203567983
Training loss on iteration 4740 = 0.22981634959578515
Training loss on iteration 4760 = 0.17840998098254204
Training loss on iteration 4780 = 0.18961492814123632
Training loss on iteration 4800 = 0.19300395622849464
Training loss on iteration 4820 = 0.21194885298609734
Training loss on iteration 4840 = 0.16757754646241665
Training loss on iteration 4860 = 0.21914851106703281
Training loss on iteration 4880 = 0.19633157923817635
Training loss on iteration 4900 = 0.20643334574997424
Training loss on iteration 4920 = 0.18821279071271418
Training loss on iteration 4940 = 0.21550037935376168
Training loss on iteration 4960 = 0.18993647806346417
Training loss on iteration 4980 = 0.18361678943037987
Training loss on iteration 5000 = 0.209170475974679
Training loss on iteration 5020 = 0.17665254473686218
Training loss on iteration 5040 = 0.18903657086193562
Training loss on iteration 5060 = 0.19645663686096668
Training loss on iteration 5080 = 0.20908008329570293
Training loss on iteration 5100 = 0.23609777130186557
Training loss on iteration 5120 = 0.21859910301864147
Training loss on iteration 5140 = 0.19513803943991662
Training loss on iteration 5160 = 0.24690118320286275
Training loss on iteration 5180 = 0.19077937714755536
Training loss on iteration 5200 = 0.20329624265432358
Training loss on iteration 5220 = 0.1845491737127304
Training loss on iteration 5240 = 0.18843376711010934
Training loss on iteration 5260 = 0.21349498815834522
Training loss on iteration 5280 = 0.2149690467864275
Training loss on iteration 5300 = 0.21537194065749646
Training loss on iteration 5320 = 0.18897102847695352
Training loss on iteration 5340 = 0.21414661072194577
Training loss on iteration 5360 = 0.2516909383237362
Training loss on iteration 5380 = 0.35992068983614445
Training loss on iteration 5400 = 0.18716562278568744
Training loss on iteration 5420 = 0.18017360121011733
Training loss on iteration 5440 = 0.21520336605608464
Training loss on iteration 5460 = 0.2112373150885105
Training loss on iteration 5480 = 0.17286068573594093
Training loss on iteration 5500 = 0.18225729316473008
Training loss on iteration 5520 = 0.18891116715967654
Training loss on iteration 5540 = 0.18792260885238649
Training loss on iteration 5560 = 0.18407105207443236
Training loss on iteration 5580 = 0.1730692744255066
Training loss on iteration 5600 = 0.21009774096310138
Training loss on iteration 5620 = 0.19394369572401046
Training loss on iteration 5640 = 0.18886129148304462
Training loss on iteration 5660 = 0.20661148317158223
Training loss on iteration 5680 = 0.1950892608612776
Training loss on iteration 5700 = 0.19672643318772315
Training loss on iteration 5720 = 0.18820072375237942
Training loss on iteration 5740 = 0.196596223115921
Training loss on iteration 5760 = 0.18790775276720523
Training loss on iteration 5780 = 0.2091221522539854
Training loss on iteration 5800 = 0.15634154602885247
Training loss on iteration 5820 = 0.1901755541563034
Training loss on iteration 5840 = 0.20276932865381242
Training loss on iteration 5860 = 0.24420699402689933
Training loss on iteration 5880 = 0.19391494542360305
Training loss on iteration 5900 = 0.17691416144371033
Training loss on iteration 5920 = 0.19104674831032753
Training loss on iteration 5940 = 0.20554514266550541
Training loss on iteration 5960 = 0.20573763437569142
Training loss on iteration 5980 = 0.21400810070335866
Training loss on iteration 6000 = 0.1888308234512806
Training loss on iteration 6020 = 0.20686958432197572
Training loss on iteration 6040 = 0.1956269282847643
Training loss on iteration 6060 = 0.19768627025187016
Training loss on iteration 6080 = 0.170178746804595
Training loss on iteration 6100 = 0.19510671943426133
Training loss on iteration 6120 = 0.20264213234186174
Training loss on iteration 6140 = 0.21118395179510116
Training loss on iteration 6160 = 0.20180331990122796
Training loss on iteration 6180 = 0.2056831546127796
Training loss on iteration 6200 = 0.21391898840665818
Training loss on iteration 6220 = 0.2271013781428337
Training loss on iteration 6240 = 0.19172076173126698
Training loss on iteration 6260 = 0.20826632902026176
Training loss on iteration 6280 = 0.22890248708426952
Training loss on iteration 6300 = 0.2573535606265068
Training loss on iteration 6320 = 0.1854562759399414
Training loss on iteration 6340 = 0.18497791066765784
Training loss on iteration 6360 = 0.19030804969370366
Training loss on iteration 6380 = 0.19329955130815507
Training loss on iteration 0 = 0.1699078232049942
Training loss on iteration 20 = 0.14641161561012267
Training loss on iteration 40 = 0.19531617388129235
Training loss on iteration 60 = 0.19617123007774354
Training loss on iteration 80 = 0.20681404285132884
Training loss on iteration 100 = 0.15716542154550553
Training loss on iteration 120 = 0.1705370619893074
Training loss on iteration 140 = 0.15739688985049724
Training loss on iteration 160 = 0.16467432118952274
Training loss on iteration 180 = 0.19402823746204376
Training loss on iteration 200 = 0.16074708439409732
Training loss on iteration 220 = 0.15518127642571927
Training loss on iteration 240 = 0.1692064467817545
Training loss on iteration 260 = 0.17550196535885335
Training loss on iteration 280 = 0.1654708657413721
Training loss on iteration 300 = 0.17364067398011684
Training loss on iteration 320 = 0.14873459786176682
Training loss on iteration 340 = 0.17817091941833496
Training loss on iteration 360 = 0.1755089219659567
Training loss on iteration 380 = 0.16092228516936302
Training loss on iteration 400 = 0.1892200827598572
Training loss on iteration 420 = 0.1601164907217026
Training loss on iteration 440 = 0.1666458763182163
Training loss on iteration 460 = 0.1805501628667116
Training loss on iteration 480 = 0.19020382240414618
Training loss on iteration 500 = 0.17754941768944263
Training loss on iteration 520 = 0.19860783368349075
Training loss on iteration 540 = 0.1849737089127302
Training loss on iteration 560 = 0.17782896310091018
Training loss on iteration 580 = 0.1605048667639494
Training loss on iteration 600 = 0.1566153259947896
Training loss on iteration 620 = 0.19777236841619014
Training loss on iteration 640 = 0.16254533939063548
Training loss on iteration 660 = 0.19002150408923627
Training loss on iteration 680 = 0.16761909797787666
Training loss on iteration 700 = 0.17367812246084213
Training loss on iteration 720 = 0.19408392310142517
Training loss on iteration 740 = 0.23343042135238648
Training loss on iteration 760 = 0.20877522081136704
Training loss on iteration 780 = 0.20682195499539374
Training loss on iteration 800 = 0.1535919975489378
Training loss on iteration 820 = 0.18388743326067924
Training loss on iteration 840 = 0.15375820808112622
Training loss on iteration 860 = 0.18536739200353622
Training loss on iteration 880 = 0.16355934590101243
Training loss on iteration 900 = 0.16950392797589303
Training loss on iteration 920 = 0.16078948341310023
Training loss on iteration 940 = 0.16724807024002075
Training loss on iteration 960 = 0.1546987049281597
Training loss on iteration 980 = 0.1605258397758007
Training loss on iteration 1000 = 0.2001915168017149
Training loss on iteration 1020 = 0.16244118586182593
Training loss on iteration 1040 = 0.1720684751868248
Training loss on iteration 1060 = 0.19890864081680776
Training loss on iteration 1080 = 0.1687486246228218
Training loss on iteration 1100 = 0.19513492956757544
Training loss on iteration 1120 = 0.19032349139451982
Training loss on iteration 1140 = 0.1687944158911705
Training loss on iteration 1160 = 0.1677918341010809
Training loss on iteration 1180 = 0.15000460520386696
Training loss on iteration 1200 = 0.1866703648120165
Training loss on iteration 1220 = 0.1817235115915537
Training loss on iteration 1240 = 0.16400158628821374
Training loss on iteration 1260 = 0.16195737719535827
Training loss on iteration 1280 = 0.1784230139106512
Training loss on iteration 1300 = 0.1825358632951975
Training loss on iteration 1320 = 0.17147970646619798
Training loss on iteration 1340 = 0.1994830656796694
Training loss on iteration 1360 = 0.1635027050971985
Training loss on iteration 1380 = 0.18899738863110543
Training loss on iteration 1400 = 0.17148887142539024
Training loss on iteration 1420 = 0.17484218142926694
Training loss on iteration 1440 = 0.15589332170784473
Training loss on iteration 1460 = 0.16656908802688122
Training loss on iteration 1480 = 0.17718344256281854
Training loss on iteration 1500 = 0.1753815770149231
Training loss on iteration 1520 = 0.21404853090643883
Training loss on iteration 1540 = 0.17171266824007034
Training loss on iteration 1560 = 0.1938936211168766
Training loss on iteration 1580 = 0.17655263990163803
Training loss on iteration 1600 = 0.2002622216939926
Training loss on iteration 1620 = 0.19938544370234013
Training loss on iteration 1640 = 0.2012978471815586
Training loss on iteration 1660 = 0.20276079848408698
Training loss on iteration 1680 = 0.17353910021483898
Training loss on iteration 1700 = 0.19050862975418567
Training loss on iteration 1720 = 0.1648464284837246
Training loss on iteration 1740 = 0.17147850878536702
Training loss on iteration 1760 = 0.1762770339846611
Training loss on iteration 1780 = 0.18489206358790397
Training loss on iteration 1800 = 0.17013991959393024
Training loss on iteration 1820 = 0.1782499633729458
Training loss on iteration 1840 = 0.2085067953914404
Training loss on iteration 1860 = 0.18665605857968331
Training loss on iteration 1880 = 0.18087076470255853
Training loss on iteration 1900 = 0.17872076705098153
Training loss on iteration 1920 = 0.1950765449553728
Training loss on iteration 1940 = 0.15570846982300282
Training loss on iteration 1960 = 0.18367025181651114
Training loss on iteration 1980 = 0.17960537187755107
Training loss on iteration 2000 = 0.21385094411671163
Training loss on iteration 2020 = 0.1732599761337042
Training loss on iteration 2040 = 0.1749010007828474
Training loss on iteration 2060 = 0.16440694592893124
Training loss on iteration 2080 = 0.20903199091553687
Training loss on iteration 2100 = 0.19341735932976006
Training loss on iteration 2120 = 0.19203615337610244
Training loss on iteration 2140 = 0.18654383793473245
Training loss on iteration 2160 = 0.1787464063614607
Training loss on iteration 2180 = 0.19098011329770087
Training loss on iteration 2200 = 0.19856259264051915
Training loss on iteration 2220 = 0.1841173842549324
Training loss on iteration 2240 = 0.20215725600719453
Training loss on iteration 2260 = 0.1959357738494873
Training loss on iteration 2280 = 0.19912276268005372
Training loss on iteration 2300 = 0.1958891458809376
Training loss on iteration 2320 = 0.19984779730439187
Training loss on iteration 2340 = 0.17558874227106572
Training loss on iteration 2360 = 0.1780151080340147
Training loss on iteration 2380 = 0.1803928479552269
Training loss on iteration 2400 = 0.17651146985590457
Training loss on iteration 2420 = 0.19548347555100917
Training loss on iteration 2440 = 0.18358295038342476
Training loss on iteration 2460 = 0.18427460826933384
Training loss on iteration 2480 = 0.2947364814579487
Training loss on iteration 2500 = 0.19676830284297467
Training loss on iteration 2520 = 0.19816369079053403
Training loss on iteration 2540 = 0.19185178093612193
Training loss on iteration 2560 = 0.21939715817570687
Training loss on iteration 2580 = 0.19384801909327506
Training loss on iteration 2600 = 0.2259959664195776
Training loss on iteration 2620 = 0.20716650784015656
Training loss on iteration 2640 = 0.18731220737099646
Training loss on iteration 2660 = 0.24104814529418944
Training loss on iteration 2680 = 0.19573632702231408
Training loss on iteration 2700 = 0.19175395034253598
Training loss on iteration 2720 = 0.19548275098204612
Training loss on iteration 2740 = 0.1748407047241926
Training loss on iteration 2760 = 0.18196099922060965
Training loss on iteration 2780 = 0.21582638509571553
Training loss on iteration 2800 = 0.1833173654973507
Training loss on iteration 2820 = 0.17430691942572593
Training loss on iteration 2840 = 0.19041751697659492
Training loss on iteration 2860 = 0.19168839789927006
Training loss on iteration 2880 = 0.18741619996726513
Training loss on iteration 2900 = 0.17674282416701317
Training loss on iteration 2920 = 0.19295558258891105
Training loss on iteration 2940 = 0.18621490485966205
Training loss on iteration 2960 = 0.2256995853036642
Training loss on iteration 2980 = 0.18907618373632432
Training loss on iteration 3000 = 0.36035006754100324
Training loss on iteration 3020 = 0.18942231833934783
Training loss on iteration 3040 = 0.1938916936516762
Training loss on iteration 3060 = 0.1840278536081314
Training loss on iteration 3080 = 0.1857441309839487
Training loss on iteration 3100 = 0.19153343737125397
Training loss on iteration 3120 = 0.1808673445135355
Training loss on iteration 3140 = 0.16922964230179788
Training loss on iteration 3160 = 0.17812136188149452
Training loss on iteration 3180 = 0.20841646939516068
Training loss on iteration 3200 = 0.1958971131592989
Training loss on iteration 3220 = 0.1776143193244934
Training loss on iteration 3240 = 0.17643763460218906
Training loss on iteration 3260 = 0.1675674431025982
Training loss on iteration 3280 = 0.18726465068757533
Training loss on iteration 3300 = 0.1569486502557993
Training loss on iteration 3320 = 0.2683128207921982
Training loss on iteration 3340 = 0.19274854473769665
Training loss on iteration 3360 = 0.1770758204162121
Training loss on iteration 3380 = 0.17392958477139472
Training loss on iteration 3400 = 0.17043109871447087
Training loss on iteration 3420 = 0.16553012654185295
Training loss on iteration 3440 = 0.20527030229568483
Training loss on iteration 3460 = 0.17943670675158502
Training loss on iteration 3480 = 0.20529889389872552
Training loss on iteration 3500 = 0.19262292385101318
Training loss on iteration 3520 = 0.1975226230919361
Training loss on iteration 3540 = 0.17504449486732482
Training loss on iteration 3560 = 0.2053255781531334
Training loss on iteration 3580 = 0.19094539545476436
Training loss on iteration 3600 = 0.21960579678416253
Training loss on iteration 3620 = 0.19045894034206867
Training loss on iteration 3640 = 0.19904640018939973
Training loss on iteration 3660 = 0.17518633231520653
Training loss on iteration 3680 = 0.1873032208532095
Training loss on iteration 3700 = 0.19087558388710021
Training loss on iteration 3720 = 0.20252746120095252
Training loss on iteration 3740 = 0.1978947300463915
Training loss on iteration 3760 = 0.2139239739626646
Training loss on iteration 3780 = 0.16726435348391533
Training loss on iteration 3800 = 0.16092003434896468
Training loss on iteration 3820 = 0.19243701808154584
Training loss on iteration 3840 = 0.18187788017094136
Training loss on iteration 3860 = 0.20130472294986249
Training loss on iteration 3880 = 0.18648246265947818
Training loss on iteration 3900 = 0.1779827419668436
Training loss on iteration 3920 = 0.18281041979789733
Training loss on iteration 3940 = 0.18376533575356008
Training loss on iteration 3960 = 0.16964389830827714
Training loss on iteration 3980 = 0.17836060114204882
Training loss on iteration 4000 = 0.1890134010463953
Training loss on iteration 4020 = 0.16420261524617671
Training loss on iteration 4040 = 0.16276715472340583
Training loss on iteration 4060 = 0.19954928308725356
Training loss on iteration 4080 = 0.22225708551704884
Training loss on iteration 4100 = 0.20572462268173694
Training loss on iteration 4120 = 0.21138205863535403
Training loss on iteration 4140 = 0.1911539513617754
Training loss on iteration 4160 = 0.1738663215190172
Training loss on iteration 4180 = 0.19267142079770566
Training loss on iteration 4200 = 0.16962288841605186
Training loss on iteration 4220 = 0.1676787804812193
Training loss on iteration 4240 = 0.17607946023344995
Training loss on iteration 4260 = 0.2095719076693058
Training loss on iteration 4280 = 0.20416971780359744
Training loss on iteration 4300 = 0.19922877177596093
Training loss on iteration 4320 = 0.2232234872877598
Training loss on iteration 4340 = 0.1784960925579071
Training loss on iteration 4360 = 0.19579686112701894
Training loss on iteration 4380 = 0.17808537185192108
Training loss on iteration 4400 = 0.17459949739277364
Training loss on iteration 4420 = 0.1954838003963232
Training loss on iteration 4440 = 0.17556839063763618
Training loss on iteration 4460 = 0.26828224398195744
Training loss on iteration 4480 = 0.16558749563992023
Training loss on iteration 4500 = 0.17664721384644508
Training loss on iteration 4520 = 0.18279637917876243
Training loss on iteration 4540 = 0.17414179034531116
Training loss on iteration 4560 = 0.1887143947184086
Training loss on iteration 4580 = 0.20697387233376502
Training loss on iteration 4600 = 0.19452095329761504
Training loss on iteration 4620 = 0.19177932143211365
Training loss on iteration 4640 = 0.18800238743424416
Training loss on iteration 4660 = 0.1991764672100544
Training loss on iteration 4680 = 0.18690528050065042
Training loss on iteration 4700 = 0.1998319812119007
Training loss on iteration 4720 = 0.16638944000005723
Training loss on iteration 4740 = 0.1802649050951004
Training loss on iteration 4760 = 0.18134470619261264
Training loss on iteration 4780 = 0.18671401850879193
Training loss on iteration 4800 = 0.17600694671273232
Training loss on iteration 4820 = 0.17569079920649527
Training loss on iteration 4840 = 0.1950624778866768
Training loss on iteration 4860 = 0.2006076082587242
Training loss on iteration 4880 = 0.174838400632143
Training loss on iteration 4900 = 0.1859705824404955
Training loss on iteration 4920 = 0.20782511457800865
Training loss on iteration 4940 = 0.2126793995499611
Training loss on iteration 4960 = 0.20094166025519372
Training loss on iteration 4980 = 0.19650190211832524
Training loss on iteration 5000 = 0.180525753647089
Training loss on iteration 5020 = 0.16594794802367688
Training loss on iteration 5040 = 0.208081092312932
Training loss on iteration 5060 = 0.21757099889218806
Training loss on iteration 5080 = 0.19028350785374643
Training loss on iteration 5100 = 0.18466852307319642
Training loss on iteration 5120 = 0.20239346511662007
Training loss on iteration 5140 = 0.19835241436958312
Training loss on iteration 5160 = 0.19197399728000164
Training loss on iteration 5180 = 0.2043506760150194
Training loss on iteration 5200 = 0.18104499727487564
Training loss on iteration 5220 = 0.23839461989700794
Training loss on iteration 5240 = 0.1934783160686493
Training loss on iteration 5260 = 0.20609241351485252
Training loss on iteration 5280 = 0.2061931997537613
Training loss on iteration 5300 = 0.1799838226288557
Training loss on iteration 5320 = 0.18363868556916713
Training loss on iteration 5340 = 0.17940779216587543
Training loss on iteration 5360 = 0.22644434943795205
Training loss on iteration 5380 = 0.16753311045467853
Training loss on iteration 5400 = 0.354709692671895
Training loss on iteration 5420 = 0.20944391191005707
Training loss on iteration 5440 = 0.19583081938326358
Training loss on iteration 5460 = 0.18145545460283757
Training loss on iteration 5480 = 0.18531019166111945
Training loss on iteration 5500 = 0.2085491217672825
Training loss on iteration 5520 = 0.19239360727369786
Training loss on iteration 5540 = 0.1982469480484724
Training loss on iteration 5560 = 0.2477620765566826
Training loss on iteration 5580 = 0.21081866063177584
Training loss on iteration 5600 = 0.17729027792811394
Training loss on iteration 5620 = 0.17620795853435994
Training loss on iteration 5640 = 0.21599520035088063
Training loss on iteration 5660 = 0.2093054886907339
Training loss on iteration 5680 = 0.2274422340095043
Training loss on iteration 5700 = 0.2133395079523325
Training loss on iteration 5720 = 0.19600964710116386
Training loss on iteration 5740 = 0.1729228064417839
Training loss on iteration 5760 = 0.17234827056527138
Training loss on iteration 5780 = 0.1898696780204773
Training loss on iteration 5800 = 0.20741049572825432
Training loss on iteration 5820 = 0.1713800895959139
Training loss on iteration 5840 = 0.18759304136037827
Training loss on iteration 5860 = 0.18541461005806922
Training loss on iteration 5880 = 0.18683403730392456
Training loss on iteration 5900 = 0.19302144981920719
Training loss on iteration 5920 = 0.17119463980197908
Training loss on iteration 5940 = 0.19008909203112126
Training loss on iteration 5960 = 0.19948285147547723
Training loss on iteration 5980 = 0.1907485779374838
Training loss on iteration 6000 = 0.1951337933540344
Training loss on iteration 6020 = 0.19052202440798283
Training loss on iteration 6040 = 0.1774197220802307
Training loss on iteration 6060 = 0.19644298888742923
Training loss on iteration 6080 = 0.17775441631674765
Training loss on iteration 6100 = 0.18502888604998588
Training loss on iteration 6120 = 0.19742111824452876
Training loss on iteration 6140 = 0.18936998806893826
Training loss on iteration 6160 = 0.21555981524288653
Training loss on iteration 6180 = 0.1702125784009695
Training loss on iteration 6200 = 0.21310711652040482
Training loss on iteration 6220 = 0.2163609739392996
Training loss on iteration 6240 = 0.21136521697044372
Training loss on iteration 6260 = 0.18192424513399602
Training loss on iteration 6280 = 0.1915544457733631
Training loss on iteration 6300 = 0.19659891314804553
Training loss on iteration 6320 = 0.1934297051280737
Training loss on iteration 6340 = 0.1953356582671404
Training loss on iteration 6360 = 0.17774608135223388
Training loss on iteration 6380 = 0.1634310457855463
Training loss on iteration 0 = 0.11115290224552155
Training loss on iteration 20 = 0.1634724974632263
Training loss on iteration 40 = 0.17177742570638657
Training loss on iteration 60 = 0.17267850190401077
Training loss on iteration 80 = 0.3409024890512228
Training loss on iteration 100 = 0.15676094368100166
Training loss on iteration 120 = 0.14435917809605597
Training loss on iteration 140 = 0.15155140943825246
Training loss on iteration 160 = 0.18572988994419576
Training loss on iteration 180 = 0.1584922794252634
Training loss on iteration 200 = 0.1606082946062088
Training loss on iteration 220 = 0.21186359114944936
Training loss on iteration 240 = 0.18231738358736038
Training loss on iteration 260 = 0.1688133418560028
Training loss on iteration 280 = 0.14453635215759278
Training loss on iteration 300 = 0.17017692252993583
Training loss on iteration 320 = 0.15296979285776616
Training loss on iteration 340 = 0.16004173085093498
Training loss on iteration 360 = 0.15827365294098855
Training loss on iteration 380 = 0.15190801545977592
Training loss on iteration 400 = 0.18645197339355946
Training loss on iteration 420 = 0.15104443505406379
Training loss on iteration 440 = 0.17768582068383693
Training loss on iteration 460 = 0.15988799817860128
Training loss on iteration 480 = 0.17883241549134254
Training loss on iteration 500 = 0.16196835413575172
Training loss on iteration 520 = 0.1703237522393465
Training loss on iteration 540 = 0.16577188484370708
Training loss on iteration 560 = 0.15759116001427173
Training loss on iteration 580 = 0.17634340301156043
Training loss on iteration 600 = 0.17160708233714103
Training loss on iteration 620 = 0.18429370336234568
Training loss on iteration 640 = 0.18794913478195668
Training loss on iteration 660 = 0.14916977994143962
Training loss on iteration 680 = 0.1743142545223236
Training loss on iteration 700 = 0.17008818611502646
Training loss on iteration 720 = 0.1736754935234785
Training loss on iteration 740 = 0.1461344722658396
Training loss on iteration 760 = 0.15209907442331314
Training loss on iteration 780 = 0.16591798402369023
Training loss on iteration 800 = 0.15725087001919746
Training loss on iteration 820 = 0.16201553605496882
Training loss on iteration 840 = 0.18643410839140415
Training loss on iteration 860 = 0.18339416421949864
Training loss on iteration 880 = 0.17397893071174622
Training loss on iteration 900 = 0.1756561454385519
Training loss on iteration 920 = 0.17345364019274712
Training loss on iteration 940 = 0.18951814025640487
Training loss on iteration 960 = 0.18036956787109376
Training loss on iteration 980 = 0.15134514272212982
Training loss on iteration 1000 = 0.17971125543117522
Training loss on iteration 1020 = 0.16676490493118762
Training loss on iteration 1040 = 0.1951558906584978
Training loss on iteration 1060 = 0.15124739222228528
Training loss on iteration 1080 = 0.1435965422540903
Training loss on iteration 1100 = 0.16550254821777344
Training loss on iteration 1120 = 0.33898420445621014
Training loss on iteration 1140 = 0.19419415667653084
Training loss on iteration 1160 = 0.204717006534338
Training loss on iteration 1180 = 0.1686452865600586
Training loss on iteration 1200 = 0.16580182164907456
Training loss on iteration 1220 = 0.18432683423161506
Training loss on iteration 1240 = 0.15766570791602136
Training loss on iteration 1260 = 0.1699858669191599
Training loss on iteration 1280 = 0.17555754967033863
Training loss on iteration 1300 = 0.16885164007544518
Training loss on iteration 1320 = 0.1694782555103302
Training loss on iteration 1340 = 0.16474830023944378
Training loss on iteration 1360 = 0.2309205137193203
Training loss on iteration 1380 = 0.17700553089380264
Training loss on iteration 1400 = 0.17122702226042746
Training loss on iteration 1420 = 0.16937944255769252
Training loss on iteration 1440 = 0.17406633719801903
Training loss on iteration 1460 = 0.16968382187187672
Training loss on iteration 1480 = 0.18899582736194134
Training loss on iteration 1500 = 0.17228368073701858
Training loss on iteration 1520 = 0.19221045076847076
Training loss on iteration 1540 = 0.15961387678980826
Training loss on iteration 1560 = 0.17567191123962403
Training loss on iteration 1580 = 0.17358087748289108
Training loss on iteration 1600 = 0.1668899480253458
Training loss on iteration 1620 = 0.17295681573450566
Training loss on iteration 1640 = 0.16156985871493817
Training loss on iteration 1660 = 0.1793571088463068
Training loss on iteration 1680 = 0.1709327396005392
Training loss on iteration 1700 = 0.2066905193030834
Training loss on iteration 1720 = 0.18276679180562497
Training loss on iteration 1740 = 0.17720657736063003
Training loss on iteration 1760 = 0.2029842659831047
Training loss on iteration 1780 = 0.1999675638973713
Training loss on iteration 1800 = 0.169683688133955
Training loss on iteration 1820 = 0.19542686715722085
Training loss on iteration 1840 = 0.1761892806738615
Training loss on iteration 1860 = 0.1658602014183998
Training loss on iteration 1880 = 0.15505981408059596
Training loss on iteration 1900 = 0.1757565602660179
Training loss on iteration 1920 = 0.18452187292277814
Training loss on iteration 1940 = 0.16576983593404293
Training loss on iteration 1960 = 0.1658386029303074
Training loss on iteration 1980 = 0.18033234514296054
Training loss on iteration 2000 = 0.1882086094468832
Training loss on iteration 2020 = 0.2071252852678299
Training loss on iteration 2040 = 0.15887390300631524
Training loss on iteration 2060 = 0.16953981593251227
Training loss on iteration 2080 = 0.20069056041538716
Training loss on iteration 2100 = 0.18280568048357965
Training loss on iteration 2120 = 0.19341943748295307
Training loss on iteration 2140 = 0.21685609221458435
Training loss on iteration 2160 = 0.15471625477075576
Training loss on iteration 2180 = 0.17406728193163873
Training loss on iteration 2200 = 0.21414814554154873
Training loss on iteration 2220 = 0.15610353499650956
Training loss on iteration 2240 = 0.16279473155736923
Training loss on iteration 2260 = 0.1745046678930521
Training loss on iteration 2280 = 0.19277545772492885
Training loss on iteration 2300 = 0.20408407412469387
Training loss on iteration 2320 = 0.17047946527600288
Training loss on iteration 2340 = 0.1669762570410967
Training loss on iteration 2360 = 0.18437563590705394
Training loss on iteration 2380 = 0.16067726612091066
Training loss on iteration 2400 = 0.17871522083878516
Training loss on iteration 2420 = 0.16919177696108817
Training loss on iteration 2440 = 0.16381713673472403
Training loss on iteration 2460 = 0.16024017482995986
Training loss on iteration 2480 = 0.17293476797640323
Training loss on iteration 2500 = 0.1863541528582573
Training loss on iteration 2520 = 0.1937211614102125
Training loss on iteration 2540 = 0.1898685798048973
Training loss on iteration 2560 = 0.17227596789598465
Training loss on iteration 2580 = 0.16396760605275632
Training loss on iteration 2600 = 0.19091982766985893
Training loss on iteration 2620 = 0.17101738303899766
Training loss on iteration 2640 = 0.1792938306927681
Training loss on iteration 2660 = 0.2027134370058775
Training loss on iteration 2680 = 0.18010258711874486
Training loss on iteration 2700 = 0.2101667620241642
Training loss on iteration 2720 = 0.16890802569687366
Training loss on iteration 2740 = 0.2027382880449295
Training loss on iteration 2760 = 0.17517784982919693
Training loss on iteration 2780 = 0.18569456674158574
Training loss on iteration 2800 = 0.1767982069402933
Training loss on iteration 2820 = 0.18796890489757062
Training loss on iteration 2840 = 0.19671958759427072
Training loss on iteration 2860 = 0.17453345768153666
Training loss on iteration 2880 = 0.1815018966794014
Training loss on iteration 2900 = 0.18956475034356118
Training loss on iteration 2920 = 0.18076356090605258
Training loss on iteration 2940 = 0.18781111538410186
Training loss on iteration 2960 = 0.1775252629071474
Training loss on iteration 2980 = 0.18596805781126022
Training loss on iteration 3000 = 0.1637779947370291
Training loss on iteration 3020 = 0.19704741202294826
Training loss on iteration 3040 = 0.20116205066442489
Training loss on iteration 3060 = 0.18648174703121184
Training loss on iteration 3080 = 0.16538314707577229
Training loss on iteration 3100 = 0.15163640156388283
Training loss on iteration 3120 = 0.17709743790328503
Training loss on iteration 3140 = 0.18535538129508494
Training loss on iteration 3160 = 0.19164439886808396
Training loss on iteration 3180 = 0.16772392392158508
Training loss on iteration 3200 = 0.19283263832330705
Training loss on iteration 3220 = 0.19432530663907527
Training loss on iteration 3240 = 0.185900766402483
Training loss on iteration 3260 = 0.17318631410598756
Training loss on iteration 3280 = 0.1532560519874096
Training loss on iteration 3300 = 0.18808615393936634
Training loss on iteration 3320 = 0.19271284230053426
Training loss on iteration 3340 = 0.1762055080384016
Training loss on iteration 3360 = 0.16590491458773612
Training loss on iteration 3380 = 0.16627958714962005
Training loss on iteration 3400 = 0.1681607339531183
Training loss on iteration 3420 = 0.22118011750280858
Training loss on iteration 3440 = 0.19536588788032533
Training loss on iteration 3460 = 0.18359401933848857
Training loss on iteration 3480 = 0.16321029365062714
Training loss on iteration 3500 = 0.2133132167160511
Training loss on iteration 3520 = 0.17787932008504867
Training loss on iteration 3540 = 0.18117255978286267
Training loss on iteration 3560 = 0.19364517070353032
Training loss on iteration 3580 = 0.1852719619870186
Training loss on iteration 3600 = 0.18222161792218686
Training loss on iteration 3620 = 0.1666139055043459
Training loss on iteration 3640 = 0.17413591966032982
Training loss on iteration 3660 = 0.1693353485316038
Training loss on iteration 3680 = 0.1655415941029787
Training loss on iteration 3700 = 0.20029603317379951
Training loss on iteration 3720 = 0.16785654239356518
Training loss on iteration 3740 = 0.23335447385907174
Training loss on iteration 3760 = 0.19156994745135308
Training loss on iteration 3780 = 0.17729074284434318
Training loss on iteration 3800 = 0.17139227502048016
Training loss on iteration 3820 = 0.1768151342868805
Training loss on iteration 3840 = 0.15937586687505245
Training loss on iteration 3860 = 0.1802425667643547
Training loss on iteration 3880 = 0.16025243438780307
Training loss on iteration 3900 = 0.20759829953312875
Training loss on iteration 3920 = 0.17388850674033166
Training loss on iteration 3940 = 0.15639140084385872
Training loss on iteration 3960 = 0.24052187316119672
Training loss on iteration 3980 = 0.16414063721895217
Training loss on iteration 4000 = 0.16778654530644416
Training loss on iteration 4020 = 0.18799561485648156
Training loss on iteration 4040 = 0.17973140068352222
Training loss on iteration 4060 = 0.2078241802752018
Training loss on iteration 4080 = 0.15545645207166672
Training loss on iteration 4100 = 0.15741901621222495
Training loss on iteration 4120 = 0.14867992848157882
Training loss on iteration 4140 = 0.1669222455471754
Training loss on iteration 4160 = 0.1928712621331215
Training loss on iteration 4180 = 0.17361345179378987
Training loss on iteration 4200 = 0.1646121945232153
Training loss on iteration 4220 = 0.19642458185553552
Training loss on iteration 4240 = 0.17633578330278396
Training loss on iteration 4260 = 0.21078912615776063
Training loss on iteration 4280 = 0.24190737903118134
Training loss on iteration 4300 = 0.17601727433502673
Training loss on iteration 4320 = 0.17629556320607662
Training loss on iteration 4340 = 0.18505801893770696
Training loss on iteration 4360 = 0.17936049103736879
Training loss on iteration 4380 = 0.20089793391525745
Training loss on iteration 4400 = 0.18597698584198952
Training loss on iteration 4420 = 0.18030915632843972
Training loss on iteration 4440 = 0.18311901725828647
Training loss on iteration 4460 = 0.1518461372703314
Training loss on iteration 4480 = 0.18269324265420436
Training loss on iteration 4500 = 0.16404090113937855
Training loss on iteration 4520 = 0.17997639253735542
Training loss on iteration 4540 = 0.18656408414244652
Training loss on iteration 4560 = 0.1971242420375347
Training loss on iteration 4580 = 0.21231098249554634
Training loss on iteration 4600 = 0.1832966711372137
Training loss on iteration 4620 = 0.186030001565814
Training loss on iteration 4640 = 0.1722254104912281
Training loss on iteration 4660 = 0.18877550587058067
Training loss on iteration 4680 = 0.18389954492449762
Training loss on iteration 4700 = 0.1615403786301613
Training loss on iteration 4720 = 0.16981503628194333
Training loss on iteration 4740 = 0.19010542333126068
Training loss on iteration 4760 = 0.1757772143930197
Training loss on iteration 4780 = 0.1736639391630888
Training loss on iteration 4800 = 0.17309580706059932
Training loss on iteration 4820 = 0.16626564487814904
Training loss on iteration 4840 = 0.2077658262103796
Training loss on iteration 4860 = 0.18533470258116722
Training loss on iteration 4880 = 0.16276027522981168
Training loss on iteration 4900 = 0.1752047460526228
Training loss on iteration 4920 = 0.1841924987733364
Training loss on iteration 4940 = 0.19456359781324864
Training loss on iteration 4960 = 0.16815138831734658
Training loss on iteration 4980 = 0.17872547395527363
Training loss on iteration 5000 = 0.16973042935132981
Training loss on iteration 5020 = 0.19308818317949772
Training loss on iteration 5040 = 0.1983904905617237
Training loss on iteration 5060 = 0.17243728786706924
Training loss on iteration 5080 = 0.18611049018800258
Training loss on iteration 5100 = 0.25811494775116445
Training loss on iteration 5120 = 0.18886259868741034
Training loss on iteration 5140 = 0.16751008592545985
Training loss on iteration 5160 = 0.23873838856816293
Training loss on iteration 5180 = 0.18258964493870736
Training loss on iteration 5200 = 0.1978123176842928
Training loss on iteration 5220 = 0.2011196341365576
Training loss on iteration 5240 = 0.18492933101952075
Training loss on iteration 5260 = 0.17110087238252164
Training loss on iteration 5280 = 0.1922096587717533
Training loss on iteration 5300 = 0.1825927309691906
Training loss on iteration 5320 = 0.16070675626397132
Training loss on iteration 5340 = 0.20507799685001374
Training loss on iteration 5360 = 0.16328902058303357
Training loss on iteration 5380 = 0.2074427206069231
Training loss on iteration 5400 = 0.18850893452763556
Training loss on iteration 5420 = 0.18752348758280277
Training loss on iteration 5440 = 0.18939577266573906
Training loss on iteration 5460 = 0.17964470945298672
Training loss on iteration 5480 = 0.16922170519828797
Training loss on iteration 5500 = 0.21807911060750484
Training loss on iteration 5520 = 0.18725034818053246
Training loss on iteration 5540 = 0.17094181329011918
Training loss on iteration 5560 = 0.18259513862431048
Training loss on iteration 5580 = 0.1803204219788313
Training loss on iteration 5600 = 0.21971249543130397
Training loss on iteration 5620 = 0.17985508851706983
Training loss on iteration 5640 = 0.1925558403134346
Training loss on iteration 5660 = 0.19295684210956096
Training loss on iteration 5680 = 0.21344683282077312
Training loss on iteration 5700 = 0.2013884846121073
Training loss on iteration 5720 = 0.1698046986013651
Training loss on iteration 5740 = 0.18154796846210958
Training loss on iteration 5760 = 0.1773702621459961
Training loss on iteration 5780 = 0.17392061203718184
Training loss on iteration 5800 = 0.16563356406986712
Training loss on iteration 5820 = 0.17631577253341674
Training loss on iteration 5840 = 0.1804928991943598
Training loss on iteration 5860 = 0.18200834132730961
Training loss on iteration 5880 = 0.19178736135363578
Training loss on iteration 5900 = 0.1939296081662178
Training loss on iteration 5920 = 0.1774387750774622
Training loss on iteration 5940 = 0.1805390615016222
Training loss on iteration 5960 = 0.19267305582761765
Training loss on iteration 5980 = 0.20332344584167003
Training loss on iteration 6000 = 0.18327292911708354
Training loss on iteration 6020 = 0.20300336815416814
Training loss on iteration 6040 = 0.18640095219016076
Training loss on iteration 6060 = 0.1814657960087061
Training loss on iteration 6080 = 0.18980484530329705
Training loss on iteration 6100 = 0.16446862146258354
Training loss on iteration 6120 = 0.19123302549123763
Training loss on iteration 6140 = 0.1861241165548563
Training loss on iteration 6160 = 0.19022509567439555
Training loss on iteration 6180 = 0.15688121765851976
Training loss on iteration 6200 = 0.19766706302762033
Training loss on iteration 6220 = 0.20666659139096738
Training loss on iteration 6240 = 0.24611265100538732
Training loss on iteration 6260 = 0.1974995717406273
Training loss on iteration 6280 = 0.1815577145665884
Training loss on iteration 6300 = 0.19933981969952583
Training loss on iteration 6320 = 0.18609922379255295
Training loss on iteration 6340 = 0.18230074271559715
Training loss on iteration 6360 = 0.1717702116817236
Training loss on iteration 6380 = 0.1706472434103489
Training loss on iteration 0 = 0.1438399851322174
Training loss on iteration 20 = 0.16260649226605892
Training loss on iteration 40 = 0.16528350226581096
Training loss on iteration 60 = 0.15192016996443272
Training loss on iteration 80 = 0.16741613894701005
Training loss on iteration 100 = 0.15084563717246055
Training loss on iteration 120 = 0.15305582173168658
Training loss on iteration 140 = 0.154750694707036
Training loss on iteration 160 = 0.1601073581725359
Training loss on iteration 180 = 0.1586351439356804
Training loss on iteration 200 = 0.16748717054724693
Training loss on iteration 220 = 0.1676028162240982
Training loss on iteration 240 = 0.15529339723289012
Training loss on iteration 260 = 0.1582847848534584
Training loss on iteration 280 = 0.15872803144156933
Training loss on iteration 300 = 0.15958461575210095
Training loss on iteration 320 = 0.17084488794207572
Training loss on iteration 340 = 0.1548899456858635
Training loss on iteration 360 = 0.15864230170845986
Training loss on iteration 380 = 0.15560573991388083
Training loss on iteration 400 = 0.15969581305980682
Training loss on iteration 420 = 0.17402899824082851
Training loss on iteration 440 = 0.1685654018074274
Training loss on iteration 460 = 0.16441235169768334
Training loss on iteration 480 = 0.17355090826749803
Training loss on iteration 500 = 0.14726966768503189
Training loss on iteration 520 = 0.16078487746417522
Training loss on iteration 540 = 0.1522515807300806
Training loss on iteration 560 = 0.14833267256617547
Training loss on iteration 580 = 0.1567518975585699
Training loss on iteration 600 = 0.15679659731686116
Training loss on iteration 620 = 0.1642089530825615
Training loss on iteration 640 = 0.16678727902472018
Training loss on iteration 660 = 0.18683452680706977
Training loss on iteration 680 = 0.1658102922141552
Training loss on iteration 700 = 0.16103109605610372
Training loss on iteration 720 = 0.16620585061609744
Training loss on iteration 740 = 0.1630576655268669
Training loss on iteration 760 = 0.1687383536249399
Training loss on iteration 780 = 0.15724684931337835
Training loss on iteration 800 = 0.16355402506887912
Training loss on iteration 820 = 0.15520767606794833
Training loss on iteration 840 = 0.18648620918393136
Training loss on iteration 860 = 0.15368146412074565
Training loss on iteration 880 = 0.17005682699382305
Training loss on iteration 900 = 0.17412365972995758
Training loss on iteration 920 = 0.14942786805331706
Training loss on iteration 940 = 0.1839522138237953
Training loss on iteration 960 = 0.19195147044956684
Training loss on iteration 980 = 0.14248113781213761
Training loss on iteration 1000 = 0.15864703767001628
Training loss on iteration 1020 = 0.1436637118458748
Training loss on iteration 1040 = 0.13689800761640072
Training loss on iteration 1060 = 0.16025627292692662
Training loss on iteration 1080 = 0.1646218378096819
Training loss on iteration 1100 = 0.15333658158779145
Training loss on iteration 1120 = 0.14729615300893784
Training loss on iteration 1140 = 0.1849585734307766
Training loss on iteration 1160 = 0.18249024003744124
Training loss on iteration 1180 = 0.15757341794669627
Training loss on iteration 1200 = 0.16745716333389282
Training loss on iteration 1220 = 0.1781411323696375
Training loss on iteration 1240 = 0.15730654783546924
Training loss on iteration 1260 = 0.1591442536562681
Training loss on iteration 1280 = 0.1872962314635515
Training loss on iteration 1300 = 0.15998515859246254
Training loss on iteration 1320 = 0.15492308735847474
Training loss on iteration 1340 = 0.16281307227909564
Training loss on iteration 1360 = 0.15806124545633793
Training loss on iteration 1380 = 0.16465815342962742
Training loss on iteration 1400 = 0.1747374903410673
Training loss on iteration 1420 = 0.1522195369005203
Training loss on iteration 1440 = 0.1602350424975157
Training loss on iteration 1460 = 0.17162023968994616
Training loss on iteration 1480 = 0.1832290083169937
Training loss on iteration 1500 = 0.17098692655563355
Training loss on iteration 1520 = 0.1512218251824379
Training loss on iteration 1540 = 0.16566131636500359
Training loss on iteration 1560 = 0.2040185485035181
Training loss on iteration 1580 = 0.1769996952265501
Training loss on iteration 1600 = 0.17092208117246627
Training loss on iteration 1620 = 0.17683264277875424
Training loss on iteration 1640 = 0.17263525910675526
Training loss on iteration 1660 = 0.15995571874082087
Training loss on iteration 1680 = 0.1391963627189398
Training loss on iteration 1700 = 0.14859985634684564
Training loss on iteration 1720 = 0.15865231715142727
Training loss on iteration 1740 = 0.17678572051227093
Training loss on iteration 1760 = 0.16346505135297776
Training loss on iteration 1780 = 0.14816925078630447
Training loss on iteration 1800 = 0.1759097281843424
Training loss on iteration 1820 = 0.14933967217803001
Training loss on iteration 1840 = 0.16766167879104615
Training loss on iteration 1860 = 0.17232090830802918
Training loss on iteration 1880 = 0.17134111523628234
Training loss on iteration 1900 = 0.1825799774378538
Training loss on iteration 1920 = 0.16889878883957862
Training loss on iteration 1940 = 0.1510595079511404
Training loss on iteration 1960 = 0.2032521814107895
Training loss on iteration 1980 = 0.1762211825698614
Training loss on iteration 2000 = 0.16472261399030685
Training loss on iteration 2020 = 0.18350844867527485
Training loss on iteration 2040 = 0.14590443186461927
Training loss on iteration 2060 = 0.18880798816680908
Training loss on iteration 2080 = 0.18137641102075577
Training loss on iteration 2100 = 0.21179016046226024
Training loss on iteration 2120 = 0.16530770771205425
Training loss on iteration 2140 = 0.18186776228249074
Training loss on iteration 2160 = 0.16345400363206863
Training loss on iteration 2180 = 0.17774270959198474
Training loss on iteration 2200 = 0.17933544218540193
Training loss on iteration 2220 = 0.23061907477676868
Training loss on iteration 2240 = 0.1716626439243555
Training loss on iteration 2260 = 0.16550724171102046
Training loss on iteration 2280 = 0.17313823588192462
Training loss on iteration 2300 = 0.15867478623986245
Training loss on iteration 2320 = 0.17030527740716933
Training loss on iteration 2340 = 0.15935154706239701
Training loss on iteration 2360 = 0.19406426884233952
Training loss on iteration 2380 = 0.15938478969037534
Training loss on iteration 2400 = 0.14727711342275143
Training loss on iteration 2420 = 0.17257348708808423
Training loss on iteration 2440 = 0.1668595802038908
Training loss on iteration 2460 = 0.1865482371300459
Training loss on iteration 2480 = 0.19451190009713173
Training loss on iteration 2500 = 0.15150354839861394
Training loss on iteration 2520 = 0.17331406846642494
Training loss on iteration 2540 = 0.16245035007596015
Training loss on iteration 2560 = 0.1786131363362074
Training loss on iteration 2580 = 0.160808078199625
Training loss on iteration 2600 = 0.20212237164378166
Training loss on iteration 2620 = 0.22683776393532754
Training loss on iteration 2640 = 0.18720281198620797
Training loss on iteration 2660 = 0.16731819994747638
Training loss on iteration 2680 = 0.16678927578032016
Training loss on iteration 2700 = 0.15816169641911984
Training loss on iteration 2720 = 0.19841569662094116
Training loss on iteration 2740 = 0.16445898078382015
Training loss on iteration 2760 = 0.15688923597335816
Training loss on iteration 2780 = 0.1522646602243185
Training loss on iteration 2800 = 0.17752771079540253
Training loss on iteration 2820 = 0.16759610250592233
Training loss on iteration 2840 = 0.1789895102381706
Training loss on iteration 2860 = 0.20471460297703742
Training loss on iteration 2880 = 0.19301649332046508
Training loss on iteration 2900 = 0.21202488839626313
Training loss on iteration 2920 = 0.19254962243139745
Training loss on iteration 2940 = 0.17509812042117118
Training loss on iteration 2960 = 0.16773229502141476
Training loss on iteration 2980 = 0.16768143363296986
Training loss on iteration 3000 = 0.16359681487083436
Training loss on iteration 3020 = 0.16876067221164703
Training loss on iteration 3040 = 0.16236239485442638
Training loss on iteration 3060 = 0.17364001758396624
Training loss on iteration 3080 = 0.16954270973801613
Training loss on iteration 3100 = 0.19128068201243878
Training loss on iteration 3120 = 0.20286014974117278
Training loss on iteration 3140 = 0.1686508532613516
Training loss on iteration 3160 = 0.1629977010190487
Training loss on iteration 3180 = 0.19381774105131627
Training loss on iteration 3200 = 0.16692504808306693
Training loss on iteration 3220 = 0.17548362500965595
Training loss on iteration 3240 = 0.1665277935564518
Training loss on iteration 3260 = 0.18587977811694145
Training loss on iteration 3280 = 0.1680486161261797
Training loss on iteration 3300 = 0.15435277707874775
Training loss on iteration 3320 = 0.18866378143429757
Training loss on iteration 3340 = 0.16016611084342003
Training loss on iteration 3360 = 0.16452354080975057
Training loss on iteration 3380 = 0.18059214875102042
Training loss on iteration 3400 = 0.1853744439780712
Training loss on iteration 3420 = 0.15373854674398899
Training loss on iteration 3440 = 0.16735571771860122
Training loss on iteration 3460 = 0.1798542935401201
Training loss on iteration 3480 = 0.1749417431652546
Training loss on iteration 3500 = 0.18977913819253445
Training loss on iteration 3520 = 0.17244249992072583
Training loss on iteration 3540 = 0.15733800418674945
Training loss on iteration 3560 = 0.1708439152687788
Training loss on iteration 3580 = 0.17249489836394788
Training loss on iteration 3600 = 0.1723596528172493
Training loss on iteration 3620 = 0.196942238509655
Training loss on iteration 3640 = 0.1753292366862297
Training loss on iteration 3660 = 0.18511286191642284
Training loss on iteration 3680 = 0.17285857833921908
Training loss on iteration 3700 = 0.18661454319953918
Training loss on iteration 3720 = 0.1704069759696722
Training loss on iteration 3740 = 0.19090428501367568
Training loss on iteration 3760 = 0.1729475785046816
Training loss on iteration 3780 = 0.1763748127967119
Training loss on iteration 3800 = 0.19138117991387843
Training loss on iteration 3820 = 0.16619535759091378
Training loss on iteration 3840 = 0.17946699410676956
Training loss on iteration 3860 = 0.17524957172572614
Training loss on iteration 3880 = 0.15256198309361935
Training loss on iteration 3900 = 0.18507183529436588
Training loss on iteration 3920 = 0.17051901519298554
Training loss on iteration 3940 = 0.1576521024107933
Training loss on iteration 3960 = 0.18061148151755332
Training loss on iteration 3980 = 0.16755415201187135
Training loss on iteration 4000 = 0.15336947776377202
Training loss on iteration 4020 = 0.16761147044599056
Training loss on iteration 4040 = 0.18976218067109585
Training loss on iteration 4060 = 0.17913896329700946
Training loss on iteration 4080 = 0.1922186579555273
Training loss on iteration 4100 = 0.15947811529040337
Training loss on iteration 4120 = 0.17063064835965633
Training loss on iteration 4140 = 0.18382078297436238
Training loss on iteration 4160 = 0.18265940733253955
Training loss on iteration 4180 = 0.17692476622760295
Training loss on iteration 4200 = 0.16722221709787846
Training loss on iteration 4220 = 0.15880613625049592
Training loss on iteration 4240 = 0.17681531943380832
Training loss on iteration 4260 = 0.1768243208527565
Training loss on iteration 4280 = 0.19886048436164855
Training loss on iteration 4300 = 0.16481241807341576
Training loss on iteration 4320 = 0.16385537162423133
Training loss on iteration 4340 = 0.17211006246507168
Training loss on iteration 4360 = 0.16891300678253174
Training loss on iteration 4380 = 0.1716890137642622
Training loss on iteration 4400 = 0.14989510737359524
Training loss on iteration 4420 = 0.1620878417044878
Training loss on iteration 4440 = 0.15945254527032376
Training loss on iteration 4460 = 0.15551320537924768
Training loss on iteration 4480 = 0.15368842110037803
Training loss on iteration 4500 = 0.1598910953849554
Training loss on iteration 4520 = 0.1638920821249485
Training loss on iteration 4540 = 0.16033803373575212
Training loss on iteration 4560 = 0.17026293352246286
Training loss on iteration 4580 = 0.16441426947712898
Training loss on iteration 4600 = 0.16293032467365265
Training loss on iteration 4620 = 0.1679486319422722
Training loss on iteration 4640 = 0.1741736765950918
Training loss on iteration 4660 = 0.1737638559192419
Training loss on iteration 4680 = 0.16280114278197289
Training loss on iteration 4700 = 0.17101788856089115
Training loss on iteration 4720 = 0.2546414002776146
Training loss on iteration 4740 = 0.18477322086691855
Training loss on iteration 4760 = 0.17182356603443621
Training loss on iteration 4780 = 0.16380872689187526
Training loss on iteration 4800 = 0.15835728049278258
Training loss on iteration 4820 = 0.1868965808302164
Training loss on iteration 4840 = 0.25301152989268305
Training loss on iteration 4860 = 0.20056657269597053
Training loss on iteration 4880 = 0.2033661086112261
Training loss on iteration 4900 = 0.18348054066300393
Training loss on iteration 4920 = 0.1720853377133608
Training loss on iteration 4940 = 0.1739498969167471
Training loss on iteration 4960 = 0.15329358391463757
Training loss on iteration 4980 = 0.18505536317825316
Training loss on iteration 5000 = 0.16243975907564162
Training loss on iteration 5020 = 0.19242102913558484
Training loss on iteration 5040 = 0.17551281191408635
Training loss on iteration 5060 = 0.17760977745056153
Training loss on iteration 5080 = 0.16953122578561305
Training loss on iteration 5100 = 0.19261378422379494
Training loss on iteration 5120 = 0.1647298477590084
Training loss on iteration 5140 = 0.1732050556689501
Training loss on iteration 5160 = 0.1677377659827471
Training loss on iteration 5180 = 0.18448560163378716
Training loss on iteration 5200 = 0.18054549396038055
Training loss on iteration 5220 = 0.17655454091727735
Training loss on iteration 5240 = 0.18055326528847218
Training loss on iteration 5260 = 0.16216774024069308
Training loss on iteration 5280 = 0.13744366727769375
Training loss on iteration 5300 = 0.19103155247867107
Training loss on iteration 5320 = 0.1874758955091238
Training loss on iteration 5340 = 0.17083800360560417
Training loss on iteration 5360 = 0.17982881851494312
Training loss on iteration 5380 = 0.1635356955230236
Training loss on iteration 5400 = 0.14836902730166912
Training loss on iteration 5420 = 0.17020065300166606
Training loss on iteration 5440 = 0.15001260563731195
Training loss on iteration 5460 = 0.1854787189513445
Training loss on iteration 5480 = 0.16225841604173183
Training loss on iteration 5500 = 0.17267514690756797
Training loss on iteration 5520 = 0.17498475983738898
Training loss on iteration 5540 = 0.17648741155862807
Training loss on iteration 5560 = 0.16859790831804275
Training loss on iteration 5580 = 0.1499233055859804
Training loss on iteration 5600 = 0.17756696417927742
Training loss on iteration 5620 = 0.18310996033251287
Training loss on iteration 5640 = 0.1782045889645815
Training loss on iteration 5660 = 0.16293704882264137
Training loss on iteration 5680 = 0.1772615771740675
Training loss on iteration 5700 = 0.207151610404253
Training loss on iteration 5720 = 0.39757571518421175
Training loss on iteration 5740 = 0.15856848247349262
Training loss on iteration 5760 = 0.19480704590678216
Training loss on iteration 5780 = 0.18678190968930722
Training loss on iteration 5800 = 0.18365266025066376
Training loss on iteration 5820 = 0.1700901251286268
Training loss on iteration 5840 = 0.16570776291191577
Training loss on iteration 5860 = 0.20110907219350338
Training loss on iteration 5880 = 0.16331499740481376
Training loss on iteration 5900 = 0.49012558422982694
Training loss on iteration 5920 = 0.23546481132507324
Training loss on iteration 5940 = 0.33384172059595585
Training loss on iteration 5960 = 0.20345196761190892
Training loss on iteration 5980 = 0.2052551381289959
Training loss on iteration 6000 = 0.20281065106391907
Training loss on iteration 6020 = 0.20745134800672532
Training loss on iteration 6040 = 0.17167088352143764
Training loss on iteration 6060 = 0.20207781307399272
Training loss on iteration 6080 = 0.19129033870995044
Training loss on iteration 6100 = 0.18431755825877189
Training loss on iteration 6120 = 0.1916914541274309
Training loss on iteration 6140 = 0.16950857006013392
Training loss on iteration 6160 = 0.20289623066782952
Training loss on iteration 6180 = 0.3520827401429415
Training loss on iteration 6200 = 0.2086050845682621
Training loss on iteration 6220 = 0.19753609150648116
Training loss on iteration 6240 = 0.2407694414258003
Training loss on iteration 6260 = 0.35189026445150373
Training loss on iteration 6280 = 0.1784236878156662
Training loss on iteration 6300 = 0.188930756598711
Training loss on iteration 6320 = 0.16267990171909333
Training loss on iteration 6340 = 0.17773303873836993
Training loss on iteration 6360 = 0.195791969075799
Training loss on iteration 6380 = 0.21055783778429032
Training loss on iteration 0 = 0.1348072737455368
Training loss on iteration 20 = 0.16619319915771485
Training loss on iteration 40 = 0.14964644387364387
Training loss on iteration 60 = 0.14831382483243943
Training loss on iteration 80 = 0.15447411686182022
Training loss on iteration 100 = 0.15792943462729453
Training loss on iteration 120 = 0.20205064825713634
Training loss on iteration 140 = 0.15030268467962743
Training loss on iteration 160 = 0.18153205923736096
Training loss on iteration 180 = 0.16559802889823913
Training loss on iteration 200 = 0.1554801344871521
Training loss on iteration 220 = 0.14220697209239005
Training loss on iteration 240 = 0.1561981864273548
Training loss on iteration 260 = 0.1511095430701971
Training loss on iteration 280 = 0.3414302933961153
Training loss on iteration 300 = 0.16750352866947651
Training loss on iteration 320 = 0.13975391238927842
Training loss on iteration 340 = 0.19242502972483636
Training loss on iteration 360 = 0.15309030413627625
Training loss on iteration 380 = 0.14025555290281772
Training loss on iteration 400 = 0.14278408102691173
Training loss on iteration 420 = 0.16220242381095887
Training loss on iteration 440 = 0.14963964484632014
Training loss on iteration 460 = 0.16041504107415677
Training loss on iteration 480 = 0.14866626262664795
Training loss on iteration 500 = 0.1710132822394371
Training loss on iteration 520 = 0.16510083191096783
Training loss on iteration 540 = 0.1702353000640869
Training loss on iteration 560 = 0.15097124502062798
Training loss on iteration 580 = 0.17861214354634286
Training loss on iteration 600 = 0.16254266016185284
Training loss on iteration 620 = 0.15844294801354408
Training loss on iteration 640 = 0.14468361921608447
Training loss on iteration 660 = 0.15916557610034943
Training loss on iteration 680 = 0.1505966607481241
Training loss on iteration 700 = 0.15687472112476825
Training loss on iteration 720 = 0.18307044096291064
Training loss on iteration 740 = 0.14330691657960415
Training loss on iteration 760 = 0.15758621282875537
Training loss on iteration 780 = 0.1862052794545889
Training loss on iteration 800 = 0.13970067165791988
Training loss on iteration 820 = 0.15258328765630721
Training loss on iteration 840 = 0.1485313441604376
Training loss on iteration 860 = 0.1745091862976551
Training loss on iteration 880 = 0.14139172844588757
Training loss on iteration 900 = 0.14823946580290795
Training loss on iteration 920 = 0.13754212260246276
Training loss on iteration 940 = 0.15205774158239366
Training loss on iteration 960 = 0.16544952020049095
Training loss on iteration 980 = 0.16229182295501232
Training loss on iteration 1000 = 0.15083132572472097
Training loss on iteration 1020 = 0.1571194302290678
Training loss on iteration 1040 = 0.1565383903682232
Training loss on iteration 1060 = 0.15240300074219704
Training loss on iteration 1080 = 0.14034381732344628
Training loss on iteration 1100 = 0.1501563426107168
Training loss on iteration 1120 = 0.165113440528512
Training loss on iteration 1140 = 0.19359572604298592
Training loss on iteration 1160 = 0.16767068170011043
Training loss on iteration 1180 = 0.1802940722554922
Training loss on iteration 1200 = 0.17825872749090194
Training loss on iteration 1220 = 0.17221019193530082
Training loss on iteration 1240 = 0.19966128170490266
Training loss on iteration 1260 = 0.16131205447018146
Training loss on iteration 1280 = 0.16937792710959912
Training loss on iteration 1300 = 0.1439341962337494
Training loss on iteration 1320 = 0.14052173420786856
Training loss on iteration 1340 = 0.14575643055140972
Training loss on iteration 1360 = 0.15232671089470387
Training loss on iteration 1380 = 0.14853499829769135
Training loss on iteration 1400 = 0.1605424340814352
Training loss on iteration 1420 = 0.15922677293419837
Training loss on iteration 1440 = 0.17117279432713986
Training loss on iteration 1460 = 0.12602456286549568
Training loss on iteration 1480 = 0.15091432370245456
Training loss on iteration 1500 = 0.1438918549567461
Training loss on iteration 1520 = 0.14963814578950405
Training loss on iteration 1540 = 0.19861516058444978
Training loss on iteration 1560 = 0.1412713907659054
Training loss on iteration 1580 = 0.15148876197636127
Training loss on iteration 1600 = 0.15743109881877898
Training loss on iteration 1620 = 0.1568988673388958
Training loss on iteration 1640 = 0.15158109478652476
Training loss on iteration 1660 = 0.15387267470359803
Training loss on iteration 1680 = 0.13214489333331586
Training loss on iteration 1700 = 0.14849396236240864
Training loss on iteration 1720 = 0.1504410345107317
Training loss on iteration 1740 = 0.17290025837719442
Training loss on iteration 1760 = 0.14095061719417573
Training loss on iteration 1780 = 0.1474464200437069
Training loss on iteration 1800 = 0.1507492296397686
Training loss on iteration 1820 = 0.16173094287514686
Training loss on iteration 1840 = 0.16562520824372767
Training loss on iteration 1860 = 0.16531045660376548
Training loss on iteration 1880 = 0.14955695830285548
Training loss on iteration 1900 = 0.1535344436764717
Training loss on iteration 1920 = 0.16845268420875073
Training loss on iteration 1940 = 0.16218431517481804
Training loss on iteration 1960 = 0.1827048107981682
Training loss on iteration 1980 = 0.1588134441524744
Training loss on iteration 2000 = 0.17142250314354895
Training loss on iteration 2020 = 0.1409202266484499
Training loss on iteration 2040 = 0.16042862683534623
Training loss on iteration 2060 = 0.21371002048254012
Training loss on iteration 2080 = 0.16281496733427048
Training loss on iteration 2100 = 0.16388074979186057
Training loss on iteration 2120 = 0.16119151711463928
Training loss on iteration 2140 = 0.14708297774195672
Training loss on iteration 2160 = 0.16947346441447736
Training loss on iteration 2180 = 0.1590838372707367
Training loss on iteration 2200 = 0.16633105650544167
Training loss on iteration 2220 = 0.14036347717046738
Training loss on iteration 2240 = 0.1677106749266386
Training loss on iteration 2260 = 0.1763551950454712
Training loss on iteration 2280 = 0.15099105834960938
Training loss on iteration 2300 = 0.1671255063265562
Training loss on iteration 2320 = 0.16916607469320297
Training loss on iteration 2340 = 0.16099984385073185
Training loss on iteration 2360 = 0.153261948376894
Training loss on iteration 2380 = 0.16678437478840352
Training loss on iteration 2400 = 0.18507175967097284
Training loss on iteration 2420 = 0.1529863454401493
Training loss on iteration 2440 = 0.1447384063154459
Training loss on iteration 2460 = 0.16012251749634743
Training loss on iteration 2480 = 0.1671741534024477
Training loss on iteration 2500 = 0.14635596573352813
Training loss on iteration 2520 = 0.1551590573042631
Training loss on iteration 2540 = 0.1652765277773142
Training loss on iteration 2560 = 0.16189958043396474
Training loss on iteration 2580 = 0.1691789288073778
Training loss on iteration 2600 = 0.16448000632226467
Training loss on iteration 2620 = 0.33378082402050496
Training loss on iteration 2640 = 0.1981213103979826
Training loss on iteration 2660 = 0.15673342272639273
Training loss on iteration 2680 = 0.17267528288066386
Training loss on iteration 2700 = 0.16536197923123835
Training loss on iteration 2720 = 0.18804943151772022
Training loss on iteration 2740 = 0.17418533191084862
Training loss on iteration 2760 = 0.16924928687512875
Training loss on iteration 2780 = 0.15695241056382656
Training loss on iteration 2800 = 0.1480317533016205
Training loss on iteration 2820 = 0.1640060193836689
Training loss on iteration 2840 = 0.16580611169338227
Training loss on iteration 2860 = 0.1810698512941599
Training loss on iteration 2880 = 0.16739130169153213
Training loss on iteration 2900 = 0.16690187864005565
Training loss on iteration 2920 = 0.17122581079602242
Training loss on iteration 2940 = 0.1954588919878006
Training loss on iteration 2960 = 0.15179384872317314
Training loss on iteration 2980 = 0.15676027685403823
Training loss on iteration 3000 = 0.1462231684476137
Training loss on iteration 3020 = 0.20560168363153936
Training loss on iteration 3040 = 0.13630854450166224
Training loss on iteration 3060 = 0.16783512905240058
Training loss on iteration 3080 = 0.15118101760745048
Training loss on iteration 3100 = 0.16346295475959777
Training loss on iteration 3120 = 0.1713701466098428
Training loss on iteration 3140 = 0.15943803153932096
Training loss on iteration 3160 = 0.16495539322495462
Training loss on iteration 3180 = 0.16166639924049378
Training loss on iteration 3200 = 0.16193640604615211
Training loss on iteration 3220 = 0.14976096265017985
Training loss on iteration 3240 = 0.14873140305280685
Training loss on iteration 3260 = 0.15935202762484552
Training loss on iteration 3280 = 0.16376824490725994
Training loss on iteration 3300 = 0.15738876760005951
Training loss on iteration 3320 = 0.15756500698626041
Training loss on iteration 3340 = 0.25084176771342753
Training loss on iteration 3360 = 0.18141041994094848
Training loss on iteration 3380 = 0.15967080630362035
Training loss on iteration 3400 = 0.16050412505865097
Training loss on iteration 3420 = 0.18431389853358268
Training loss on iteration 3440 = 0.15252674669027327
Training loss on iteration 3460 = 0.15877214260399342
Training loss on iteration 3480 = 0.16155787706375122
Training loss on iteration 3500 = 0.18969648890197277
Training loss on iteration 3520 = 0.16326917447149752
Training loss on iteration 3540 = 0.1507767252624035
Training loss on iteration 3560 = 0.1611210096627474
Training loss on iteration 3580 = 0.17742240168154239
Training loss on iteration 3600 = 0.16072580516338347
Training loss on iteration 3620 = 0.1909310169517994
Training loss on iteration 3640 = 0.16268569566309452
Training loss on iteration 3660 = 0.16679496951401235
Training loss on iteration 3680 = 0.15741268023848534
Training loss on iteration 3700 = 0.13880033418536186
Training loss on iteration 3720 = 0.1473617359995842
Training loss on iteration 3740 = 0.1944939348846674
Training loss on iteration 3760 = 0.16398488394916058
Training loss on iteration 3780 = 0.16430094875395299
Training loss on iteration 3800 = 0.17708329781889914
Training loss on iteration 3820 = 0.15474347285926343
Training loss on iteration 3840 = 0.1650604523718357
Training loss on iteration 3860 = 0.18346457034349442
Training loss on iteration 3880 = 0.19864703007042409
Training loss on iteration 3900 = 0.15118542239069938
Training loss on iteration 3920 = 0.15849548541009426
Training loss on iteration 3940 = 0.15968917347490788
Training loss on iteration 3960 = 0.1791857574135065
Training loss on iteration 3980 = 0.17875058948993683
Training loss on iteration 4000 = 0.1613271351903677
Training loss on iteration 4020 = 0.14923945032060146
Training loss on iteration 4040 = 0.14545719660818576
Training loss on iteration 4060 = 0.15653724409639835
Training loss on iteration 4080 = 0.1625431150197983
Training loss on iteration 4100 = 0.16886534579098225
Training loss on iteration 4120 = 0.16108375713229178
Training loss on iteration 4140 = 0.15901104211807252
Training loss on iteration 4160 = 0.16612949036061764
Training loss on iteration 4180 = 0.17510724663734437
Training loss on iteration 4200 = 0.16256928443908691
Training loss on iteration 4220 = 0.13226713947951793
Training loss on iteration 4240 = 0.16363720037043095
Training loss on iteration 4260 = 0.16127003654837607
Training loss on iteration 4280 = 0.1445113155990839
Training loss on iteration 4300 = 0.16471860259771348
Training loss on iteration 4320 = 0.15944561474025248
Training loss on iteration 4340 = 0.1849353525787592
Training loss on iteration 4360 = 0.16600101590156555
Training loss on iteration 4380 = 0.14972743391990662
Training loss on iteration 4400 = 0.15665483251214027
Training loss on iteration 4420 = 0.17636994943022727
Training loss on iteration 4440 = 0.17893209606409072
Training loss on iteration 4460 = 0.16451750360429288
Training loss on iteration 4480 = 0.1753406323492527
Training loss on iteration 4500 = 0.17151517905294894
Training loss on iteration 4520 = 0.15421266704797745
Training loss on iteration 4540 = 0.14698963910341262
Training loss on iteration 4560 = 0.16163646429777145
Training loss on iteration 4580 = 0.14564389511942863
Training loss on iteration 4600 = 0.16120747961103915
Training loss on iteration 4620 = 0.16720617562532425
Training loss on iteration 4640 = 0.18312367387115955
Training loss on iteration 4660 = 0.16801630891859531
Training loss on iteration 4680 = 0.15689569823443889
Training loss on iteration 4700 = 0.15712650902569295
Training loss on iteration 4720 = 0.18307176865637304
Training loss on iteration 4740 = 0.16157067120075225
Training loss on iteration 4760 = 0.16101645790040492
Training loss on iteration 4780 = 0.16944366171956063
Training loss on iteration 4800 = 0.16234503872692585
Training loss on iteration 4820 = 0.16280730105936528
Training loss on iteration 4840 = 0.1520983725786209
Training loss on iteration 4860 = 0.1849804300814867
Training loss on iteration 4880 = 0.1793278709053993
Training loss on iteration 4900 = 0.18584966398775576
Training loss on iteration 4920 = 0.174509884044528
Training loss on iteration 4940 = 0.1752960629761219
Training loss on iteration 4960 = 0.17681791447103024
Training loss on iteration 4980 = 0.17573219425976278
Training loss on iteration 5000 = 0.15570560581982135
Training loss on iteration 5020 = 0.2044513188302517
Training loss on iteration 5040 = 0.1508296150714159
Training loss on iteration 5060 = 0.18744409568607806
Training loss on iteration 5080 = 0.1635209932923317
Training loss on iteration 5100 = 0.16816512122750282
Training loss on iteration 5120 = 0.16664512529969217
Training loss on iteration 5140 = 0.1560612302273512
Training loss on iteration 5160 = 0.17493644766509533
Training loss on iteration 5180 = 0.22099841944873333
Training loss on iteration 5200 = 0.17768330201506616
Training loss on iteration 5220 = 0.17074767164885998
Training loss on iteration 5240 = 0.17949760667979717
Training loss on iteration 5260 = 0.2088191621005535
Training loss on iteration 5280 = 0.19510239958763123
Training loss on iteration 5300 = 0.19629632867872715
Training loss on iteration 5320 = 0.1691908363252878
Training loss on iteration 5340 = 0.17019051797688006
Training loss on iteration 5360 = 0.16785067431628703
Training loss on iteration 5380 = 0.17870599552989005
Training loss on iteration 5400 = 0.15950674563646317
Training loss on iteration 5420 = 0.21143080331385136
Training loss on iteration 5440 = 0.19218149408698082
Training loss on iteration 5460 = 0.17584381476044655
Training loss on iteration 5480 = 0.1874294999986887
Training loss on iteration 5500 = 0.16893332675099373
Training loss on iteration 5520 = 0.19832605235278605
Training loss on iteration 5540 = 0.1898384340107441
Training loss on iteration 5560 = 0.1946409597992897
Training loss on iteration 5580 = 0.16860291175544262
Training loss on iteration 5600 = 0.15396773368120192
Training loss on iteration 5620 = 0.1591344565153122
Training loss on iteration 5640 = 0.1677790816873312
Training loss on iteration 5660 = 0.1691123265773058
Training loss on iteration 5680 = 0.16708094179630278
Training loss on iteration 5700 = 0.1756732489913702
Training loss on iteration 5720 = 0.1887573041021824
Training loss on iteration 5740 = 0.1600239511579275
Training loss on iteration 5760 = 0.16663635782897473
Training loss on iteration 5780 = 0.1662242215126753
Training loss on iteration 5800 = 0.17806981652975082
Training loss on iteration 5820 = 0.15231807641685008
Training loss on iteration 5840 = 0.18957066610455514
Training loss on iteration 5860 = 0.19386387057602406
Training loss on iteration 5880 = 0.17518407478928566
Training loss on iteration 5900 = 0.17188952825963497
Training loss on iteration 5920 = 0.17182543575763704
Training loss on iteration 5940 = 0.16231734380126
Training loss on iteration 5960 = 0.1981750290840864
Training loss on iteration 5980 = 0.1991820078343153
Training loss on iteration 6000 = 0.17009455785155297
Training loss on iteration 6020 = 0.1972462199628353
Training loss on iteration 6040 = 0.1892377261072397
Training loss on iteration 6060 = 0.19976195730268956
Training loss on iteration 6080 = 0.19239190071821213
Training loss on iteration 6100 = 0.15921013839542866
Training loss on iteration 6120 = 0.1480677530169487
Training loss on iteration 6140 = 0.17561438046395778
Training loss on iteration 6160 = 0.1673761300742626
Training loss on iteration 6180 = 0.18154336586594583
Training loss on iteration 6200 = 0.17294264659285546
Training loss on iteration 6220 = 0.1803797621279955
Training loss on iteration 6240 = 0.19194110631942748
Training loss on iteration 6260 = 0.18695287294685842
Training loss on iteration 6280 = 0.1525481928139925
Training loss on iteration 6300 = 0.1693714465945959
Training loss on iteration 6320 = 0.17848461754620076
Training loss on iteration 6340 = 0.24146667271852493
Training loss on iteration 6360 = 0.17887743338942527
Training loss on iteration 6380 = 0.15621172562241553
Training loss on iteration 0 = 0.20024822652339935
Training loss on iteration 20 = 0.15615601986646652
Training loss on iteration 40 = 0.1701225835829973
Training loss on iteration 60 = 0.13870629630982875
Training loss on iteration 80 = 0.1437190853059292
Training loss on iteration 100 = 0.1465821135789156
Training loss on iteration 120 = 0.14542400278151035
Training loss on iteration 140 = 0.14573448859155178
Training loss on iteration 160 = 0.14859963916242122
Training loss on iteration 180 = 0.13827503472566605
Training loss on iteration 200 = 0.15553219988942146
Training loss on iteration 220 = 0.15632372722029686
Training loss on iteration 240 = 0.15421081706881523
Training loss on iteration 260 = 0.15025959424674512
Training loss on iteration 280 = 0.16159509867429733
Training loss on iteration 300 = 0.14993095360696315
Training loss on iteration 320 = 0.13852437250316144
Training loss on iteration 340 = 0.14495762512087823
Training loss on iteration 360 = 0.17427044212818146
Training loss on iteration 380 = 0.15088170133531092
Training loss on iteration 400 = 0.15030498951673507
Training loss on iteration 420 = 0.12726715691387652
Training loss on iteration 440 = 0.13826280198991298
Training loss on iteration 460 = 0.14728364795446397
Training loss on iteration 480 = 0.16324349641799926
Training loss on iteration 500 = 0.15285763777792455
Training loss on iteration 520 = 0.1500107128173113
Training loss on iteration 540 = 0.17556308545172214
Training loss on iteration 560 = 0.15863688178360463
Training loss on iteration 580 = 0.15885559618473052
Training loss on iteration 600 = 0.1474426921457052
Training loss on iteration 620 = 0.14545681402087213
Training loss on iteration 640 = 0.1422599505633116
Training loss on iteration 660 = 0.14526820555329323
Training loss on iteration 680 = 0.15485451221466065
Training loss on iteration 700 = 0.1838078536093235
Training loss on iteration 720 = 0.18694290854036807
Training loss on iteration 740 = 0.15983794890344144
Training loss on iteration 760 = 0.15310814045369625
Training loss on iteration 780 = 0.15215382352471352
Training loss on iteration 800 = 0.1461720049381256
Training loss on iteration 820 = 0.15073880776762963
Training loss on iteration 840 = 0.14427873268723487
Training loss on iteration 860 = 0.16030029579997063
Training loss on iteration 880 = 0.15337701849639415
Training loss on iteration 900 = 0.13581805862486362
Training loss on iteration 920 = 0.14291282184422016
Training loss on iteration 940 = 0.14392391853034497
Training loss on iteration 960 = 0.14659710079431534
Training loss on iteration 980 = 0.1650066751986742
Training loss on iteration 1000 = 0.16212930120527744
Training loss on iteration 1020 = 0.14089747443795203
Training loss on iteration 1040 = 0.16686954200267792
Training loss on iteration 1060 = 0.15276968218386172
Training loss on iteration 1080 = 0.14855650439858437
Training loss on iteration 1100 = 0.14687465354800225
Training loss on iteration 1120 = 0.14621880017220973
Training loss on iteration 1140 = 0.15390186719596385
Training loss on iteration 1160 = 0.15344323255121708
Training loss on iteration 1180 = 0.16745366677641868
Training loss on iteration 1200 = 0.16450319290161133
Training loss on iteration 1220 = 0.14354444146156312
Training loss on iteration 1240 = 0.1459895722568035
Training loss on iteration 1260 = 0.15131159052252768
Training loss on iteration 1280 = 0.15214429348707198
Training loss on iteration 1300 = 0.15992035008966923
Training loss on iteration 1320 = 0.14569436460733415
Training loss on iteration 1340 = 0.15205579102039338
Training loss on iteration 1360 = 0.15569177865982056
Training loss on iteration 1380 = 0.16630712635815142
Training loss on iteration 1400 = 0.1664226695895195
Training loss on iteration 1420 = 0.22376623786985875
Training loss on iteration 1440 = 0.1428810629993677
Training loss on iteration 1460 = 0.15282550901174546
Training loss on iteration 1480 = 0.17452081702649594
Training loss on iteration 1500 = 0.16077720783650876
Training loss on iteration 1520 = 0.1546907678246498
Training loss on iteration 1540 = 0.15324989296495914
Training loss on iteration 1560 = 0.16154412925243378
Training loss on iteration 1580 = 0.15116012170910836
Training loss on iteration 1600 = 0.13749597854912282
Training loss on iteration 1620 = 0.13460864275693893
Training loss on iteration 1640 = 0.16044059060513974
Training loss on iteration 1660 = 0.14987924732267857
Training loss on iteration 1680 = 0.13906716071069242
Training loss on iteration 1700 = 0.15686413571238517
Training loss on iteration 1720 = 0.16582393050193786
Training loss on iteration 1740 = 0.20069351159036158
Training loss on iteration 1760 = 0.1769967671483755
Training loss on iteration 1780 = 0.17794826067984104
Training loss on iteration 1800 = 0.16530716791749
Training loss on iteration 1820 = 0.17592757381498814
Training loss on iteration 1840 = 0.16146522760391235
Training loss on iteration 1860 = 0.16808743625879288
Training loss on iteration 1880 = 0.15912395752966405
Training loss on iteration 1900 = 0.16401577442884446
Training loss on iteration 1920 = 0.1510356716811657
Training loss on iteration 1940 = 0.14803605228662492
Training loss on iteration 1960 = 0.16286302357912064
Training loss on iteration 1980 = 0.18247611746191977
Training loss on iteration 2000 = 0.15904848352074624
Training loss on iteration 2020 = 0.157515549659729
Training loss on iteration 2040 = 0.15422076135873794
Training loss on iteration 2060 = 0.1616252690553665
Training loss on iteration 2080 = 0.15746321454644202
Training loss on iteration 2100 = 0.1575355648994446
Training loss on iteration 2120 = 0.14278289563953878
Training loss on iteration 2140 = 0.1375237163156271
Training loss on iteration 2160 = 0.14817808717489242
Training loss on iteration 2180 = 0.16713568903505802
Training loss on iteration 2200 = 0.1521143615245819
Training loss on iteration 2220 = 0.1458402045071125
Training loss on iteration 2240 = 0.1524518433958292
Training loss on iteration 2260 = 0.15222113952040672
Training loss on iteration 2280 = 0.1738056182861328
Training loss on iteration 2300 = 0.13940136097371578
Training loss on iteration 2320 = 0.1591518845409155
Training loss on iteration 2340 = 0.14483675621449948
Training loss on iteration 2360 = 0.16614098250865936
Training loss on iteration 2380 = 0.1604713749140501
Training loss on iteration 2400 = 0.19087846018373966
Training loss on iteration 2420 = 0.1488884124904871
Training loss on iteration 2440 = 0.16035921461880207
Training loss on iteration 2460 = 0.15969703793525697
Training loss on iteration 2480 = 0.17740551419556141
Training loss on iteration 2500 = 0.14917840175330638
Training loss on iteration 2520 = 0.17075888030230998
Training loss on iteration 2540 = 0.16305072642862797
Training loss on iteration 2560 = 0.15135765820741653
Training loss on iteration 2580 = 0.16057434417307376
Training loss on iteration 2600 = 0.16113075017929077
Training loss on iteration 2620 = 0.16661810874938965
Training loss on iteration 2640 = 0.13588917795568706
Training loss on iteration 2660 = 0.14445248544216155
Training loss on iteration 2680 = 0.1533520959317684
Training loss on iteration 2700 = 0.18398199379444122
Training loss on iteration 2720 = 0.14117566570639611
Training loss on iteration 2740 = 0.15925334207713604
Training loss on iteration 2760 = 0.17933011054992676
Training loss on iteration 2780 = 0.15255705118179322
Training loss on iteration 2800 = 0.14058398008346557
Training loss on iteration 2820 = 0.16390640772879123
Training loss on iteration 2840 = 0.15221048779785634
Training loss on iteration 2860 = 0.18086398690938948
Training loss on iteration 2880 = 0.16575462967157364
Training loss on iteration 2900 = 0.16968532651662827
Training loss on iteration 2920 = 0.15307672806084155
Training loss on iteration 2940 = 0.16057103760540486
Training loss on iteration 2960 = 0.16045696809887885
Training loss on iteration 2980 = 0.17819705083966256
Training loss on iteration 3000 = 0.16714865006506444
Training loss on iteration 3020 = 0.1346576578915119
Training loss on iteration 3040 = 0.1631918478757143
Training loss on iteration 3060 = 0.22136632204055787
Training loss on iteration 3080 = 0.15579264834523202
Training loss on iteration 3100 = 0.14299692921340465
Training loss on iteration 3120 = 0.16159455813467502
Training loss on iteration 3140 = 0.1532083682715893
Training loss on iteration 3160 = 0.1412771102041006
Training loss on iteration 3180 = 0.16495839692652225
Training loss on iteration 3200 = 0.14864581376314162
Training loss on iteration 3220 = 0.16194535717368125
Training loss on iteration 3240 = 0.17336502112448215
Training loss on iteration 3260 = 0.15861568860709668
Training loss on iteration 3280 = 0.16563359498977662
Training loss on iteration 3300 = 0.15800690092146397
Training loss on iteration 3320 = 0.1619723491370678
Training loss on iteration 3340 = 0.15117906928062438
Training loss on iteration 3360 = 0.14893580004572868
Training loss on iteration 3380 = 0.18088284954428674
Training loss on iteration 3400 = 0.14718365333974362
Training loss on iteration 3420 = 0.16125330552458764
Training loss on iteration 3440 = 0.1734694056212902
Training loss on iteration 3460 = 0.1539238929748535
Training loss on iteration 3480 = 0.16470771320164204
Training loss on iteration 3500 = 0.14355027675628662
Training loss on iteration 3520 = 0.1542535912245512
Training loss on iteration 3540 = 0.1448841217905283
Training loss on iteration 3560 = 0.1612223669886589
Training loss on iteration 3580 = 0.18885542154312135
Training loss on iteration 3600 = 0.15457177124917507
Training loss on iteration 3620 = 0.17489069886505604
Training loss on iteration 3640 = 0.1863477248698473
Training loss on iteration 3660 = 0.16821953617036342
Training loss on iteration 3680 = 0.15055964142084122
Training loss on iteration 3700 = 0.1685653556138277
Training loss on iteration 3720 = 0.17882237620651723
Training loss on iteration 3740 = 0.16775937788188458
Training loss on iteration 3760 = 0.15916437655687332
Training loss on iteration 3780 = 0.15767667554318904
Training loss on iteration 3800 = 0.17786918058991433
Training loss on iteration 3820 = 0.16189296282827853
Training loss on iteration 3840 = 0.17013574726879596
Training loss on iteration 3860 = 0.15294565334916116
Training loss on iteration 3880 = 0.17514698058366776
Training loss on iteration 3900 = 0.15350340567529203
Training loss on iteration 3920 = 0.18169003762304783
Training loss on iteration 3940 = 0.1553871374577284
Training loss on iteration 3960 = 0.17942901216447354
Training loss on iteration 3980 = 0.1705882206559181
Training loss on iteration 4000 = 0.15940751805901526
Training loss on iteration 4020 = 0.15473221987485886
Training loss on iteration 4040 = 0.1767866000533104
Training loss on iteration 4060 = 0.14357910715043545
Training loss on iteration 4080 = 0.15357449427247047
Training loss on iteration 4100 = 0.3365498535335064
Training loss on iteration 4120 = 0.15699409320950508
Training loss on iteration 4140 = 0.1663874752819538
Training loss on iteration 4160 = 0.16725817359983922
Training loss on iteration 4180 = 0.14981123097240925
Training loss on iteration 4200 = 0.14954572394490243
Training loss on iteration 4220 = 0.1616463925689459
Training loss on iteration 4240 = 0.17868376933038235
Training loss on iteration 4260 = 0.16519611626863479
Training loss on iteration 4280 = 0.1492783736437559
Training loss on iteration 4300 = 0.14342380575835706
Training loss on iteration 4320 = 0.1827032707631588
Training loss on iteration 4340 = 0.14105168953537942
Training loss on iteration 4360 = 0.19453499056398868
Training loss on iteration 4380 = 0.1551014393568039
Training loss on iteration 4400 = 0.1881997473537922
Training loss on iteration 4420 = 0.14817404821515084
Training loss on iteration 4440 = 0.1537335816770792
Training loss on iteration 4460 = 0.14139282740652562
Training loss on iteration 4480 = 0.16454618722200393
Training loss on iteration 4500 = 0.16109131909906865
Training loss on iteration 4520 = 0.15802441276609897
Training loss on iteration 4540 = 0.156824604049325
Training loss on iteration 4560 = 0.156595878303051
Training loss on iteration 4580 = 0.14549822844564914
Training loss on iteration 4600 = 0.15300884582102298
Training loss on iteration 4620 = 0.1775884099304676
Training loss on iteration 4640 = 0.15656184814870358
Training loss on iteration 4660 = 0.16586521416902542
Training loss on iteration 4680 = 0.1797306727617979
Training loss on iteration 4700 = 0.1374444890767336
Training loss on iteration 4720 = 0.16842109635472297
Training loss on iteration 4740 = 0.15999180562794207
Training loss on iteration 4760 = 0.19424459487199783
Training loss on iteration 4780 = 0.16511800922453404
Training loss on iteration 4800 = 0.16069513596594334
Training loss on iteration 4820 = 0.13946582637727262
Training loss on iteration 4840 = 0.1487027805298567
Training loss on iteration 4860 = 0.14593139179050924
Training loss on iteration 4880 = 0.16901394873857498
Training loss on iteration 4900 = 0.16885946989059447
Training loss on iteration 4920 = 0.16587722301483154
Training loss on iteration 4940 = 0.18142411038279532
Training loss on iteration 4960 = 0.17474733032286166
Training loss on iteration 4980 = 0.14799014218151568
Training loss on iteration 5000 = 0.15823927484452724
Training loss on iteration 5020 = 0.15681511126458644
Training loss on iteration 5040 = 0.1514591421931982
Training loss on iteration 5060 = 0.15584501177072524
Training loss on iteration 5080 = 0.16288427151739598
Training loss on iteration 5100 = 0.1822098448872566
Training loss on iteration 5120 = 0.16208305172622203
Training loss on iteration 5140 = 0.17684530913829805
Training loss on iteration 5160 = 0.15772322453558446
Training loss on iteration 5180 = 0.1729831751435995
Training loss on iteration 5200 = 0.16463606245815754
Training loss on iteration 5220 = 0.18050013072788715
Training loss on iteration 5240 = 0.147947221621871
Training loss on iteration 5260 = 0.1740636095404625
Training loss on iteration 5280 = 0.2012597694993019
Training loss on iteration 5300 = 0.16501086466014386
Training loss on iteration 5320 = 0.1887092676013708
Training loss on iteration 5340 = 0.1679881401360035
Training loss on iteration 5360 = 0.17725850008428096
Training loss on iteration 5380 = 0.1816380735486746
Training loss on iteration 5400 = 0.14650615118443966
Training loss on iteration 5420 = 0.1536225013434887
Training loss on iteration 5440 = 0.16665657721459864
Training loss on iteration 5460 = 0.1616046153008938
Training loss on iteration 5480 = 0.15214260891079903
Training loss on iteration 5500 = 0.1669385362416506
Training loss on iteration 5520 = 0.17984901331365108
Training loss on iteration 5540 = 0.17085743546485901
Training loss on iteration 5560 = 0.1659570038318634
Training loss on iteration 5580 = 0.1716546256095171
Training loss on iteration 5600 = 0.15975815206766128
Training loss on iteration 5620 = 0.1565329350531101
Training loss on iteration 5640 = 0.1738952562212944
Training loss on iteration 5660 = 0.18018611781299115
Training loss on iteration 5680 = 0.14762006737291813
Training loss on iteration 5700 = 0.18377276435494422
Training loss on iteration 5720 = 0.1983990002423525
Training loss on iteration 5740 = 0.19466438442468642
Training loss on iteration 5760 = 0.1813666358590126
Training loss on iteration 5780 = 0.14122502990067004
Training loss on iteration 5800 = 0.15825118869543076
Training loss on iteration 5820 = 0.22031012773513795
Training loss on iteration 5840 = 0.16506339162588118
Training loss on iteration 5860 = 0.18455796241760253
Training loss on iteration 5880 = 0.17589584961533547
Training loss on iteration 5900 = 0.20083570033311843
Training loss on iteration 5920 = 0.16814504601061345
Training loss on iteration 5940 = 0.1578491922467947
Training loss on iteration 5960 = 0.17528245113790036
Training loss on iteration 5980 = 0.32560209967195985
Training loss on iteration 6000 = 0.16653097420930862
Training loss on iteration 6020 = 0.1750081568956375
Training loss on iteration 6040 = 0.17063032388687133
Training loss on iteration 6060 = 0.15561974309384824
Training loss on iteration 6080 = 0.17392932213842868
Training loss on iteration 6100 = 0.1625116366893053
Training loss on iteration 6120 = 0.16502240262925624
Training loss on iteration 6140 = 0.17991680651903152
Training loss on iteration 6160 = 0.1776067517697811
Training loss on iteration 6180 = 0.15787251964211463
Training loss on iteration 6200 = 0.17405487596988678
Training loss on iteration 6220 = 0.17196302562952043
Training loss on iteration 6240 = 0.19204757399857045
Training loss on iteration 6260 = 0.17827853374183178
Training loss on iteration 6280 = 0.17184060700237752
Training loss on iteration 6300 = 0.16620687991380692
Training loss on iteration 6320 = 0.15949213430285453
Training loss on iteration 6340 = 0.16389833353459834
Training loss on iteration 6360 = 0.15654298178851606
Training loss on iteration 6380 = 0.17996714487671853
Training loss on iteration 0 = 0.20177553594112396
Training loss on iteration 20 = 0.13495194017887116
Training loss on iteration 40 = 0.15420095734298228
Training loss on iteration 60 = 0.1489259947091341
Training loss on iteration 80 = 0.13670929484069347
Training loss on iteration 100 = 0.16453474387526512
Training loss on iteration 120 = 0.11534484270960092
Training loss on iteration 140 = 0.1375445533543825
Training loss on iteration 160 = 0.14367831721901894
Training loss on iteration 180 = 0.14177815839648247
Training loss on iteration 200 = 0.14284957088530065
Training loss on iteration 220 = 0.14552292004227638
Training loss on iteration 240 = 0.14317675158381463
Training loss on iteration 260 = 0.14565240778028965
Training loss on iteration 280 = 0.13508505783975125
Training loss on iteration 300 = 0.14722375757992268
Training loss on iteration 320 = 0.14112477339804172
Training loss on iteration 340 = 0.15361324399709703
Training loss on iteration 360 = 0.14519955813884736
Training loss on iteration 380 = 0.14190785735845565
Training loss on iteration 400 = 0.1639178842306137
Training loss on iteration 420 = 0.1478952270001173
Training loss on iteration 440 = 0.12978280559182168
Training loss on iteration 460 = 0.14105338416993618
Training loss on iteration 480 = 0.13410317413508893
Training loss on iteration 500 = 0.1388186037540436
Training loss on iteration 520 = 0.1900804962962866
Training loss on iteration 540 = 0.13248611316084863
Training loss on iteration 560 = 0.15681824460625648
Training loss on iteration 580 = 0.14593738242983817
Training loss on iteration 600 = 0.13972750827670097
Training loss on iteration 620 = 0.14076994508504867
Training loss on iteration 640 = 0.13383124880492686
Training loss on iteration 660 = 0.14565237946808338
Training loss on iteration 680 = 0.16435078606009484
Training loss on iteration 700 = 0.141256882250309
Training loss on iteration 720 = 0.14691129885613918
Training loss on iteration 740 = 0.16646008230745793
Training loss on iteration 760 = 0.14562200866639613
Training loss on iteration 780 = 0.14529957734048365
Training loss on iteration 800 = 0.15487861149013044
Training loss on iteration 820 = 0.13847492635250092
Training loss on iteration 840 = 0.12769068144261836
Training loss on iteration 860 = 0.1436047673225403
Training loss on iteration 880 = 0.13525838032364845
Training loss on iteration 900 = 0.16317627243697644
Training loss on iteration 920 = 0.13374604247510433
Training loss on iteration 940 = 0.14955555759370326
Training loss on iteration 960 = 0.14899868927896023
Training loss on iteration 980 = 0.1599836315959692
Training loss on iteration 1000 = 0.13374274224042892
Training loss on iteration 1020 = 0.14730114601552485
Training loss on iteration 1040 = 0.1434454597532749
Training loss on iteration 1060 = 0.1621155831962824
Training loss on iteration 1080 = 0.15515713952481747
Training loss on iteration 1100 = 0.13970865458250045
Training loss on iteration 1120 = 0.1403474621474743
Training loss on iteration 1140 = 0.16505688950419425
Training loss on iteration 1160 = 0.14002565965056418
Training loss on iteration 1180 = 0.15421389266848565
Training loss on iteration 1200 = 0.18477778509259224
Training loss on iteration 1220 = 0.17020408622920513
Training loss on iteration 1240 = 0.1409781876951456
Training loss on iteration 1260 = 0.13763504438102245
Training loss on iteration 1280 = 0.14708505906164646
Training loss on iteration 1300 = 0.1598885152488947
Training loss on iteration 1320 = 0.17646874375641347
Training loss on iteration 1340 = 0.1592022716999054
Training loss on iteration 1360 = 0.15337237752974034
Training loss on iteration 1380 = 0.13972896970808507
Training loss on iteration 1400 = 0.16002087406814097
Training loss on iteration 1420 = 0.1628067560493946
Training loss on iteration 1440 = 0.1365390747785568
Training loss on iteration 1460 = 0.1301300123333931
Training loss on iteration 1480 = 0.15352059118449687
Training loss on iteration 1500 = 0.14195642322301866
Training loss on iteration 1520 = 0.14953112006187438
Training loss on iteration 1540 = 0.15023610778152943
Training loss on iteration 1560 = 0.13985182531177998
Training loss on iteration 1580 = 0.14729424864053725
Training loss on iteration 1600 = 0.14635385908186435
Training loss on iteration 1620 = 0.18741977214813232
Training loss on iteration 1640 = 0.1478914324194193
Training loss on iteration 1660 = 0.15886606201529502
Training loss on iteration 1680 = 0.16761726476252078
Training loss on iteration 1700 = 0.1453044343739748
Training loss on iteration 1720 = 0.16584331467747687
Training loss on iteration 1740 = 0.17647041343152522
Training loss on iteration 1760 = 0.144530925527215
Training loss on iteration 1780 = 0.13999119251966477
Training loss on iteration 1800 = 0.16052232086658477
Training loss on iteration 1820 = 0.15182954184710978
Training loss on iteration 1840 = 0.14778647124767302
Training loss on iteration 1860 = 0.15419944562017918
Training loss on iteration 1880 = 0.1577780932188034
Training loss on iteration 1900 = 0.1551857989281416
Training loss on iteration 1920 = 0.16152416877448558
Training loss on iteration 1940 = 0.149314471334219
Training loss on iteration 1960 = 0.1459569361060858
Training loss on iteration 1980 = 0.1361812073737383
Training loss on iteration 2000 = 0.14199474155902864
Training loss on iteration 2020 = 0.1702228408306837
Training loss on iteration 2040 = 0.219932172447443
Training loss on iteration 2060 = 0.14345615431666375
Training loss on iteration 2080 = 0.16960733011364937
Training loss on iteration 2100 = 0.14547800570726394
Training loss on iteration 2120 = 0.16857662200927734
Training loss on iteration 2140 = 0.15341698341071605
Training loss on iteration 2160 = 0.15934415869414806
Training loss on iteration 2180 = 0.1584333438426256
Training loss on iteration 2200 = 0.14246590323746205
Training loss on iteration 2220 = 0.1483569249510765
Training loss on iteration 2240 = 0.17455410435795785
Training loss on iteration 2260 = 0.1589524418115616
Training loss on iteration 2280 = 0.14802518710494042
Training loss on iteration 2300 = 0.15013433657586575
Training loss on iteration 2320 = 0.15616672150790692
Training loss on iteration 2340 = 0.15744746141135693
Training loss on iteration 2360 = 0.1496705800294876
Training loss on iteration 2380 = 0.1558138459920883
Training loss on iteration 2400 = 0.1685736935585737
Training loss on iteration 2420 = 0.14776290245354176
Training loss on iteration 2440 = 0.15095003545284272
Training loss on iteration 2460 = 0.1632333941757679
Training loss on iteration 2480 = 0.1537122242152691
Training loss on iteration 2500 = 0.15709164477884768
Training loss on iteration 2520 = 0.1535578142851591
Training loss on iteration 2540 = 0.15579445771872996
Training loss on iteration 2560 = 0.13676282726228237
Training loss on iteration 2580 = 0.17045201994478704
Training loss on iteration 2600 = 0.15502232424914836
Training loss on iteration 2620 = 0.17772880680859088
Training loss on iteration 2640 = 0.1667134128510952
Training loss on iteration 2660 = 0.15944675616919995
Training loss on iteration 2680 = 0.14953884556889535
Training loss on iteration 2700 = 0.1481106523424387
Training loss on iteration 2720 = 0.15989794582128525
Training loss on iteration 2740 = 0.1479726739227772
Training loss on iteration 2760 = 0.14446736611425876
Training loss on iteration 2780 = 0.14448323138058186
Training loss on iteration 2800 = 0.1535723302513361
Training loss on iteration 2820 = 0.15143588185310364
Training loss on iteration 2840 = 0.14130384102463722
Training loss on iteration 2860 = 0.14791296012699603
Training loss on iteration 2880 = 0.14520576521754264
Training loss on iteration 2900 = 0.15314557179808616
Training loss on iteration 2920 = 0.1608037982136011
Training loss on iteration 2940 = 0.14638568833470345
Training loss on iteration 2960 = 0.14834122061729432
Training loss on iteration 2980 = 0.1541335865855217
Training loss on iteration 3000 = 0.14984276965260507
Training loss on iteration 3020 = 0.15195609964430332
Training loss on iteration 3040 = 0.1446512673050165
Training loss on iteration 3060 = 0.18850166015326977
Training loss on iteration 3080 = 0.14344466999173164
Training loss on iteration 3100 = 0.16138204894959926
Training loss on iteration 3120 = 0.16228494346141814
Training loss on iteration 3140 = 0.17458056546747686
Training loss on iteration 3160 = 0.15914860926568508
Training loss on iteration 3180 = 0.1353624913841486
Training loss on iteration 3200 = 0.15211973190307618
Training loss on iteration 3220 = 0.15072393380105495
Training loss on iteration 3240 = 0.159418535977602
Training loss on iteration 3260 = 0.17024512588977814
Training loss on iteration 3280 = 0.1687827330082655
Training loss on iteration 3300 = 0.15817566514015197
Training loss on iteration 3320 = 0.14757143408060075
Training loss on iteration 3340 = 0.15414550006389618
Training loss on iteration 3360 = 0.15297912620007992
Training loss on iteration 3380 = 0.13720019422471524
Training loss on iteration 3400 = 0.146502922847867
Training loss on iteration 3420 = 0.1552044667303562
Training loss on iteration 3440 = 0.14953135773539544
Training loss on iteration 3460 = 0.15692352540791035
Training loss on iteration 3480 = 0.1296689510345459
Training loss on iteration 3500 = 0.15922458842396736
Training loss on iteration 3520 = 0.15791322477161884
Training loss on iteration 3540 = 0.16751709580421448
Training loss on iteration 3560 = 0.14910095743834972
Training loss on iteration 3580 = 0.16014829464256763
Training loss on iteration 3600 = 0.31793140769004824
Training loss on iteration 3620 = 0.16093165166676043
Training loss on iteration 3640 = 0.14707606993615627
Training loss on iteration 3660 = 0.15987863652408124
Training loss on iteration 3680 = 0.1510133348405361
Training loss on iteration 3700 = 0.15678258016705512
Training loss on iteration 3720 = 0.14529560655355453
Training loss on iteration 3740 = 0.14467342458665372
Training loss on iteration 3760 = 0.1366836991161108
Training loss on iteration 3780 = 0.17308254539966583
Training loss on iteration 3800 = 0.15578770264983177
Training loss on iteration 3820 = 0.152950918674469
Training loss on iteration 3840 = 0.18315308801829816
Training loss on iteration 3860 = 0.1602322354912758
Training loss on iteration 3880 = 0.16483959257602693
Training loss on iteration 3900 = 0.14834507293999194
Training loss on iteration 3920 = 0.13367168083786965
Training loss on iteration 3940 = 0.16806352995336055
Training loss on iteration 3960 = 0.15565605238080024
Training loss on iteration 3980 = 0.13631303012371063
Training loss on iteration 4000 = 0.17023303396999837
Training loss on iteration 4020 = 0.14656044282019137
Training loss on iteration 4040 = 0.15147001408040522
Training loss on iteration 4060 = 0.16696219891309738
Training loss on iteration 4080 = 0.17650034762918948
Training loss on iteration 4100 = 0.16299597024917603
Training loss on iteration 4120 = 0.16972274854779243
Training loss on iteration 4140 = 0.14489087276160717
Training loss on iteration 4160 = 0.16042533852159976
Training loss on iteration 4180 = 0.1362120147794485
Training loss on iteration 4200 = 0.1602314691990614
Training loss on iteration 4220 = 0.1394299276173115
Training loss on iteration 4240 = 0.1496275659650564
Training loss on iteration 4260 = 0.13909775018692017
Training loss on iteration 4280 = 0.1431874819099903
Training loss on iteration 4300 = 0.16399476267397403
Training loss on iteration 4320 = 0.14415775761008262
Training loss on iteration 4340 = 0.2292148567736149
Training loss on iteration 4360 = 0.18472131602466108
Training loss on iteration 4380 = 0.17202606685459615
Training loss on iteration 4400 = 0.15817667953670025
Training loss on iteration 4420 = 0.1677259188145399
Training loss on iteration 4440 = 0.17569729946553708
Training loss on iteration 4460 = 0.15653181672096253
Training loss on iteration 4480 = 0.15040031336247922
Training loss on iteration 4500 = 0.1733766883611679
Training loss on iteration 4520 = 0.1805390276014805
Training loss on iteration 4540 = 0.1564634270966053
Training loss on iteration 4560 = 0.1705984391272068
Training loss on iteration 4580 = 0.17514712549746037
Training loss on iteration 4600 = 0.1631667274981737
Training loss on iteration 4620 = 0.16217109598219395
Training loss on iteration 4640 = 0.14907086901366712
Training loss on iteration 4660 = 0.16029273867607116
Training loss on iteration 4680 = 0.33543052710592747
Training loss on iteration 4700 = 0.16045738570392132
Training loss on iteration 4720 = 0.1511319514364004
Training loss on iteration 4740 = 0.15243590921163558
Training loss on iteration 4760 = 0.15803040526807308
Training loss on iteration 4780 = 0.14616176560521127
Training loss on iteration 4800 = 0.18453883789479733
Training loss on iteration 4820 = 0.1457064963877201
Training loss on iteration 4840 = 0.1560148436576128
Training loss on iteration 4860 = 0.16368747353553773
Training loss on iteration 4880 = 0.16837840788066388
Training loss on iteration 4900 = 0.15861999206244945
Training loss on iteration 4920 = 0.16045426540076732
Training loss on iteration 4940 = 0.15640064068138598
Training loss on iteration 4960 = 0.15619967468082904
Training loss on iteration 4980 = 0.15824743658304213
Training loss on iteration 5000 = 0.14673995785415173
Training loss on iteration 5020 = 0.1619483344256878
Training loss on iteration 5040 = 0.16867576204240323
Training loss on iteration 5060 = 0.15404382161796093
Training loss on iteration 5080 = 0.17122645042836665
Training loss on iteration 5100 = 0.1570557627826929
Training loss on iteration 5120 = 0.16161604262888432
Training loss on iteration 5140 = 0.17711647525429725
Training loss on iteration 5160 = 0.16461792178452014
Training loss on iteration 5180 = 0.1490469302982092
Training loss on iteration 5200 = 0.16244293116033076
Training loss on iteration 5220 = 0.14817673079669474
Training loss on iteration 5240 = 0.17122558318078518
Training loss on iteration 5260 = 0.14003690369427205
Training loss on iteration 5280 = 0.17510968819260597
Training loss on iteration 5300 = 0.17297287993133068
Training loss on iteration 5320 = 0.16074745990335942
Training loss on iteration 5340 = 0.15289476066827773
Training loss on iteration 5360 = 0.15255908034741877
Training loss on iteration 5380 = 0.1574105754494667
Training loss on iteration 5400 = 0.16516092978417873
Training loss on iteration 5420 = 0.15473325699567794
Training loss on iteration 5440 = 0.16927129216492176
Training loss on iteration 5460 = 0.1556905310600996
Training loss on iteration 5480 = 0.17511910796165467
Training loss on iteration 5500 = 0.14372360333800316
Training loss on iteration 5520 = 0.15733846463263035
Training loss on iteration 5540 = 0.17773223482072353
Training loss on iteration 5560 = 0.15161012187600137
Training loss on iteration 5580 = 0.1591654283925891
Training loss on iteration 5600 = 0.1464561216533184
Training loss on iteration 5620 = 0.17569909878075124
Training loss on iteration 5640 = 0.2027496799826622
Training loss on iteration 5660 = 0.1853361278772354
Training loss on iteration 5680 = 0.16850015968084336
Training loss on iteration 5700 = 0.17802819311618806
Training loss on iteration 5720 = 0.17183415405452251
Training loss on iteration 5740 = 0.1487831711769104
Training loss on iteration 5760 = 0.15699516348540782
Training loss on iteration 5780 = 0.16310278065502642
Training loss on iteration 5800 = 0.16298752650618553
Training loss on iteration 5820 = 0.14517162777483464
Training loss on iteration 5840 = 0.1618821918964386
Training loss on iteration 5860 = 0.15350668281316757
Training loss on iteration 5880 = 0.15682062022387983
Training loss on iteration 5900 = 0.1747824352234602
Training loss on iteration 5920 = 0.17142610475420952
Training loss on iteration 5940 = 0.17488701343536378
Training loss on iteration 5960 = 0.16980714723467827
Training loss on iteration 5980 = 0.176127028465271
Training loss on iteration 6000 = 0.18142214491963388
Training loss on iteration 6020 = 0.1435869202017784
Training loss on iteration 6040 = 0.16114840097725391
Training loss on iteration 6060 = 0.17314223535358905
Training loss on iteration 6080 = 0.15195694603025914
Training loss on iteration 6100 = 0.16920477859675884
Training loss on iteration 6120 = 0.186061929538846
Training loss on iteration 6140 = 0.14964137822389603
Training loss on iteration 6160 = 0.16339769624173642
Training loss on iteration 6180 = 0.14613607600331308
Training loss on iteration 6200 = 0.15169329717755317
Training loss on iteration 6220 = 0.1550094898790121
Training loss on iteration 6240 = 0.15227716341614722
Training loss on iteration 6260 = 0.1285648848861456
Training loss on iteration 6280 = 0.14432862102985383
Training loss on iteration 6300 = 0.14741121381521224
Training loss on iteration 6320 = 0.17915777191519738
Training loss on iteration 6340 = 0.16520464569330215
Training loss on iteration 6360 = 0.1614862248301506
Training loss on iteration 6380 = 0.17835184670984744
Training loss on iteration 0 = 0.1779058277606964
Training loss on iteration 20 = 0.1361669685691595
Training loss on iteration 40 = 0.13489476591348648
Training loss on iteration 60 = 0.147693095728755
Training loss on iteration 80 = 0.1288563445210457
Training loss on iteration 100 = 0.1262666817754507
Training loss on iteration 120 = 0.12702683843672274
Training loss on iteration 140 = 0.1276342324912548
Training loss on iteration 160 = 0.12770689092576504
Training loss on iteration 180 = 0.1334653776139021
Training loss on iteration 200 = 0.1221458338201046
Training loss on iteration 220 = 0.12114401943981648
Training loss on iteration 240 = 0.1474791269749403
Training loss on iteration 260 = 0.1416625715792179
Training loss on iteration 280 = 0.1274417746812105
Training loss on iteration 300 = 0.14684716127812864
Training loss on iteration 320 = 0.12487744856625796
Training loss on iteration 340 = 0.13020061813294886
Training loss on iteration 360 = 0.13303835764527322
Training loss on iteration 380 = 0.11681786589324475
Training loss on iteration 400 = 0.1320431526750326
Training loss on iteration 420 = 0.11965331733226776
Training loss on iteration 440 = 0.12369548231363296
Training loss on iteration 460 = 0.12085803709924221
Training loss on iteration 480 = 0.12791304029524325
Training loss on iteration 500 = 0.1193152453750372
Training loss on iteration 520 = 0.11747639290988446
Training loss on iteration 540 = 0.11772855259478092
Training loss on iteration 560 = 0.13815463557839394
Training loss on iteration 580 = 0.1307380899786949
Training loss on iteration 600 = 0.13306666016578675
Training loss on iteration 620 = 0.12992452457547188
Training loss on iteration 640 = 0.11652426458895207
Training loss on iteration 660 = 0.12317141033709049
Training loss on iteration 680 = 0.12558374926447868
Training loss on iteration 700 = 0.11751017645001412
Training loss on iteration 720 = 0.12520419284701348
Training loss on iteration 740 = 0.12585074938833712
Training loss on iteration 760 = 0.12669806256890298
Training loss on iteration 780 = 0.126441977545619
Training loss on iteration 800 = 0.12472867965698242
Training loss on iteration 820 = 0.13986763693392276
Training loss on iteration 840 = 0.11891880836337805
Training loss on iteration 860 = 0.1237130794674158
Training loss on iteration 880 = 0.1346017960458994
Training loss on iteration 900 = 0.13488208837807178
Training loss on iteration 920 = 0.12075379565358162
Training loss on iteration 940 = 0.12078981027007103
Training loss on iteration 960 = 0.12996320463716984
Training loss on iteration 980 = 0.1274983048439026
Training loss on iteration 1000 = 0.1272769130766392
Training loss on iteration 1020 = 0.11877464577555656
Training loss on iteration 1040 = 0.12276540212333202
Training loss on iteration 1060 = 0.11861152164638042
Training loss on iteration 1080 = 0.12798001579940319
Training loss on iteration 1100 = 0.11992270685732365
Training loss on iteration 1120 = 0.11448708735406399
Training loss on iteration 1140 = 0.1528837013989687
Training loss on iteration 1160 = 0.11715298779308796
Training loss on iteration 1180 = 0.1461164005100727
Training loss on iteration 1200 = 0.12260937839746475
Training loss on iteration 1220 = 0.11728671714663505
Training loss on iteration 1240 = 0.11837595775723457
Training loss on iteration 1260 = 0.1256613876670599
Training loss on iteration 1280 = 0.11869249269366264
Training loss on iteration 1300 = 0.12320665083825588
Training loss on iteration 1320 = 0.13149694949388505
Training loss on iteration 1340 = 0.12956978306174277
Training loss on iteration 1360 = 0.13112785406410693
Training loss on iteration 1380 = 0.12312805093824863
Training loss on iteration 1400 = 0.17248167432844638
Training loss on iteration 1420 = 0.1338058512657881
Training loss on iteration 1440 = 0.12218579202890396
Training loss on iteration 1460 = 0.13539029136300088
Training loss on iteration 1480 = 0.12834036871790885
Training loss on iteration 1500 = 0.13508937656879424
Training loss on iteration 1520 = 0.13450110591948033
Training loss on iteration 1540 = 0.1296951115131378
Training loss on iteration 1560 = 0.13750935010612012
Training loss on iteration 1580 = 0.12240898795425892
Training loss on iteration 1600 = 0.11201408356428147
Training loss on iteration 1620 = 0.11945677399635315
Training loss on iteration 1640 = 0.1425919521600008
Training loss on iteration 1660 = 0.12656947933137416
Training loss on iteration 1680 = 0.12005388475954533
Training loss on iteration 1700 = 0.13580479547381402
Training loss on iteration 1720 = 0.13287352919578552
Training loss on iteration 1740 = 0.13196847699582576
Training loss on iteration 1760 = 0.12326191253960132
Training loss on iteration 1780 = 0.11713904663920402
Training loss on iteration 1800 = 0.1368858378380537
Training loss on iteration 1820 = 0.1272388484328985
Training loss on iteration 1840 = 0.1393958032131195
Training loss on iteration 1860 = 0.12684476673603057
Training loss on iteration 1880 = 0.13506214879453182
Training loss on iteration 1900 = 0.12824467495083808
Training loss on iteration 1920 = 0.12504554614424707
Training loss on iteration 1940 = 0.12565150558948518
Training loss on iteration 1960 = 0.12698945663869382
Training loss on iteration 1980 = 0.12650061920285224
Training loss on iteration 2000 = 0.1309880180284381
Training loss on iteration 2020 = 0.12801787331700326
Training loss on iteration 2040 = 0.13165548183023928
Training loss on iteration 2060 = 0.13704219348728658
Training loss on iteration 2080 = 0.1300976186990738
Training loss on iteration 2100 = 0.14070370756089687
Training loss on iteration 2120 = 0.13407904617488384
Training loss on iteration 2140 = 0.13492984920740128
Training loss on iteration 2160 = 0.12523541525006293
Training loss on iteration 2180 = 0.11950337253510952
Training loss on iteration 2200 = 0.13616797588765622
Training loss on iteration 2220 = 0.31407632417976855
Training loss on iteration 2240 = 0.12609319686889647
Training loss on iteration 2260 = 0.13236395753920077
Training loss on iteration 2280 = 0.12873273231089116
Training loss on iteration 2300 = 0.11932278051972389
Training loss on iteration 2320 = 0.13902760967612265
Training loss on iteration 2340 = 0.1349934607744217
Training loss on iteration 2360 = 0.1278903778642416
Training loss on iteration 2380 = 0.1292643751949072
Training loss on iteration 2400 = 0.12206220254302025
Training loss on iteration 2420 = 0.11815416626632214
Training loss on iteration 2440 = 0.1170178659260273
Training loss on iteration 2460 = 0.13000493235886096
Training loss on iteration 2480 = 0.13097797110676765
Training loss on iteration 2500 = 0.11856847032904624
Training loss on iteration 2520 = 0.12127701863646508
Training loss on iteration 2540 = 0.131128753721714
Training loss on iteration 2560 = 0.13753268159925938
Training loss on iteration 2580 = 0.1405644051730633
Training loss on iteration 2600 = 0.12459264136850834
Training loss on iteration 2620 = 0.12183998338878155
Training loss on iteration 2640 = 0.1293802697211504
Training loss on iteration 2660 = 0.16911188289523124
Training loss on iteration 2680 = 0.13444123342633246
Training loss on iteration 2700 = 0.12749845683574676
Training loss on iteration 2720 = 0.13755252845585347
Training loss on iteration 2740 = 0.12804024145007134
Training loss on iteration 2760 = 0.14023286290466785
Training loss on iteration 2780 = 0.12391898594796658
Training loss on iteration 2800 = 0.12923003919422626
Training loss on iteration 2820 = 0.14186562448740006
Training loss on iteration 2840 = 0.12559816800057888
Training loss on iteration 2860 = 0.12971208356320857
Training loss on iteration 2880 = 0.12577178440988063
Training loss on iteration 2900 = 0.13668095655739307
Training loss on iteration 2920 = 0.12964243218302726
Training loss on iteration 2940 = 0.1393458519130945
Training loss on iteration 2960 = 0.12982475087046624
Training loss on iteration 2980 = 0.1231166061013937
Training loss on iteration 3000 = 0.12487959899008275
Training loss on iteration 3020 = 0.12744596041738987
Training loss on iteration 3040 = 0.12924169488251208
Training loss on iteration 3060 = 0.14904148057103156
Training loss on iteration 3080 = 0.1389622174203396
Training loss on iteration 3100 = 0.12759337648749353
Training loss on iteration 3120 = 0.1218915343284607
Training loss on iteration 3140 = 0.1338847503066063
Training loss on iteration 3160 = 0.12201752476394176
Training loss on iteration 3180 = 0.13341475389897822
Training loss on iteration 3200 = 0.13628515601158142
Training loss on iteration 3220 = 0.1535902377218008
Training loss on iteration 3240 = 0.13073143027722836
Training loss on iteration 3260 = 0.12818847224116325
Training loss on iteration 3280 = 0.1335112161934376
Training loss on iteration 3300 = 0.12565105371177196
Training loss on iteration 3320 = 0.12390997894108295
Training loss on iteration 3340 = 0.13317934423685074
Training loss on iteration 3360 = 0.13740021400153637
Training loss on iteration 3380 = 0.12154488787055015
Training loss on iteration 3400 = 0.12413806021213532
Training loss on iteration 3420 = 0.13717317543923854
Training loss on iteration 3440 = 0.13525244276970624
Training loss on iteration 3460 = 0.11858072616159916
Training loss on iteration 3480 = 0.1409712765365839
Training loss on iteration 3500 = 0.13393434137105942
Training loss on iteration 3520 = 0.14266449473798276
Training loss on iteration 3540 = 0.133702776953578
Training loss on iteration 3560 = 0.1148176420480013
Training loss on iteration 3580 = 0.11880397200584411
Training loss on iteration 3600 = 0.13062586300075055
Training loss on iteration 3620 = 0.11523698233067989
Training loss on iteration 3640 = 0.12183841411024332
Training loss on iteration 3660 = 0.15555821172893047
Training loss on iteration 3680 = 0.12088672704994678
Training loss on iteration 3700 = 0.15330423898994922
Training loss on iteration 3720 = 0.11615358032286167
Training loss on iteration 3740 = 0.14010044001042843
Training loss on iteration 3760 = 0.1213559802621603
Training loss on iteration 3780 = 0.12242658510804176
Training loss on iteration 3800 = 0.12197935543954372
Training loss on iteration 3820 = 0.11893683895468712
Training loss on iteration 3840 = 0.1514045398682356
Training loss on iteration 3860 = 0.11519663073122502
Training loss on iteration 3880 = 0.12655617259442806
Training loss on iteration 3900 = 0.13277869671583176
Training loss on iteration 3920 = 0.12422104887664318
Training loss on iteration 3940 = 0.127662855014205
Training loss on iteration 3960 = 0.12268940340727567
Training loss on iteration 3980 = 0.13625186048448085
Training loss on iteration 4000 = 0.12794648073613643
Training loss on iteration 4020 = 0.13725480139255525
Training loss on iteration 4040 = 0.13054468892514706
Training loss on iteration 4060 = 0.12905424349009992
Training loss on iteration 4080 = 0.15593045130372046
Training loss on iteration 4100 = 0.1318681702017784
Training loss on iteration 4120 = 0.13135232739150524
Training loss on iteration 4140 = 0.1268367163836956
Training loss on iteration 4160 = 0.13479296788573264
Training loss on iteration 4180 = 0.13816392682492734
Training loss on iteration 4200 = 0.13415544629096984
Training loss on iteration 4220 = 0.12046642005443572
Training loss on iteration 4240 = 0.12975444160401822
Training loss on iteration 4260 = 0.12236710824072361
Training loss on iteration 4280 = 0.13337986990809442
Training loss on iteration 4300 = 0.1360742785036564
Training loss on iteration 4320 = 0.13391034342348576
Training loss on iteration 4340 = 0.12737925425171853
Training loss on iteration 4360 = 0.1407767690718174
Training loss on iteration 4380 = 0.1393572323024273
Training loss on iteration 4400 = 0.12282497324049473
Training loss on iteration 4420 = 0.12209114618599415
Training loss on iteration 4440 = 0.12988680563867092
Training loss on iteration 4460 = 0.12440749518573284
Training loss on iteration 4480 = 0.15843145027756692
Training loss on iteration 4500 = 0.14165144972503185
Training loss on iteration 4520 = 0.125707870349288
Training loss on iteration 4540 = 0.12056747078895569
Training loss on iteration 4560 = 0.13142746016383172
Training loss on iteration 4580 = 0.12252976447343826
Training loss on iteration 4600 = 0.1377524197101593
Training loss on iteration 4620 = 0.13047749251127244
Training loss on iteration 4640 = 0.12847077958285807
Training loss on iteration 4660 = 0.13836780302226542
Training loss on iteration 4680 = 0.13108911104500293
Training loss on iteration 4700 = 0.1568385601043701
Training loss on iteration 4720 = 0.14408017620444297
Training loss on iteration 4740 = 0.13475894778966904
Training loss on iteration 4760 = 0.1385106209665537
Training loss on iteration 4780 = 0.14687988795340062
Training loss on iteration 4800 = 0.19846977330744267
Training loss on iteration 4820 = 0.13156895916908978
Training loss on iteration 4840 = 0.14078372418880464
Training loss on iteration 4860 = 0.1251470498740673
Training loss on iteration 4880 = 0.13411067835986615
Training loss on iteration 4900 = 0.13503240384161472
Training loss on iteration 4920 = 0.16212968677282333
Training loss on iteration 4940 = 0.14605131298303603
Training loss on iteration 4960 = 0.1515626020729542
Training loss on iteration 4980 = 0.12547682859003545
Training loss on iteration 5000 = 0.12806191109120846
Training loss on iteration 5020 = 0.13593893647193908
Training loss on iteration 5040 = 0.1385430995374918
Training loss on iteration 5060 = 0.15300526060163974
Training loss on iteration 5080 = 0.12391269095242023
Training loss on iteration 5100 = 0.13113845251500605
Training loss on iteration 5120 = 0.14646891467273235
Training loss on iteration 5140 = 0.12916544564068316
Training loss on iteration 5160 = 0.13823035806417466
Training loss on iteration 5180 = 0.13256129808723927
Training loss on iteration 5200 = 0.1291903905570507
Training loss on iteration 5220 = 0.1416229248046875
Training loss on iteration 5240 = 0.12707797288894654
Training loss on iteration 5260 = 0.13283226937055587
Training loss on iteration 5280 = 0.140784958191216
Training loss on iteration 5300 = 0.12840841263532637
Training loss on iteration 5320 = 0.12671728990972042
Training loss on iteration 5340 = 0.1403281819075346
Training loss on iteration 5360 = 0.13647011630237102
Training loss on iteration 5380 = 0.1450939793139696
Training loss on iteration 5400 = 0.12762701213359834
Training loss on iteration 5420 = 0.13549748361110686
Training loss on iteration 5440 = 0.1305897120386362
Training loss on iteration 5460 = 0.13138615898787975
Training loss on iteration 5480 = 0.12954095825552941
Training loss on iteration 5500 = 0.1496938720345497
Training loss on iteration 5520 = 0.12834715247154235
Training loss on iteration 5540 = 0.13052226826548577
Training loss on iteration 5560 = 0.1288527339696884
Training loss on iteration 5580 = 0.11959590241312981
Training loss on iteration 5600 = 0.27296561896800997
Training loss on iteration 5620 = 0.13566427268087863
Training loss on iteration 5640 = 0.13172315396368503
Training loss on iteration 5660 = 0.13083655275404454
Training loss on iteration 5680 = 0.13745481818914412
Training loss on iteration 5700 = 0.13563732542097567
Training loss on iteration 5720 = 0.12362795248627663
Training loss on iteration 5740 = 0.14723549596965313
Training loss on iteration 5760 = 0.12408053539693356
Training loss on iteration 5780 = 0.1282382935285568
Training loss on iteration 5800 = 0.1301150444895029
Training loss on iteration 5820 = 0.1340711358934641
Training loss on iteration 5840 = 0.11768163628876209
Training loss on iteration 5860 = 0.1289126068353653
Training loss on iteration 5880 = 0.12140989750623703
Training loss on iteration 5900 = 0.14100766479969024
Training loss on iteration 5920 = 0.13630195781588555
Training loss on iteration 5940 = 0.14681992828845977
Training loss on iteration 5960 = 0.14006885550916195
Training loss on iteration 5980 = 0.15184396132826805
Training loss on iteration 6000 = 0.12475905977189541
Training loss on iteration 6020 = 0.1490095254033804
Training loss on iteration 6040 = 0.17537995502352716
Training loss on iteration 6060 = 0.14672350734472275
Training loss on iteration 6080 = 0.13897822387516498
Training loss on iteration 6100 = 0.14365149214863776
Training loss on iteration 6120 = 0.1422870632261038
Training loss on iteration 6140 = 0.13383863717317582
Training loss on iteration 6160 = 0.12469853200018406
Training loss on iteration 6180 = 0.12774025909602643
Training loss on iteration 6200 = 0.12375906892120839
Training loss on iteration 6220 = 0.13909814469516277
Training loss on iteration 6240 = 0.13365305364131927
Training loss on iteration 6260 = 0.13672797977924347
Training loss on iteration 6280 = 0.15180333629250525
Training loss on iteration 6300 = 0.15445608720183374
Training loss on iteration 6320 = 0.13443004488945007
Training loss on iteration 6340 = 0.12749843671917915
Training loss on iteration 6360 = 0.1352894440293312
Training loss on iteration 6380 = 0.13321898132562637
Training loss on iteration 0 = 0.13763262331485748
Training loss on iteration 20 = 0.15565369985997676
Training loss on iteration 40 = 0.11827587932348252
Training loss on iteration 60 = 0.11710067428648471
Training loss on iteration 80 = 0.1130405280739069
Training loss on iteration 100 = 0.11459594536572695
Training loss on iteration 120 = 0.11485427692532539
Training loss on iteration 140 = 0.11414029821753502
Training loss on iteration 160 = 0.12674304358661176
Training loss on iteration 180 = 0.15756042748689653
Training loss on iteration 200 = 0.12610545195639133
Training loss on iteration 220 = 0.12093765623867511
Training loss on iteration 240 = 0.10612917654216289
Training loss on iteration 260 = 0.10417827628552914
Training loss on iteration 280 = 0.13052638955414295
Training loss on iteration 300 = 0.12177972346544266
Training loss on iteration 320 = 0.12038708552718162
Training loss on iteration 340 = 0.12332293763756752
Training loss on iteration 360 = 0.11915404088795185
Training loss on iteration 380 = 0.13481547571718694
Training loss on iteration 400 = 0.1159933514893055
Training loss on iteration 420 = 0.12390802763402461
Training loss on iteration 440 = 0.1254237860441208
Training loss on iteration 460 = 0.1281701847910881
Training loss on iteration 480 = 0.10841958001255989
Training loss on iteration 500 = 0.12130613438785076
Training loss on iteration 520 = 0.12657618392258882
Training loss on iteration 540 = 0.10842507928609849
Training loss on iteration 560 = 0.1249623242765665
Training loss on iteration 580 = 0.11803805679082871
Training loss on iteration 600 = 0.105344707518816
Training loss on iteration 620 = 0.11250843405723572
Training loss on iteration 640 = 0.12082096450030803
Training loss on iteration 660 = 0.12184991873800755
Training loss on iteration 680 = 0.1291380375623703
Training loss on iteration 700 = 0.11391311809420586
Training loss on iteration 720 = 0.1271063681691885
Training loss on iteration 740 = 0.13164253048598767
Training loss on iteration 760 = 0.11645581759512424
Training loss on iteration 780 = 0.11483876556158065
Training loss on iteration 800 = 0.11866925321519375
Training loss on iteration 820 = 0.11381411664187908
Training loss on iteration 840 = 0.11477798242121935
Training loss on iteration 860 = 0.11980492696166038
Training loss on iteration 880 = 0.10889333747327327
Training loss on iteration 900 = 0.122841134108603
Training loss on iteration 920 = 0.12420241795480251
Training loss on iteration 940 = 0.11222686655819417
Training loss on iteration 960 = 0.11620169021189213
Training loss on iteration 980 = 0.13918216452002524
Training loss on iteration 1000 = 0.1275118485093117
Training loss on iteration 1020 = 0.13052070885896683
Training loss on iteration 1040 = 0.10403033010661603
Training loss on iteration 1060 = 0.1334093026816845
Training loss on iteration 1080 = 0.11860594861209392
Training loss on iteration 1100 = 0.13985205478966237
Training loss on iteration 1120 = 0.12342522405087948
Training loss on iteration 1140 = 0.11321088261902332
Training loss on iteration 1160 = 0.1133701216429472
Training loss on iteration 1180 = 0.12294555380940438
Training loss on iteration 1200 = 0.12432992085814476
Training loss on iteration 1220 = 0.12345872856676579
Training loss on iteration 1240 = 0.13550868704915048
Training loss on iteration 1260 = 0.12175421975553036
Training loss on iteration 1280 = 0.12436959519982338
Training loss on iteration 1300 = 0.12592632956802846
Training loss on iteration 1320 = 0.11228709220886231
Training loss on iteration 1340 = 0.11912698149681092
Training loss on iteration 1360 = 0.11539265569299459
Training loss on iteration 1380 = 0.12256616428494453
Training loss on iteration 1400 = 0.12566580809652805
Training loss on iteration 1420 = 0.11726388521492481
Training loss on iteration 1440 = 0.1225585587322712
Training loss on iteration 1460 = 0.1238632183521986
Training loss on iteration 1480 = 0.1155099630355835
Training loss on iteration 1500 = 0.1279583040624857
Training loss on iteration 1520 = 0.12036175653338432
Training loss on iteration 1540 = 0.12457819990813732
Training loss on iteration 1560 = 0.10880806148052216
Training loss on iteration 1580 = 0.1258133241906762
Training loss on iteration 1600 = 0.13156917169690133
Training loss on iteration 1620 = 0.11848052963614464
Training loss on iteration 1640 = 0.11816003136336803
Training loss on iteration 1660 = 0.11741234511137008
Training loss on iteration 1680 = 0.12085434217005968
Training loss on iteration 1700 = 0.11091801263391972
Training loss on iteration 1720 = 0.12385104969143867
Training loss on iteration 1740 = 0.12086090110242367
Training loss on iteration 1760 = 0.11244976557791234
Training loss on iteration 1780 = 0.11416315361857414
Training loss on iteration 1800 = 0.11106450669467449
Training loss on iteration 1820 = 0.11948277167975903
Training loss on iteration 1840 = 0.13039394579827784
Training loss on iteration 1860 = 0.11923232823610305
Training loss on iteration 1880 = 0.11632955558598042
Training loss on iteration 1900 = 0.1200675342231989
Training loss on iteration 1920 = 0.11652495749294758
Training loss on iteration 1940 = 0.13398028984665872
Training loss on iteration 1960 = 0.12079907730221748
Training loss on iteration 1980 = 0.12205312177538871
Training loss on iteration 2000 = 0.1292060635983944
Training loss on iteration 2020 = 0.11146968081593514
Training loss on iteration 2040 = 0.11918277367949486
Training loss on iteration 2060 = 0.11463628150522709
Training loss on iteration 2080 = 0.1272503398358822
Training loss on iteration 2100 = 0.12087877579033375
Training loss on iteration 2120 = 0.13038830701261758
Training loss on iteration 2140 = 0.13111387398093938
Training loss on iteration 2160 = 0.1226679977029562
Training loss on iteration 2180 = 0.12061781287193299
Training loss on iteration 2200 = 0.11271146349608899
Training loss on iteration 2220 = 0.13389688953757287
Training loss on iteration 2240 = 0.12848841920495033
Training loss on iteration 2260 = 0.1164631150662899
Training loss on iteration 2280 = 0.12107707858085633
Training loss on iteration 2300 = 0.12945705205202102
Training loss on iteration 2320 = 0.12654379196465015
Training loss on iteration 2340 = 0.14331198967993258
Training loss on iteration 2360 = 0.12690637186169623
Training loss on iteration 2380 = 0.12824046909809111
Training loss on iteration 2400 = 0.1260882645845413
Training loss on iteration 2420 = 0.14442008472979068
Training loss on iteration 2440 = 0.11180993393063546
Training loss on iteration 2460 = 0.11780380047857761
Training loss on iteration 2480 = 0.14113255701959132
Training loss on iteration 2500 = 0.12594029642641544
Training loss on iteration 2520 = 0.1179707482457161
Training loss on iteration 2540 = 0.11643131598830223
Training loss on iteration 2560 = 0.1271661203354597
Training loss on iteration 2580 = 0.13012208491563798
Training loss on iteration 2600 = 0.12485152930021286
Training loss on iteration 2620 = 0.12199810929596425
Training loss on iteration 2640 = 0.1220263984054327
Training loss on iteration 2660 = 0.1386455874890089
Training loss on iteration 2680 = 0.11588787101209164
Training loss on iteration 2700 = 0.12071311697363854
Training loss on iteration 2720 = 0.12884214259684085
Training loss on iteration 2740 = 0.12332191318273544
Training loss on iteration 2760 = 0.12341545522212982
Training loss on iteration 2780 = 0.11824267953634263
Training loss on iteration 2800 = 0.16873574145138265
Training loss on iteration 2820 = 0.12468755804002285
Training loss on iteration 2840 = 0.16064886264503003
Training loss on iteration 2860 = 0.13111030012369157
Training loss on iteration 2880 = 0.10820580050349235
Training loss on iteration 2900 = 0.12828553132712842
Training loss on iteration 2920 = 0.12958804257214068
Training loss on iteration 2940 = 0.13446121327579022
Training loss on iteration 2960 = 0.8227212581783533
Training loss on iteration 2980 = 0.12753633596003056
Training loss on iteration 3000 = 0.15108483768999575
Training loss on iteration 3020 = 0.13313973508775234
Training loss on iteration 3040 = 0.12928485870361328
Training loss on iteration 3060 = 0.12123185880482197
Training loss on iteration 3080 = 0.12805699482560157
Training loss on iteration 3100 = 0.1331872921437025
Training loss on iteration 3120 = 0.1252701211720705
Training loss on iteration 3140 = 0.14360277354717255
Training loss on iteration 3160 = 0.13339654877781867
Training loss on iteration 3180 = 0.13271577842533588
Training loss on iteration 3200 = 0.14173334948718547
Training loss on iteration 3220 = 0.13873071521520614
Training loss on iteration 3240 = 0.12497847080230713
Training loss on iteration 3260 = 0.12213019952178002
Training loss on iteration 3280 = 0.135154964402318
Training loss on iteration 3300 = 0.12383678294718266
Training loss on iteration 3320 = 0.12806774862110615
Training loss on iteration 3340 = 0.10825811773538589
Training loss on iteration 3360 = 0.12079249285161495
Training loss on iteration 3380 = 0.12229961715638638
Training loss on iteration 3400 = 0.140990786626935
Training loss on iteration 3420 = 0.11427673250436783
Training loss on iteration 3440 = 0.12756604626774787
Training loss on iteration 3460 = 0.11863867528736591
Training loss on iteration 3480 = 0.11389573328197003
Training loss on iteration 3500 = 0.12681874223053455
Training loss on iteration 3520 = 0.11622951962053776
Training loss on iteration 3540 = 0.11794393546879292
Training loss on iteration 3560 = 0.13014515656977893
Training loss on iteration 3580 = 0.112777797318995
Training loss on iteration 3600 = 0.11651615798473358
Training loss on iteration 3620 = 0.13655170761048793
Training loss on iteration 3640 = 0.1290796149522066
Training loss on iteration 3660 = 0.13138059452176093
Training loss on iteration 3680 = 0.12946887724101544
Training loss on iteration 3700 = 0.1290000665932894
Training loss on iteration 3720 = 0.1225128136575222
Training loss on iteration 3740 = 0.11867942698299885
Training loss on iteration 3760 = 0.13231166452169418
Training loss on iteration 3780 = 0.13392710089683532
Training loss on iteration 3800 = 0.13361962437629699
Training loss on iteration 3820 = 0.14554050229489804
Training loss on iteration 3840 = 0.12637512423098088
Training loss on iteration 3860 = 0.2976580340415239
Training loss on iteration 3880 = 0.126400363817811
Training loss on iteration 3900 = 0.13082752451300622
Training loss on iteration 3920 = 0.1235475029796362
Training loss on iteration 3940 = 0.12117411717772483
Training loss on iteration 3960 = 0.13010139837861062
Training loss on iteration 3980 = 0.13033746015280484
Training loss on iteration 4000 = 0.12148803174495697
Training loss on iteration 4020 = 0.12596244141459464
Training loss on iteration 4040 = 0.1257760114967823
Training loss on iteration 4060 = 0.12233403064310551
Training loss on iteration 4080 = 0.12621812336146832
Training loss on iteration 4100 = 0.10717952847480774
Training loss on iteration 4120 = 0.11835581250488758
Training loss on iteration 4140 = 0.13363591581583023
Training loss on iteration 4160 = 0.12211184166371822
Training loss on iteration 4180 = 0.11430563926696777
Training loss on iteration 4200 = 0.14065846540033816
Training loss on iteration 4220 = 0.12059728577733039
Training loss on iteration 4240 = 0.11422115080058574
Training loss on iteration 4260 = 0.1341966785490513
Training loss on iteration 4280 = 0.13442171812057496
Training loss on iteration 4300 = 0.11678486056625843
Training loss on iteration 4320 = 0.11315814368426799
Training loss on iteration 4340 = 0.11099111810326576
Training loss on iteration 4360 = 0.1281388361006975
Training loss on iteration 4380 = 0.13668221831321717
Training loss on iteration 4400 = 0.12861686609685422
Training loss on iteration 4420 = 0.12804011739790438
Training loss on iteration 4440 = 0.13400181867182254
Training loss on iteration 4460 = 0.13720779903233052
Training loss on iteration 4480 = 0.12668441683053971
Training loss on iteration 4500 = 0.11812926307320595
Training loss on iteration 4520 = 0.13247827999293804
Training loss on iteration 4540 = 0.11832977272570133
Training loss on iteration 4560 = 0.12698548138141633
Training loss on iteration 4580 = 0.1118416354060173
Training loss on iteration 4600 = 0.11696089170873165
Training loss on iteration 4620 = 0.1289963774383068
Training loss on iteration 4640 = 0.12047086954116822
Training loss on iteration 4660 = 0.1381584919989109
Training loss on iteration 4680 = 0.1310281626880169
Training loss on iteration 4700 = 0.12615730687975885
Training loss on iteration 4720 = 0.14172131642699243
Training loss on iteration 4740 = 0.15575074385851623
Training loss on iteration 4760 = 0.1140218561515212
Training loss on iteration 4780 = 0.1420114278793335
Training loss on iteration 4800 = 0.12300261408090592
Training loss on iteration 4820 = 0.11419910565018654
Training loss on iteration 4840 = 0.11898048557341098
Training loss on iteration 4860 = 0.1278469070792198
Training loss on iteration 4880 = 0.12170909903943539
Training loss on iteration 4900 = 0.12380188703536987
Training loss on iteration 4920 = 0.11135638616979122
Training loss on iteration 4940 = 0.11774293668568134
Training loss on iteration 4960 = 0.12401502244174481
Training loss on iteration 4980 = 0.13007396832108498
Training loss on iteration 5000 = 0.13324485383927823
Training loss on iteration 5020 = 0.1269334562122822
Training loss on iteration 5040 = 0.1444966860115528
Training loss on iteration 5060 = 0.11656089648604392
Training loss on iteration 5080 = 0.11795898452401161
Training loss on iteration 5100 = 0.1409008540213108
Training loss on iteration 5120 = 0.11493596881628036
Training loss on iteration 5140 = 0.1345787275582552
Training loss on iteration 5160 = 0.13553415648639203
Training loss on iteration 5180 = 0.12613091431558132
Training loss on iteration 5200 = 0.2845755461603403
Training loss on iteration 5220 = 0.12329311929643154
Training loss on iteration 5240 = 0.11988944709300994
Training loss on iteration 5260 = 0.12795491926372052
Training loss on iteration 5280 = 0.1279181569814682
Training loss on iteration 5300 = 0.12066251412034035
Training loss on iteration 5320 = 0.1880313403904438
Training loss on iteration 5340 = 0.12948461808264256
Training loss on iteration 5360 = 0.11757265366613864
Training loss on iteration 5380 = 0.12692573554813863
Training loss on iteration 5400 = 0.1498986903578043
Training loss on iteration 5420 = 0.13006930723786353
Training loss on iteration 5440 = 0.11886272430419922
Training loss on iteration 5460 = 0.1541977945715189
Training loss on iteration 5480 = 0.12956277579069136
Training loss on iteration 5500 = 0.13210573866963388
Training loss on iteration 5520 = 0.14708425626158714
Training loss on iteration 5540 = 0.11985147260129451
Training loss on iteration 5560 = 0.12921141237020492
Training loss on iteration 5580 = 0.12320666499435902
Training loss on iteration 5600 = 0.1200396042317152
Training loss on iteration 5620 = 0.12483611144125462
Training loss on iteration 5640 = 0.1387538779526949
Training loss on iteration 5660 = 0.11873612478375435
Training loss on iteration 5680 = 0.12781671509146691
Training loss on iteration 5700 = 0.1396468285471201
Training loss on iteration 5720 = 0.13655866794288157
Training loss on iteration 5740 = 0.13288161903619766
Training loss on iteration 5760 = 0.12609987407922746
Training loss on iteration 5780 = 0.14578709602355958
Training loss on iteration 5800 = 0.12119010388851166
Training loss on iteration 5820 = 0.127686445787549
Training loss on iteration 5840 = 0.120579319819808
Training loss on iteration 5860 = 0.12376358807086944
Training loss on iteration 5880 = 0.1267294753342867
Training loss on iteration 5900 = 0.12027061358094215
Training loss on iteration 5920 = 0.1456375528126955
Training loss on iteration 5940 = 0.12476554103195667
Training loss on iteration 5960 = 0.13129949122667312
Training loss on iteration 5980 = 0.13126407600939274
Training loss on iteration 6000 = 0.12505206204950808
Training loss on iteration 6020 = 0.13036193903535603
Training loss on iteration 6040 = 0.11336718313395977
Training loss on iteration 6060 = 0.12456589676439762
Training loss on iteration 6080 = 0.1504468373954296
Training loss on iteration 6100 = 0.12753140665590762
Training loss on iteration 6120 = 0.13031071648001671
Training loss on iteration 6140 = 0.11984159611165524
Training loss on iteration 6160 = 0.12983637675642967
Training loss on iteration 6180 = 0.1425630073994398
Training loss on iteration 6200 = 0.12036020457744598
Training loss on iteration 6220 = 0.13615000918507575
Training loss on iteration 6240 = 0.12092458382248879
Training loss on iteration 6260 = 0.12635667771100997
Training loss on iteration 6280 = 0.14998551458120346
Training loss on iteration 6300 = 0.13562996536493302
Training loss on iteration 6320 = 0.12639988772571087
Training loss on iteration 6340 = 0.12931543067097664
Training loss on iteration 6360 = 0.15860870257019996
Training loss on iteration 6380 = 0.12234978303313256
Training loss on iteration 0 = 0.10844454914331436
Training loss on iteration 20 = 0.11273867003619671
Training loss on iteration 40 = 0.11908550038933755
Training loss on iteration 60 = 0.14391958229243756
Training loss on iteration 80 = 0.1365004327148199
Training loss on iteration 100 = 0.11096189469099045
Training loss on iteration 120 = 0.11576673798263074
Training loss on iteration 140 = 0.12292680591344833
Training loss on iteration 160 = 0.11475725062191486
Training loss on iteration 180 = 0.1087198868393898
Training loss on iteration 200 = 0.11658336855471134
Training loss on iteration 220 = 0.1252307068556547
Training loss on iteration 240 = 0.12240958437323571
Training loss on iteration 260 = 0.2759974071756005
Training loss on iteration 280 = 0.11524694934487342
Training loss on iteration 300 = 0.1125815462321043
Training loss on iteration 320 = 0.10309068597853184
Training loss on iteration 340 = 0.11059676297008991
Training loss on iteration 360 = 0.10743589755147695
Training loss on iteration 380 = 0.1129394829273224
Training loss on iteration 400 = 0.11424040757119655
Training loss on iteration 420 = 0.10963343605399131
Training loss on iteration 440 = 0.10998615026473998
Training loss on iteration 460 = 0.1107021164149046
Training loss on iteration 480 = 0.1076675619930029
Training loss on iteration 500 = 0.10478909723460675
Training loss on iteration 520 = 0.11991238035261631
Training loss on iteration 540 = 0.11020524129271507
Training loss on iteration 560 = 0.11070313677191734
Training loss on iteration 580 = 0.12247875481843948
Training loss on iteration 600 = 0.12240914851427079
Training loss on iteration 620 = 0.11853400208055972
Training loss on iteration 640 = 0.1058654073625803
Training loss on iteration 660 = 0.12616304382681848
Training loss on iteration 680 = 0.11818190775811672
Training loss on iteration 700 = 0.11226773113012314
Training loss on iteration 720 = 0.11293779090046882
Training loss on iteration 740 = 0.11911001205444335
Training loss on iteration 760 = 0.11208556853234768
Training loss on iteration 780 = 0.11293099038302898
Training loss on iteration 800 = 0.1146179348230362
Training loss on iteration 820 = 0.10979862250387669
Training loss on iteration 840 = 0.1065859816968441
Training loss on iteration 860 = 0.11836458630859852
Training loss on iteration 880 = 0.10707395300269126
Training loss on iteration 900 = 0.10652757231146097
Training loss on iteration 920 = 0.1262015514075756
Training loss on iteration 940 = 0.11737397238612175
Training loss on iteration 960 = 0.12069537527859212
Training loss on iteration 980 = 0.11266721449792386
Training loss on iteration 1000 = 0.12130070887506009
Training loss on iteration 1020 = 0.10102507062256336
Training loss on iteration 1040 = 0.11605066992342472
Training loss on iteration 1060 = 0.1746062498539686
Training loss on iteration 1080 = 0.11713574342429638
Training loss on iteration 1100 = 0.11732566244900226
Training loss on iteration 1120 = 0.11872194260358811
Training loss on iteration 1140 = 0.11066844649612903
Training loss on iteration 1160 = 0.12151311002671719
Training loss on iteration 1180 = 0.29176836013793944
Training loss on iteration 1200 = 0.12097401544451714
Training loss on iteration 1220 = 0.11255277432501316
Training loss on iteration 1240 = 0.11337016336619854
Training loss on iteration 1260 = 0.10051660165190697
Training loss on iteration 1280 = 0.12098786123096943
Training loss on iteration 1300 = 0.100457114726305
Training loss on iteration 1320 = 0.11182982623577117
Training loss on iteration 1340 = 0.13199273385107518
Training loss on iteration 1360 = 0.11649122387170792
Training loss on iteration 1380 = 0.13438779935240747
Training loss on iteration 1400 = 0.11569382287561894
Training loss on iteration 1420 = 0.1165515124797821
Training loss on iteration 1440 = 0.10782168991863728
Training loss on iteration 1460 = 0.11455130614340306
Training loss on iteration 1480 = 0.12818313464522363
Training loss on iteration 1500 = 0.12416710592806339
Training loss on iteration 1520 = 0.11466693095862865
Training loss on iteration 1540 = 0.11229277066886426
Training loss on iteration 1560 = 0.1086499910801649
Training loss on iteration 1580 = 0.12019283138215542
Training loss on iteration 1600 = 0.11958697773516178
Training loss on iteration 1620 = 0.11399510875344276
Training loss on iteration 1640 = 0.11465223245322705
Training loss on iteration 1660 = 0.11924056112766265
Training loss on iteration 1680 = 0.11766357831656933
Training loss on iteration 1700 = 0.13171280324459075
Training loss on iteration 1720 = 0.12550762128084897
Training loss on iteration 1740 = 0.1292331699281931
Training loss on iteration 1760 = 0.10697523150593043
Training loss on iteration 1780 = 0.10279103480279446
Training loss on iteration 1800 = 0.12163315117359161
Training loss on iteration 1820 = 0.11009120307862759
Training loss on iteration 1840 = 0.11142920255661011
Training loss on iteration 1860 = 0.11606895960867405
Training loss on iteration 1880 = 0.1191805899143219
Training loss on iteration 1900 = 0.12740418277680873
Training loss on iteration 1920 = 0.1218751236796379
Training loss on iteration 1940 = 0.12616168856620788
Training loss on iteration 1960 = 0.11320149675011634
Training loss on iteration 1980 = 0.12616344578564168
Training loss on iteration 2000 = 0.1178425069898367
Training loss on iteration 2020 = 0.11356403082609176
Training loss on iteration 2040 = 0.1199923112988472
Training loss on iteration 2060 = 0.11403107792139053
Training loss on iteration 2080 = 0.11315740812569856
Training loss on iteration 2100 = 0.12431522533297538
Training loss on iteration 2120 = 0.12391960881650448
Training loss on iteration 2140 = 0.12530308552086353
Training loss on iteration 2160 = 0.12115456610918045
Training loss on iteration 2180 = 0.13115674406290054
Training loss on iteration 2200 = 0.12997762858867645
Training loss on iteration 2220 = 0.12133064791560173
Training loss on iteration 2240 = 0.12487017028033734
Training loss on iteration 2260 = 0.12245710715651512
Training loss on iteration 2280 = 0.11630731858313084
Training loss on iteration 2300 = 0.11742859669029712
Training loss on iteration 2320 = 0.11500841677188874
Training loss on iteration 2340 = 0.1335726145654917
Training loss on iteration 2360 = 0.11748181171715259
Training loss on iteration 2380 = 0.1139281690120697
Training loss on iteration 2400 = 0.12977513633668422
Training loss on iteration 2420 = 0.11286428645253181
Training loss on iteration 2440 = 0.1148933406919241
Training loss on iteration 2460 = 0.11890134811401368
Training loss on iteration 2480 = 0.11126778833568096
Training loss on iteration 2500 = 0.10399786867201329
Training loss on iteration 2520 = 0.10238925889134406
Training loss on iteration 2540 = 0.11134798042476177
Training loss on iteration 2560 = 0.11087546274065971
Training loss on iteration 2580 = 0.10004987567663193
Training loss on iteration 2600 = 0.11391414757817983
Training loss on iteration 2620 = 0.12179067768156529
Training loss on iteration 2640 = 0.11778946071863175
Training loss on iteration 2660 = 0.11634689159691333
Training loss on iteration 2680 = 0.12259467467665672
Training loss on iteration 2700 = 0.12929616570472718
Training loss on iteration 2720 = 0.11952216736972332
Training loss on iteration 2740 = 0.12921788357198238
Training loss on iteration 2760 = 0.12299400903284549
Training loss on iteration 2780 = 0.12949234582483768
Training loss on iteration 2800 = 0.13504654429852964
Training loss on iteration 2820 = 0.11878283806145191
Training loss on iteration 2840 = 0.12540641874074937
Training loss on iteration 2860 = 0.13127459324896334
Training loss on iteration 2880 = 0.1180061750113964
Training loss on iteration 2900 = 0.1331572562456131
Training loss on iteration 2920 = 0.10937014818191529
Training loss on iteration 2940 = 0.10977066494524479
Training loss on iteration 2960 = 0.12105441987514495
Training loss on iteration 2980 = 0.12680585645139217
Training loss on iteration 3000 = 0.11764748506247998
Training loss on iteration 3020 = 0.10793719105422497
Training loss on iteration 3040 = 0.11190599910914897
Training loss on iteration 3060 = 0.11481357142329215
Training loss on iteration 3080 = 0.1078412376344204
Training loss on iteration 3100 = 0.1147685132920742
Training loss on iteration 3120 = 0.11347279436886311
Training loss on iteration 3140 = 0.12892282865941523
Training loss on iteration 3160 = 0.13187182359397412
Training loss on iteration 3180 = 0.11816098354756832
Training loss on iteration 3200 = 0.1872634243220091
Training loss on iteration 3220 = 0.11376515552401542
Training loss on iteration 3240 = 0.14128432460129262
Training loss on iteration 3260 = 0.11404521614313126
Training loss on iteration 3280 = 0.14444707948714494
Training loss on iteration 3300 = 0.12215301655232906
Training loss on iteration 3320 = 0.11395113337785005
Training loss on iteration 3340 = 0.1372897058725357
Training loss on iteration 3360 = 0.13456246294081212
Training loss on iteration 3380 = 0.10934972427785397
Training loss on iteration 3400 = 0.11064854934811592
Training loss on iteration 3420 = 0.13732145726680756
Training loss on iteration 3440 = 0.12308706529438496
Training loss on iteration 3460 = 0.12519251815974713
Training loss on iteration 3480 = 0.12430640757083893
Training loss on iteration 3500 = 0.11050163246691228
Training loss on iteration 3520 = 0.12750407345592976
Training loss on iteration 3540 = 0.13124192990362643
Training loss on iteration 3560 = 0.11522364541888237
Training loss on iteration 3580 = 0.12910359539091587
Training loss on iteration 3600 = 0.11825604140758514
Training loss on iteration 3620 = 0.11563827209174633
Training loss on iteration 3640 = 0.11615345478057862
Training loss on iteration 3660 = 0.13607596643269063
Training loss on iteration 3680 = 0.13294161707162858
Training loss on iteration 3700 = 0.15358483418822289
Training loss on iteration 3720 = 0.11132356561720372
Training loss on iteration 3740 = 0.12647594287991523
Training loss on iteration 3760 = 0.11571940705180168
Training loss on iteration 3780 = 0.13315797373652458
Training loss on iteration 3800 = 0.12746522910892963
Training loss on iteration 3820 = 0.1051254078745842
Training loss on iteration 3840 = 0.13861194625496864
Training loss on iteration 3860 = 0.13420029617846013
Training loss on iteration 3880 = 0.13128549084067345
Training loss on iteration 3900 = 0.11113130822777748
Training loss on iteration 3920 = 0.12313910983502865
Training loss on iteration 3940 = 0.12421634700149298
Training loss on iteration 3960 = 0.12747624479234218
Training loss on iteration 3980 = 0.11925637684762477
Training loss on iteration 4000 = 0.11640653274953365
Training loss on iteration 4020 = 0.14607945568859576
Training loss on iteration 4040 = 0.12910341732203962
Training loss on iteration 4060 = 0.12041838318109513
Training loss on iteration 4080 = 0.11749550774693489
Training loss on iteration 4100 = 0.11112145669758319
Training loss on iteration 4120 = 0.12161911018192768
Training loss on iteration 4140 = 0.16222308166325092
Training loss on iteration 4160 = 0.12221477590501309
Training loss on iteration 4180 = 0.12398389391601086
Training loss on iteration 4200 = 0.11846644133329391
Training loss on iteration 4220 = 0.12176879085600376
Training loss on iteration 4240 = 0.11384974755346774
Training loss on iteration 4260 = 0.12954900674521924
Training loss on iteration 4280 = 0.13384988345205784
Training loss on iteration 4300 = 0.11897344682365656
Training loss on iteration 4320 = 0.11902929097414017
Training loss on iteration 4340 = 0.12111157961189747
Training loss on iteration 4360 = 0.1291797559708357
Training loss on iteration 4380 = 0.14795721955597402
Training loss on iteration 4400 = 0.12345871441066265
Training loss on iteration 4420 = 0.132194222509861
Training loss on iteration 4440 = 0.1264643080532551
Training loss on iteration 4460 = 0.12465635947883129
Training loss on iteration 4480 = 0.13333879224956036
Training loss on iteration 4500 = 0.1227075695991516
Training loss on iteration 4520 = 0.11331055276095867
Training loss on iteration 4540 = 0.1314676895737648
Training loss on iteration 4560 = 0.12091110423207282
Training loss on iteration 4580 = 0.11192608997225761
Training loss on iteration 4600 = 0.11586086824536324
Training loss on iteration 4620 = 0.12605400756001472
Training loss on iteration 4640 = 0.13655903562903404
Training loss on iteration 4660 = 0.12846476659178735
Training loss on iteration 4680 = 0.124731370434165
Training loss on iteration 4700 = 0.13126962557435035
Training loss on iteration 4720 = 0.1233637697994709
Training loss on iteration 4740 = 0.11554819829761982
Training loss on iteration 4760 = 0.12832084968686103
Training loss on iteration 4780 = 0.12752729132771493
Training loss on iteration 4800 = 0.16685267984867097
Training loss on iteration 4820 = 0.12416717410087585
Training loss on iteration 4840 = 0.12194554172456265
Training loss on iteration 4860 = 0.11447964645922185
Training loss on iteration 4880 = 0.11830116249620914
Training loss on iteration 4900 = 0.1245710700750351
Training loss on iteration 4920 = 0.1278286948800087
Training loss on iteration 4940 = 0.13249895051121713
Training loss on iteration 4960 = 0.1183862566947937
Training loss on iteration 4980 = 0.12768863327801228
Training loss on iteration 5000 = 0.11854804903268815
Training loss on iteration 5020 = 0.13122513443231582
Training loss on iteration 5040 = 0.11555741764605046
Training loss on iteration 5060 = 0.12228717356920242
Training loss on iteration 5080 = 0.13136647827923298
Training loss on iteration 5100 = 0.12289977259933949
Training loss on iteration 5120 = 0.12222621850669384
Training loss on iteration 5140 = 0.12554577998816968
Training loss on iteration 5160 = 0.12082288451492787
Training loss on iteration 5180 = 0.12631936334073543
Training loss on iteration 5200 = 0.1457524809986353
Training loss on iteration 5220 = 0.13801742270588874
Training loss on iteration 5240 = 0.12655894085764885
Training loss on iteration 5260 = 0.13338432535529138
Training loss on iteration 5280 = 0.1182616725564003
Training loss on iteration 5300 = 0.12832527775317432
Training loss on iteration 5320 = 0.11473312377929687
Training loss on iteration 5340 = 0.12507335133850575
Training loss on iteration 5360 = 0.1306200422346592
Training loss on iteration 5380 = 0.11493422351777553
Training loss on iteration 5400 = 0.10673758722841739
Training loss on iteration 5420 = 0.11036816723644734
Training loss on iteration 5440 = 0.10636375024914742
Training loss on iteration 5460 = 0.12843011729419232
Training loss on iteration 5480 = 0.13562559746205807
Training loss on iteration 5500 = 0.12559786178171634
Training loss on iteration 5520 = 0.11896285563707351
Training loss on iteration 5540 = 0.12834239304065703
Training loss on iteration 5560 = 0.12200747728347779
Training loss on iteration 5580 = 0.1267052788287401
Training loss on iteration 5600 = 0.1166045967489481
Training loss on iteration 5620 = 0.13085669465363026
Training loss on iteration 5640 = 0.1281111691147089
Training loss on iteration 5660 = 0.11930570602416993
Training loss on iteration 5680 = 0.11759513281285763
Training loss on iteration 5700 = 0.12214121446013451
Training loss on iteration 5720 = 0.11895731650292873
Training loss on iteration 5740 = 0.12198317348957062
Training loss on iteration 5760 = 0.12097558043897153
Training loss on iteration 5780 = 0.13465762808918952
Training loss on iteration 5800 = 0.1342541642487049
Training loss on iteration 5820 = 0.12373636960983277
Training loss on iteration 5840 = 0.13156350068747996
Training loss on iteration 5860 = 0.1148222602903843
Training loss on iteration 5880 = 0.11868454404175281
Training loss on iteration 5900 = 0.11461328193545342
Training loss on iteration 5920 = 0.12003356404602528
Training loss on iteration 5940 = 0.12163274213671685
Training loss on iteration 5960 = 0.1266186960041523
Training loss on iteration 5980 = 0.11093518398702144
Training loss on iteration 6000 = 0.16447707675397397
Training loss on iteration 6020 = 0.11700332760810853
Training loss on iteration 6040 = 0.12160566002130509
Training loss on iteration 6060 = 0.11964108012616634
Training loss on iteration 6080 = 0.12819564566016198
Training loss on iteration 6100 = 0.1281540971249342
Training loss on iteration 6120 = 0.1259942375123501
Training loss on iteration 6140 = 0.1271039467304945
Training loss on iteration 6160 = 0.12103640623390674
Training loss on iteration 6180 = 0.12709032371640205
Training loss on iteration 6200 = 0.12737931720912457
Training loss on iteration 6220 = 0.11090588234364987
Training loss on iteration 6240 = 0.13004059940576554
Training loss on iteration 6260 = 0.12725152708590032
Training loss on iteration 6280 = 0.12569232173264028
Training loss on iteration 6300 = 0.12242086566984653
Training loss on iteration 6320 = 0.11455757170915604
Training loss on iteration 6340 = 0.11710839681327342
Training loss on iteration 6360 = 0.1197349812835455
Training loss on iteration 6380 = 0.11768770217895508
Training loss on iteration 0 = 0.1037818044424057
Training loss on iteration 20 = 0.1211211521178484
Training loss on iteration 40 = 0.12520610466599463
Training loss on iteration 60 = 0.15113265849649907
Training loss on iteration 80 = 0.11413909532129765
Training loss on iteration 100 = 0.11649870462715625
Training loss on iteration 120 = 0.13325160667300223
Training loss on iteration 140 = 0.1088873740285635
Training loss on iteration 160 = 0.11293873712420463
Training loss on iteration 180 = 0.11644942425191403
Training loss on iteration 200 = 0.10737704765051603
Training loss on iteration 220 = 0.10404648929834366
Training loss on iteration 240 = 0.10709495618939399
Training loss on iteration 260 = 0.10564724542200565
Training loss on iteration 280 = 0.1202539324760437
Training loss on iteration 300 = 0.10183576047420502
Training loss on iteration 320 = 0.10739479418843985
Training loss on iteration 340 = 0.10975148882716894
Training loss on iteration 360 = 0.11506973784416914
Training loss on iteration 380 = 0.1132953867316246
Training loss on iteration 400 = 0.12866655215620995
Training loss on iteration 420 = 0.11486684903502464
Training loss on iteration 440 = 0.1164218494668603
Training loss on iteration 460 = 0.11183086540549994
Training loss on iteration 480 = 0.12157705575227737
Training loss on iteration 500 = 0.11862562000751495
Training loss on iteration 520 = 0.12373495064675807
Training loss on iteration 540 = 0.105433439463377
Training loss on iteration 560 = 0.11758641600608825
Training loss on iteration 580 = 0.10524806212633848
Training loss on iteration 600 = 0.12363116554915905
Training loss on iteration 620 = 0.125624636746943
Training loss on iteration 640 = 0.11587838381528855
Training loss on iteration 660 = 0.11780973505228758
Training loss on iteration 680 = 0.10188399218022823
Training loss on iteration 700 = 0.12576753590255976
Training loss on iteration 720 = 0.11308243758976459
Training loss on iteration 740 = 0.09783008545637131
Training loss on iteration 760 = 0.09412349425256253
Training loss on iteration 780 = 0.12013915069401264
Training loss on iteration 800 = 0.11588677577674389
Training loss on iteration 820 = 0.11137590929865837
Training loss on iteration 840 = 0.1140252783894539
Training loss on iteration 860 = 0.1122575893998146
Training loss on iteration 880 = 0.10790776088833809
Training loss on iteration 900 = 0.10956344213336706
Training loss on iteration 920 = 0.1150374636054039
Training loss on iteration 940 = 0.10921937339007855
Training loss on iteration 960 = 0.1315461814403534
Training loss on iteration 980 = 0.14987163431942463
Training loss on iteration 1000 = 0.1163773164153099
Training loss on iteration 1020 = 0.11271500829607248
Training loss on iteration 1040 = 0.11126042306423187
Training loss on iteration 1060 = 0.11185090951621532
Training loss on iteration 1080 = 0.1288353394716978
Training loss on iteration 1100 = 0.11229471601545811
Training loss on iteration 1120 = 0.11724315136671067
Training loss on iteration 1140 = 0.10861312597990036
Training loss on iteration 1160 = 0.12204125449061394
Training loss on iteration 1180 = 0.11662269569933414
Training loss on iteration 1200 = 0.10480676498264074
Training loss on iteration 1220 = 0.11784436963498593
Training loss on iteration 1240 = 0.11837969906628132
Training loss on iteration 1260 = 0.11203470192849636
Training loss on iteration 1280 = 0.1250952512025833
Training loss on iteration 1300 = 0.10892155691981316
Training loss on iteration 1320 = 0.1084992304444313
Training loss on iteration 1340 = 0.12016531750559807
Training loss on iteration 1360 = 0.13376588709652423
Training loss on iteration 1380 = 0.11446227803826332
Training loss on iteration 1400 = 0.12417556755244732
Training loss on iteration 1420 = 0.1284067876636982
Training loss on iteration 1440 = 0.1117225218564272
Training loss on iteration 1460 = 0.10996457040309907
Training loss on iteration 1480 = 0.11581895649433135
Training loss on iteration 1500 = 0.1145870964974165
Training loss on iteration 1520 = 0.110280105099082
Training loss on iteration 1540 = 0.10575562454760075
Training loss on iteration 1560 = 0.11745924800634384
Training loss on iteration 1580 = 0.1236612007021904
Training loss on iteration 1600 = 0.1372550304979086
Training loss on iteration 1620 = 0.17852647006511688
Training loss on iteration 1640 = 0.1471819870173931
Training loss on iteration 1660 = 0.10901383757591247
Training loss on iteration 1680 = 0.12069693244993687
Training loss on iteration 1700 = 0.11692005507647991
Training loss on iteration 1720 = 0.11358522213995456
Training loss on iteration 1740 = 0.11256278567016124
Training loss on iteration 1760 = 0.11880216486752034
Training loss on iteration 1780 = 0.12021418884396554
Training loss on iteration 1800 = 0.11268334574997425
Training loss on iteration 1820 = 0.11469368040561675
Training loss on iteration 1840 = 0.1131633847951889
Training loss on iteration 1860 = 0.11213661395013333
Training loss on iteration 1880 = 0.1195001732558012
Training loss on iteration 1900 = 0.12590989135205746
Training loss on iteration 1920 = 0.12035279907286167
Training loss on iteration 1940 = 0.12216819934546948
Training loss on iteration 1960 = 0.12395362332463264
Training loss on iteration 1980 = 0.1043751958757639
Training loss on iteration 2000 = 0.11565646268427372
Training loss on iteration 2020 = 0.12119797952473163
Training loss on iteration 2040 = 0.11727832462638617
Training loss on iteration 2060 = 0.11784139946103096
Training loss on iteration 2080 = 0.11407270357012748
Training loss on iteration 2100 = 0.11681115664541722
Training loss on iteration 2120 = 0.12244432233273983
Training loss on iteration 2140 = 0.1161916272714734
Training loss on iteration 2160 = 0.12187967412173747
Training loss on iteration 2180 = 0.11598390005528927
Training loss on iteration 2200 = 0.114238703250885
Training loss on iteration 2220 = 0.1335741899907589
Training loss on iteration 2240 = 0.11380154117941857
Training loss on iteration 2260 = 0.11231400407850742
Training loss on iteration 2280 = 0.1177304646000266
Training loss on iteration 2300 = 0.11639272421598434
Training loss on iteration 2320 = 0.10742073766887188
Training loss on iteration 2340 = 0.11168010309338569
Training loss on iteration 2360 = 0.13436391428112984
Training loss on iteration 2380 = 0.11184603571891785
Training loss on iteration 2400 = 0.1115772856399417
Training loss on iteration 2420 = 0.11335187703371048
Training loss on iteration 2440 = 0.11418392844498157
Training loss on iteration 2460 = 0.11835575699806214
Training loss on iteration 2480 = 0.14854223057627677
Training loss on iteration 2500 = 0.11118156015872956
Training loss on iteration 2520 = 0.11456701159477234
Training loss on iteration 2540 = 0.10934224277734757
Training loss on iteration 2560 = 0.13215465955436229
Training loss on iteration 2580 = 0.12136327438056468
Training loss on iteration 2600 = 0.1048658985644579
Training loss on iteration 2620 = 0.11394840814173221
Training loss on iteration 2640 = 0.11706768870353698
Training loss on iteration 2660 = 0.11743160374462605
Training loss on iteration 2680 = 0.12315300740301609
Training loss on iteration 2700 = 0.13355514071881772
Training loss on iteration 2720 = 0.11984882969409227
Training loss on iteration 2740 = 0.11090005487203598
Training loss on iteration 2760 = 0.10923582389950752
Training loss on iteration 2780 = 0.10822914969176053
Training loss on iteration 2800 = 0.1294693287461996
Training loss on iteration 2820 = 0.11988816112279892
Training loss on iteration 2840 = 0.12937347888946532
Training loss on iteration 2860 = 0.12239199727773667
Training loss on iteration 2880 = 0.11850207820534706
Training loss on iteration 2900 = 0.1124272096902132
Training loss on iteration 2920 = 0.12433666549623013
Training loss on iteration 2940 = 0.11381887160241604
Training loss on iteration 2960 = 0.11975730545818805
Training loss on iteration 2980 = 0.11716255657374859
Training loss on iteration 3000 = 0.11429558079689742
Training loss on iteration 3020 = 0.11170137375593185
Training loss on iteration 3040 = 0.11918765604496002
Training loss on iteration 3060 = 0.11412061676383019
Training loss on iteration 3080 = 0.12576997205615043
Training loss on iteration 3100 = 0.11131975837051869
Training loss on iteration 3120 = 0.12482777237892151
Training loss on iteration 3140 = 0.12619375996291637
Training loss on iteration 3160 = 0.11302686929702759
Training loss on iteration 3180 = 0.15336282812058927
Training loss on iteration 3200 = 0.1207446038722992
Training loss on iteration 3220 = 0.11508699543774129
Training loss on iteration 3240 = 0.13047264665365219
Training loss on iteration 3260 = 0.0929847676306963
Training loss on iteration 3280 = 0.12218915820121765
Training loss on iteration 3300 = 0.10840524043887853
Training loss on iteration 3320 = 0.1183532278984785
Training loss on iteration 3340 = 0.1125297388061881
Training loss on iteration 3360 = 0.11805887557566166
Training loss on iteration 3380 = 0.11285507269203662
Training loss on iteration 3400 = 0.1190902765840292
Training loss on iteration 3420 = 0.11219385601580142
Training loss on iteration 3440 = 0.10535650588572025
Training loss on iteration 3460 = 0.11416517123579979
Training loss on iteration 3480 = 0.11753465570509433
Training loss on iteration 3500 = 0.11682754084467888
Training loss on iteration 3520 = 0.15041831657290458
Training loss on iteration 3540 = 0.11421373076736926
Training loss on iteration 3560 = 0.10209059566259385
Training loss on iteration 3580 = 0.11308086663484573
Training loss on iteration 3600 = 0.1359002385288477
Training loss on iteration 3620 = 0.11668181829154492
Training loss on iteration 3640 = 0.10892209894955159
Training loss on iteration 3660 = 0.11324583031237126
Training loss on iteration 3680 = 0.12569533959031104
Training loss on iteration 3700 = 0.11527163162827492
Training loss on iteration 3720 = 0.12336537279188634
Training loss on iteration 3740 = 0.1314867053180933
Training loss on iteration 3760 = 0.12189247421920299
Training loss on iteration 3780 = 0.11788534149527549
Training loss on iteration 3800 = 0.12987169325351716
Training loss on iteration 3820 = 0.12553984560072423
Training loss on iteration 3840 = 0.1232377041131258
Training loss on iteration 3860 = 0.11835921667516232
Training loss on iteration 3880 = 0.12438678070902824
Training loss on iteration 3900 = 0.11490612961351872
Training loss on iteration 3920 = 0.12989016734063624
Training loss on iteration 3940 = 0.11238096095621586
Training loss on iteration 3960 = 0.11582789272069931
Training loss on iteration 3980 = 0.10534551553428173
Training loss on iteration 4000 = 0.11771257631480694
Training loss on iteration 4020 = 0.11306349523365497
Training loss on iteration 4040 = 0.12678430788218975
Training loss on iteration 4060 = 0.12252565342932939
Training loss on iteration 4080 = 0.11409052386879921
Training loss on iteration 4100 = 0.12130902446806431
Training loss on iteration 4120 = 0.1312057051807642
Training loss on iteration 4140 = 0.11149943694472313
Training loss on iteration 4160 = 0.1131384838372469
Training loss on iteration 4180 = 0.10861711129546166
Training loss on iteration 4200 = 0.13952183946967125
Training loss on iteration 4220 = 0.1263974390923977
Training loss on iteration 4240 = 0.12149269320070744
Training loss on iteration 4260 = 0.11849954389035702
Training loss on iteration 4280 = 0.11506304889917374
Training loss on iteration 4300 = 0.1573423646390438
Training loss on iteration 4320 = 0.1201826088130474
Training loss on iteration 4340 = 0.29989218935370443
Training loss on iteration 4360 = 0.11145955622196198
Training loss on iteration 4380 = 0.11632927618920803
Training loss on iteration 4400 = 0.12426676638424397
Training loss on iteration 4420 = 0.11342363208532333
Training loss on iteration 4440 = 0.13198343738913537
Training loss on iteration 4460 = 0.11699828635901213
Training loss on iteration 4480 = 0.12086140401661397
Training loss on iteration 4500 = 0.11511508412659169
Training loss on iteration 4520 = 0.12206391356885433
Training loss on iteration 4540 = 0.11236754432320595
Training loss on iteration 4560 = 0.11564039252698421
Training loss on iteration 4580 = 0.12304247952997685
Training loss on iteration 4600 = 0.11600998491048813
Training loss on iteration 4620 = 0.12126400843262672
Training loss on iteration 4640 = 0.1151321966201067
Training loss on iteration 4660 = 0.13577084057033062
Training loss on iteration 4680 = 0.13074962869286538
Training loss on iteration 4700 = 0.11527469120919705
Training loss on iteration 4720 = 0.10964209362864494
Training loss on iteration 4740 = 0.13881450183689595
Training loss on iteration 4760 = 0.12443963773548603
Training loss on iteration 4780 = 0.11631049513816834
Training loss on iteration 4800 = 0.11961092166602612
Training loss on iteration 4820 = 0.11645810157060624
Training loss on iteration 4840 = 0.11710991933941842
Training loss on iteration 4860 = 0.1028392918407917
Training loss on iteration 4880 = 0.11281931325793267
Training loss on iteration 4900 = 0.11378589235246181
Training loss on iteration 4920 = 0.11168373934924603
Training loss on iteration 4940 = 0.12393448092043399
Training loss on iteration 4960 = 0.11963600181043148
Training loss on iteration 4980 = 0.12614622451364993
Training loss on iteration 5000 = 0.11463642157614232
Training loss on iteration 5020 = 0.11653166674077511
Training loss on iteration 5040 = 0.1253858707845211
Training loss on iteration 5060 = 0.12483859248459339
Training loss on iteration 5080 = 0.15010467134416103
Training loss on iteration 5100 = 0.1170820165425539
Training loss on iteration 5120 = 0.11494537778198718
Training loss on iteration 5140 = 0.12026622276753188
Training loss on iteration 5160 = 0.12530964128673078
Training loss on iteration 5180 = 0.11458053067326546
Training loss on iteration 5200 = 0.13305989801883697
Training loss on iteration 5220 = 0.11139535196125508
Training loss on iteration 5240 = 0.10923642441630363
Training loss on iteration 5260 = 0.11929259598255157
Training loss on iteration 5280 = 0.12841290477663278
Training loss on iteration 5300 = 0.11460601724684238
Training loss on iteration 5320 = 0.15057576335966588
Training loss on iteration 5340 = 0.11433283109217882
Training loss on iteration 5360 = 0.12410587817430496
Training loss on iteration 5380 = 0.12592528518289328
Training loss on iteration 5400 = 0.10811287797987461
Training loss on iteration 5420 = 0.11955505162477494
Training loss on iteration 5440 = 0.12010324783623219
Training loss on iteration 5460 = 0.12308020293712615
Training loss on iteration 5480 = 0.1326560016721487
Training loss on iteration 5500 = 0.1328235100954771
Training loss on iteration 5520 = 0.12152819149196148
Training loss on iteration 5540 = 0.13157910704612732
Training loss on iteration 5560 = 0.11459414176642894
Training loss on iteration 5580 = 0.11801457703113556
Training loss on iteration 5600 = 0.1245395801961422
Training loss on iteration 5620 = 0.11255386248230934
Training loss on iteration 5640 = 0.12297828048467636
Training loss on iteration 5660 = 0.13391712680459023
Training loss on iteration 5680 = 0.12893701046705247
Training loss on iteration 5700 = 0.13686027526855468
Training loss on iteration 5720 = 0.11496924608945847
Training loss on iteration 5740 = 0.12235478200018406
Training loss on iteration 5760 = 0.12137096263468265
Training loss on iteration 5780 = 0.12889733128249645
Training loss on iteration 5800 = 0.10689037665724754
Training loss on iteration 5820 = 0.11443499960005284
Training loss on iteration 5840 = 0.10664778407663107
Training loss on iteration 5860 = 0.11580104157328605
Training loss on iteration 5880 = 0.12259217575192452
Training loss on iteration 5900 = 0.12304226011037826
Training loss on iteration 5920 = 0.1295128233730793
Training loss on iteration 5940 = 0.12898535951972007
Training loss on iteration 5960 = 0.12432731911540032
Training loss on iteration 5980 = 0.12018793895840645
Training loss on iteration 6000 = 0.12628261856734752
Training loss on iteration 6020 = 0.11794475764036179
Training loss on iteration 6040 = 0.12679351456463336
Training loss on iteration 6060 = 0.12842928767204284
Training loss on iteration 6080 = 0.12210291251540184
Training loss on iteration 6100 = 0.12400838546454906
Training loss on iteration 6120 = 0.2771555118262768
Training loss on iteration 6140 = 0.13929916061460973
Training loss on iteration 6160 = 0.12315355613827705
Training loss on iteration 6180 = 0.1149326954036951
Training loss on iteration 6200 = 0.11096233874559402
Training loss on iteration 6220 = 0.11202067285776138
Training loss on iteration 6240 = 0.12508547827601432
Training loss on iteration 6260 = 0.1356948170810938
Training loss on iteration 6280 = 0.12036576420068741
Training loss on iteration 6300 = 0.12708306387066842
Training loss on iteration 6320 = 0.11658030897378921
Training loss on iteration 6340 = 0.1178529568016529
Training loss on iteration 6360 = 0.11610613167285919
Training loss on iteration 6380 = 0.12333241626620292
Training loss on iteration 0 = 0.15734578669071198
Training loss on iteration 20 = 0.10344005078077316
Training loss on iteration 40 = 0.10010340996086597
Training loss on iteration 60 = 0.1254583042114973
Training loss on iteration 80 = 0.10991387460380793
Training loss on iteration 100 = 0.10843258574604989
Training loss on iteration 120 = 0.11515504010021686
Training loss on iteration 140 = 0.1082498837262392
Training loss on iteration 160 = 0.1113480918109417
Training loss on iteration 180 = 0.10371742695569992
Training loss on iteration 200 = 0.10283217132091522
Training loss on iteration 220 = 0.10850735865533352
Training loss on iteration 240 = 0.10114527381956577
Training loss on iteration 260 = 0.1221362043172121
Training loss on iteration 280 = 0.10512742809951306
Training loss on iteration 300 = 0.10631192475557327
Training loss on iteration 320 = 0.11395692564547062
Training loss on iteration 340 = 0.10694970488548279
Training loss on iteration 360 = 0.11511159017682075
Training loss on iteration 380 = 0.11194588132202625
Training loss on iteration 400 = 0.1032677073031664
Training loss on iteration 420 = 0.10630839169025422
Training loss on iteration 440 = 0.10923304725438357
Training loss on iteration 460 = 0.26229156702756884
Training loss on iteration 480 = 0.10903466530144215
Training loss on iteration 500 = 0.10525764748454094
Training loss on iteration 520 = 0.09666811712086201
Training loss on iteration 540 = 0.10471233315765857
Training loss on iteration 560 = 0.10677525065839291
Training loss on iteration 580 = 0.11375359259545803
Training loss on iteration 600 = 0.1265599973499775
Training loss on iteration 620 = 0.114368418417871
Training loss on iteration 640 = 0.11232770457863808
Training loss on iteration 660 = 0.10285953618586063
Training loss on iteration 680 = 0.10137436166405678
Training loss on iteration 700 = 0.11280947476625443
Training loss on iteration 720 = 0.11140537895262241
Training loss on iteration 740 = 0.10342343151569366
Training loss on iteration 760 = 0.12572360672056676
Training loss on iteration 780 = 0.10632801689207554
Training loss on iteration 800 = 0.11679886169731617
Training loss on iteration 820 = 0.11915107369422913
Training loss on iteration 840 = 0.12730612717568873
Training loss on iteration 860 = 0.10433361567556858
Training loss on iteration 880 = 0.11094288229942321
Training loss on iteration 900 = 0.10131029747426509
Training loss on iteration 920 = 0.11667401231825351
Training loss on iteration 940 = 0.11375769600272179
Training loss on iteration 960 = 0.11151594705879689
Training loss on iteration 980 = 0.1177647590637207
Training loss on iteration 1000 = 0.10491600409150123
Training loss on iteration 1020 = 0.10188900530338288
Training loss on iteration 1040 = 0.1164896197617054
Training loss on iteration 1060 = 0.10933271795511246
Training loss on iteration 1080 = 0.12129460982978343
Training loss on iteration 1100 = 0.10143192894756795
Training loss on iteration 1120 = 0.09932728875428438
Training loss on iteration 1140 = 0.10658743791282177
Training loss on iteration 1160 = 0.12646168582141398
Training loss on iteration 1180 = 0.11913342326879502
Training loss on iteration 1200 = 0.11057973206043244
Training loss on iteration 1220 = 0.10512100718915462
Training loss on iteration 1240 = 0.11315009593963624
Training loss on iteration 1260 = 0.10936431214213371
Training loss on iteration 1280 = 0.10652467049658298
Training loss on iteration 1300 = 0.10522209517657757
Training loss on iteration 1320 = 0.12608552500605583
Training loss on iteration 1340 = 0.1135637529194355
Training loss on iteration 1360 = 0.12686431221663952
Training loss on iteration 1380 = 0.11277698650956154
Training loss on iteration 1400 = 0.11658871509134769
Training loss on iteration 1420 = 0.11268651075661182
Training loss on iteration 1440 = 0.1171763189136982
Training loss on iteration 1460 = 0.11507611460983753
Training loss on iteration 1480 = 0.11913968287408352
Training loss on iteration 1500 = 0.10286928918212652
Training loss on iteration 1520 = 0.11583026126027107
Training loss on iteration 1540 = 0.1135745070874691
Training loss on iteration 1560 = 0.1306433103978634
Training loss on iteration 1580 = 0.12005872502923012
Training loss on iteration 1600 = 0.11473090499639511
Training loss on iteration 1620 = 0.11235940754413605
Training loss on iteration 1640 = 0.11716095507144927
Training loss on iteration 1660 = 0.11636277250945568
Training loss on iteration 1680 = 0.11338021084666253
Training loss on iteration 1700 = 0.10923379696905614
Training loss on iteration 1720 = 0.11268345154821872
Training loss on iteration 1740 = 0.11690169610083104
Training loss on iteration 1760 = 0.1252350715920329
Training loss on iteration 1780 = 0.1221971020102501
Training loss on iteration 1800 = 0.14802428744733334
Training loss on iteration 1820 = 0.11523765809834004
Training loss on iteration 1840 = 0.10876882560551167
Training loss on iteration 1860 = 0.10419688336551189
Training loss on iteration 1880 = 0.1108774708583951
Training loss on iteration 1900 = 0.11119183823466301
Training loss on iteration 1920 = 0.109363978728652
Training loss on iteration 1940 = 0.1279425911605358
Training loss on iteration 1960 = 0.12050206009298563
Training loss on iteration 1980 = 0.10030695684254169
Training loss on iteration 2000 = 0.11553272716701031
Training loss on iteration 2020 = 0.1209485713392496
Training loss on iteration 2040 = 0.12758850380778314
Training loss on iteration 2060 = 0.11110775880515575
Training loss on iteration 2080 = 0.10352374818176031
Training loss on iteration 2100 = 0.11870919577777386
Training loss on iteration 2120 = 0.11609282046556473
Training loss on iteration 2140 = 0.10929588675498962
Training loss on iteration 2160 = 0.12008651550859213
Training loss on iteration 2180 = 0.11563998982310295
Training loss on iteration 2200 = 0.10974547639489174
Training loss on iteration 2220 = 0.12217959575355053
Training loss on iteration 2240 = 0.10774109661579132
Training loss on iteration 2260 = 0.1389749389141798
Training loss on iteration 2280 = 0.10957656837999821
Training loss on iteration 2300 = 0.10843576490879059
Training loss on iteration 2320 = 0.10798881370574236
Training loss on iteration 2340 = 0.11776595935225487
Training loss on iteration 2360 = 0.10854879282414913
Training loss on iteration 2380 = 0.11885817535221577
Training loss on iteration 2400 = 0.11229534558951855
Training loss on iteration 2420 = 0.12420446872711181
Training loss on iteration 2440 = 0.113931293040514
Training loss on iteration 2460 = 0.10725490935146809
Training loss on iteration 2480 = 0.11972416639328003
Training loss on iteration 2500 = 0.11237092353403569
Training loss on iteration 2520 = 0.12622375562787055
Training loss on iteration 2540 = 0.11385555826127529
Training loss on iteration 2560 = 0.10850236192345619
Training loss on iteration 2580 = 0.1440914200618863
Training loss on iteration 2600 = 0.13592271506786346
Training loss on iteration 2620 = 0.10681275688111783
Training loss on iteration 2640 = 0.10537121705710888
Training loss on iteration 2660 = 0.12235455699265003
Training loss on iteration 2680 = 0.096937589161098
Training loss on iteration 2700 = 0.1105654239654541
Training loss on iteration 2720 = 0.10958520863205194
Training loss on iteration 2740 = 0.1330475240945816
Training loss on iteration 2760 = 0.13871036171913148
Training loss on iteration 2780 = 0.10548830442130566
Training loss on iteration 2800 = 0.10788149535655975
Training loss on iteration 2820 = 0.10492559000849724
Training loss on iteration 2840 = 0.11965496949851513
Training loss on iteration 2860 = 0.12247724458575249
Training loss on iteration 2880 = 0.11095870472490788
Training loss on iteration 2900 = 0.12478411197662354
Training loss on iteration 2920 = 0.11237369887530804
Training loss on iteration 2940 = 0.11187228932976723
Training loss on iteration 2960 = 0.14965726621448994
Training loss on iteration 2980 = 0.11927155889570713
Training loss on iteration 3000 = 0.1115301564335823
Training loss on iteration 3020 = 0.1090147279202938
Training loss on iteration 3040 = 0.115461952611804
Training loss on iteration 3060 = 0.10321459174156189
Training loss on iteration 3080 = 0.11750053651630879
Training loss on iteration 3100 = 0.11589030176401138
Training loss on iteration 3120 = 0.11711807250976562
Training loss on iteration 3140 = 0.1183516763150692
Training loss on iteration 3160 = 0.113459587469697
Training loss on iteration 3180 = 0.1272581212222576
Training loss on iteration 3200 = 0.17949164770543574
Training loss on iteration 3220 = 0.12018378265202045
Training loss on iteration 3240 = 0.12477509044110775
Training loss on iteration 3260 = 0.11037650220096111
Training loss on iteration 3280 = 0.11647166982293129
Training loss on iteration 3300 = 0.11503485292196274
Training loss on iteration 3320 = 0.11498828791081905
Training loss on iteration 3340 = 0.11185746565461159
Training loss on iteration 3360 = 0.10026963371783496
Training loss on iteration 3380 = 0.11201227977871894
Training loss on iteration 3400 = 0.11784604005515575
Training loss on iteration 3420 = 0.10942274946719407
Training loss on iteration 3440 = 0.11679550036787986
Training loss on iteration 3460 = 0.10760555043816566
Training loss on iteration 3480 = 0.11079669930040836
Training loss on iteration 3500 = 0.11430694460868836
Training loss on iteration 3520 = 0.09874794892966747
Training loss on iteration 3540 = 0.11876537799835205
Training loss on iteration 3560 = 0.11026743277907372
Training loss on iteration 3580 = 0.2694176912307739
Training loss on iteration 3600 = 0.13376162201166153
Training loss on iteration 3620 = 0.11459856368601322
Training loss on iteration 3640 = 0.12360207438468933
Training loss on iteration 3660 = 0.12059130445122719
Training loss on iteration 3680 = 0.11368323713541031
Training loss on iteration 3700 = 0.12746749259531498
Training loss on iteration 3720 = 0.11097273137420416
Training loss on iteration 3740 = 0.11414102613925933
Training loss on iteration 3760 = 0.11973834298551082
Training loss on iteration 3780 = 0.11953596174716949
Training loss on iteration 3800 = 0.11228955127298831
Training loss on iteration 3820 = 0.12391485944390297
Training loss on iteration 3840 = 0.10514948070049286
Training loss on iteration 3860 = 0.12530891560018062
Training loss on iteration 3880 = 0.29236257895827295
Training loss on iteration 3900 = 0.11817952916026116
Training loss on iteration 3920 = 0.10957206040620804
Training loss on iteration 3940 = 0.12619054429233073
Training loss on iteration 3960 = 0.10930254831910133
Training loss on iteration 3980 = 0.11578513868153095
Training loss on iteration 4000 = 0.13726937733590602
Training loss on iteration 4020 = 0.1234306737780571
Training loss on iteration 4040 = 0.11918649673461915
Training loss on iteration 4060 = 0.11698517315089703
Training loss on iteration 4080 = 0.10875031016767026
Training loss on iteration 4100 = 0.12146474532783032
Training loss on iteration 4120 = 0.11596637181937694
Training loss on iteration 4140 = 0.11264208871871233
Training loss on iteration 4160 = 0.10703429095447063
Training loss on iteration 4180 = 0.13756914287805558
Training loss on iteration 4200 = 0.11932081989943981
Training loss on iteration 4220 = 0.12475667521357536
Training loss on iteration 4240 = 0.12035129070281983
Training loss on iteration 4260 = 0.12315301559865474
Training loss on iteration 4280 = 0.13386696614325047
Training loss on iteration 4300 = 0.10586409028619528
Training loss on iteration 4320 = 0.10784484148025512
Training loss on iteration 4340 = 0.11483658142387868
Training loss on iteration 4360 = 0.11600210629403591
Training loss on iteration 4380 = 0.11225763484835624
Training loss on iteration 4400 = 0.11737552732229233
Training loss on iteration 4420 = 0.10881764851510525
Training loss on iteration 4440 = 0.1156592071056366
Training loss on iteration 4460 = 0.10937326885759831
Training loss on iteration 4480 = 0.10182891637086869
Training loss on iteration 4500 = 0.12406830787658692
Training loss on iteration 4520 = 0.11519123241305351
Training loss on iteration 4540 = 0.11415192894637585
Training loss on iteration 4560 = 0.10603059493005276
Training loss on iteration 4580 = 0.11136051155626774
Training loss on iteration 4600 = 0.11417126171290874
Training loss on iteration 4620 = 0.11791658252477646
Training loss on iteration 4640 = 0.12078343443572521
Training loss on iteration 4660 = 0.12097124718129634
Training loss on iteration 4680 = 0.13517090156674386
Training loss on iteration 4700 = 0.12037603985518217
Training loss on iteration 4720 = 0.11995754204690456
Training loss on iteration 4740 = 0.12416463866829872
Training loss on iteration 4760 = 0.11277756094932556
Training loss on iteration 4780 = 0.12477108053863048
Training loss on iteration 4800 = 0.11866461932659149
Training loss on iteration 4820 = 0.12364516891539097
Training loss on iteration 4840 = 0.11405790876597166
Training loss on iteration 4860 = 0.10762078389525413
Training loss on iteration 4880 = 0.11320854183286429
Training loss on iteration 4900 = 0.12741900756955146
Training loss on iteration 4920 = 0.1181343749165535
Training loss on iteration 4940 = 0.12202631123363972
Training loss on iteration 4960 = 0.1347831655293703
Training loss on iteration 4980 = 0.12260953336954117
Training loss on iteration 5000 = 0.11731336824595928
Training loss on iteration 5020 = 0.11874859631061555
Training loss on iteration 5040 = 0.11686931643635035
Training loss on iteration 5060 = 0.11216214187443256
Training loss on iteration 5080 = 0.10539178065955639
Training loss on iteration 5100 = 0.13507180102169514
Training loss on iteration 5120 = 0.10283278971910477
Training loss on iteration 5140 = 0.1346665184944868
Training loss on iteration 5160 = 0.11433744505047798
Training loss on iteration 5180 = 0.12546226046979428
Training loss on iteration 5200 = 0.1245298970490694
Training loss on iteration 5220 = 0.11075651198625565
Training loss on iteration 5240 = 0.13200705647468566
Training loss on iteration 5260 = 0.10874537788331509
Training loss on iteration 5280 = 0.10719530154019594
Training loss on iteration 5300 = 0.12214549370110035
Training loss on iteration 5320 = 0.11863803379237652
Training loss on iteration 5340 = 0.11676330864429474
Training loss on iteration 5360 = 0.11863973625004291
Training loss on iteration 5380 = 0.11893125586211681
Training loss on iteration 5400 = 0.12549661174416543
Training loss on iteration 5420 = 0.11904655136168003
Training loss on iteration 5440 = 0.12628498040139674
Training loss on iteration 5460 = 0.1233217028900981
Training loss on iteration 5480 = 0.1141261711716652
Training loss on iteration 5500 = 0.11975855864584446
Training loss on iteration 5520 = 0.12265601940453053
Training loss on iteration 5540 = 0.12317773830145598
Training loss on iteration 5560 = 0.1052835738286376
Training loss on iteration 5580 = 0.1236030038446188
Training loss on iteration 5600 = 0.11590966023504734
Training loss on iteration 5620 = 0.11830085963010788
Training loss on iteration 5640 = 0.1328351493924856
Training loss on iteration 5660 = 0.11529784612357616
Training loss on iteration 5680 = 0.10992810651659965
Training loss on iteration 5700 = 0.11582173332571984
Training loss on iteration 5720 = 0.11351180858910084
Training loss on iteration 5740 = 0.12724094726145269
Training loss on iteration 5760 = 0.11545232944190502
Training loss on iteration 5780 = 0.11873024441301823
Training loss on iteration 5800 = 0.11657914705574512
Training loss on iteration 5820 = 0.11521182470023632
Training loss on iteration 5840 = 0.12596896514296532
Training loss on iteration 5860 = 0.11671902090311051
Training loss on iteration 5880 = 0.1126343172043562
Training loss on iteration 5900 = 0.11198357753455639
Training loss on iteration 5920 = 0.12367728166282177
Training loss on iteration 5940 = 0.13965982832014562
Training loss on iteration 5960 = 0.12263947762548924
Training loss on iteration 5980 = 0.11658804751932621
Training loss on iteration 6000 = 0.12211653217673302
Training loss on iteration 6020 = 0.1208720900118351
Training loss on iteration 6040 = 0.15168559290468692
Training loss on iteration 6060 = 0.11412453316152096
Training loss on iteration 6080 = 0.11030044704675675
Training loss on iteration 6100 = 0.16800303515046836
Training loss on iteration 6120 = 0.12234042175114154
Training loss on iteration 6140 = 0.12209724336862564
Training loss on iteration 6160 = 0.11341111175715923
Training loss on iteration 6180 = 0.110877401009202
Training loss on iteration 6200 = 0.1136526383459568
Training loss on iteration 6220 = 0.11689899004995823
Training loss on iteration 6240 = 0.12192664109170437
Training loss on iteration 6260 = 0.11325253695249557
Training loss on iteration 6280 = 0.11979940310120582
Training loss on iteration 6300 = 0.11838204488158226
Training loss on iteration 6320 = 0.11900084242224693
Training loss on iteration 6340 = 0.12328591234982014
Training loss on iteration 6360 = 0.1157698031514883
Training loss on iteration 6380 = 0.12520679812878371
Training loss on iteration 0 = 0.12916924059391022
Training loss on iteration 20 = 0.11127075739204884
Training loss on iteration 40 = 0.10972262509167194
Training loss on iteration 60 = 0.12478545792400837
Training loss on iteration 80 = 0.11117795445024967
Training loss on iteration 100 = 0.10573657415807247
Training loss on iteration 120 = 0.11550963018089533
Training loss on iteration 140 = 0.10697197802364826
Training loss on iteration 160 = 0.10516520384699106
Training loss on iteration 180 = 0.12537620067596436
Training loss on iteration 200 = 0.12862454876303672
Training loss on iteration 220 = 0.11189515758305787
Training loss on iteration 240 = 0.10712058953940869
Training loss on iteration 260 = 0.1084240086376667
Training loss on iteration 280 = 0.11578688025474548
Training loss on iteration 300 = 0.1163030806928873
Training loss on iteration 320 = 0.105399264767766
Training loss on iteration 340 = 0.11659371256828308
Training loss on iteration 360 = 0.12087385356426239
Training loss on iteration 380 = 0.10207865089178085
Training loss on iteration 400 = 0.11009424049407243
Training loss on iteration 420 = 0.1399648107588291
Training loss on iteration 440 = 0.11347158029675483
Training loss on iteration 460 = 0.10759298466145992
Training loss on iteration 480 = 0.10275157503783702
Training loss on iteration 500 = 0.1269362222403288
Training loss on iteration 520 = 0.09584324117749929
Training loss on iteration 540 = 0.10539300255477428
Training loss on iteration 560 = 0.12728365808725356
Training loss on iteration 580 = 0.09610725231468678
Training loss on iteration 600 = 0.10114444829523564
Training loss on iteration 620 = 0.100167740508914
Training loss on iteration 640 = 0.26518398970365525
Training loss on iteration 660 = 0.11481449399143458
Training loss on iteration 680 = 0.11456554755568504
Training loss on iteration 700 = 0.10944629386067391
Training loss on iteration 720 = 0.11435556896030903
Training loss on iteration 740 = 0.10669930316507817
Training loss on iteration 760 = 0.10145730525255203
Training loss on iteration 780 = 0.10106335766613483
Training loss on iteration 800 = 0.10968717690557242
Training loss on iteration 820 = 0.10654123686254025
Training loss on iteration 840 = 0.12291761487722397
Training loss on iteration 860 = 0.11273464038968087
Training loss on iteration 880 = 0.10327210668474436
Training loss on iteration 900 = 0.1102282851934433
Training loss on iteration 920 = 0.10570616330951452
Training loss on iteration 940 = 0.10474596284329891
Training loss on iteration 960 = 0.1088189072906971
Training loss on iteration 980 = 0.12137420400977135
Training loss on iteration 1000 = 0.10788772143423557
Training loss on iteration 1020 = 0.12082523293793201
Training loss on iteration 1040 = 0.14883583039045334
Training loss on iteration 1060 = 0.10658231414854527
Training loss on iteration 1080 = 0.10551343262195587
Training loss on iteration 1100 = 0.11172529011964798
Training loss on iteration 1120 = 0.11090725958347321
Training loss on iteration 1140 = 0.11235173791646957
Training loss on iteration 1160 = 0.11369881015270948
Training loss on iteration 1180 = 0.09827258661389351
Training loss on iteration 1200 = 0.10089787859469652
Training loss on iteration 1220 = 0.10969553124159574
Training loss on iteration 1240 = 0.11290237456560134
Training loss on iteration 1260 = 0.0996269827708602
Training loss on iteration 1280 = 0.10262515507638455
Training loss on iteration 1300 = 0.11349842250347138
Training loss on iteration 1320 = 0.11401870399713517
Training loss on iteration 1340 = 0.11118255369365215
Training loss on iteration 1360 = 0.10398760735988617
Training loss on iteration 1380 = 0.1070713210850954
Training loss on iteration 1400 = 0.1128554280847311
Training loss on iteration 1420 = 0.1119635183364153
Training loss on iteration 1440 = 0.10333102233707905
Training loss on iteration 1460 = 0.10062618553638458
Training loss on iteration 1480 = 0.10074727404862642
Training loss on iteration 1500 = 0.11453151181340218
Training loss on iteration 1520 = 0.1151372317224741
Training loss on iteration 1540 = 0.11016795206815004
Training loss on iteration 1560 = 0.11423412412405014
Training loss on iteration 1580 = 0.10109151042997837
Training loss on iteration 1600 = 0.1097299624234438
Training loss on iteration 1620 = 0.1084059726446867
Training loss on iteration 1640 = 0.10482087172567844
Training loss on iteration 1660 = 0.1064336620271206
Training loss on iteration 1680 = 0.10998053103685379
Training loss on iteration 1700 = 0.09577422682195902
Training loss on iteration 1720 = 0.11384332571178675
Training loss on iteration 1740 = 0.11202669814229012
Training loss on iteration 1760 = 0.10241450928151608
Training loss on iteration 1780 = 0.10129551626741887
Training loss on iteration 1800 = 0.14268509186804296
Training loss on iteration 1820 = 0.12026824951171874
Training loss on iteration 1840 = 0.11464314460754395
Training loss on iteration 1860 = 0.09847436994314193
Training loss on iteration 1880 = 0.1177076730877161
Training loss on iteration 1900 = 0.10759755559265613
Training loss on iteration 1920 = 0.12421539761126041
Training loss on iteration 1940 = 0.10570948421955109
Training loss on iteration 1960 = 0.10557172112166882
Training loss on iteration 1980 = 0.10739234182983637
Training loss on iteration 2000 = 0.10122160241007805
Training loss on iteration 2020 = 0.10706806778907776
Training loss on iteration 2040 = 0.11898928172886372
Training loss on iteration 2060 = 0.10151227861642838
Training loss on iteration 2080 = 0.10813735350966454
Training loss on iteration 2100 = 0.09896299503743648
Training loss on iteration 2120 = 0.1092300221323967
Training loss on iteration 2140 = 0.10770849026739597
Training loss on iteration 2160 = 0.09878299199044704
Training loss on iteration 2180 = 0.1168073620647192
Training loss on iteration 2200 = 0.12357724979519844
Training loss on iteration 2220 = 0.12638878859579564
Training loss on iteration 2240 = 0.1095229633152485
Training loss on iteration 2260 = 0.1186149150133133
Training loss on iteration 2280 = 0.10387231856584549
Training loss on iteration 2300 = 0.11774875298142433
Training loss on iteration 2320 = 0.10103654898703099
Training loss on iteration 2340 = 0.10866337232291698
Training loss on iteration 2360 = 0.11439525820314884
Training loss on iteration 2380 = 0.1359080646187067
Training loss on iteration 2400 = 0.12022916376590728
Training loss on iteration 2420 = 0.10979750845581293
Training loss on iteration 2440 = 0.10675607621669769
Training loss on iteration 2460 = 0.10997316464781762
Training loss on iteration 2480 = 0.10192957371473313
Training loss on iteration 2500 = 0.1081043915823102
Training loss on iteration 2520 = 0.10681291036307812
Training loss on iteration 2540 = 0.10786617323756217
Training loss on iteration 2560 = 0.119607849791646
Training loss on iteration 2580 = 0.11350830867886544
Training loss on iteration 2600 = 0.11823742724955082
Training loss on iteration 2620 = 0.11529434025287628
Training loss on iteration 2640 = 0.10650830566883088
Training loss on iteration 2660 = 0.10746201314032078
Training loss on iteration 2680 = 0.11305174045264721
Training loss on iteration 2700 = 0.1103079542517662
Training loss on iteration 2720 = 0.1081825215369463
Training loss on iteration 2740 = 0.11142009198665619
Training loss on iteration 2760 = 0.12113640569150448
Training loss on iteration 2780 = 0.11626736745238304
Training loss on iteration 2800 = 0.13355221822857857
Training loss on iteration 2820 = 0.11199502386152745
Training loss on iteration 2840 = 0.11070485264062882
Training loss on iteration 2860 = 0.1202416643500328
Training loss on iteration 2880 = 0.12206007540225983
Training loss on iteration 2900 = 0.11535193230956793
Training loss on iteration 2920 = 0.11367480047047138
Training loss on iteration 2940 = 0.10602588057518006
Training loss on iteration 2960 = 0.1241564940661192
Training loss on iteration 2980 = 0.11629478111863137
Training loss on iteration 3000 = 0.10778086706995964
Training loss on iteration 3020 = 0.11393796354532242
Training loss on iteration 3040 = 0.11315884590148925
Training loss on iteration 3060 = 0.10824488811194896
Training loss on iteration 3080 = 0.10876530520617962
Training loss on iteration 3100 = 0.12188365012407303
Training loss on iteration 3120 = 0.10845574587583542
Training loss on iteration 3140 = 0.10886172242462636
Training loss on iteration 3160 = 0.11677973121404647
Training loss on iteration 3180 = 0.11651589535176754
Training loss on iteration 3200 = 0.1051528749987483
Training loss on iteration 3220 = 0.11509590893983841
Training loss on iteration 3240 = 0.11166212521493435
Training loss on iteration 3260 = 0.10955152958631516
Training loss on iteration 3280 = 0.11343875862658023
Training loss on iteration 3300 = 0.11302681919187307
Training loss on iteration 3320 = 0.1767074555158615
Training loss on iteration 3340 = 0.11213267482817173
Training loss on iteration 3360 = 0.1122474679723382
Training loss on iteration 3380 = 0.11211355291306972
Training loss on iteration 3400 = 0.10251310169696808
Training loss on iteration 3420 = 0.10621786508709193
Training loss on iteration 3440 = 0.10331467725336552
Training loss on iteration 3460 = 0.13960354030132294
Training loss on iteration 3480 = 0.12881090678274632
Training loss on iteration 3500 = 0.11605949737131596
Training loss on iteration 3520 = 0.11191187426447868
Training loss on iteration 3540 = 0.10326620880514384
Training loss on iteration 3560 = 0.11752898059785366
Training loss on iteration 3580 = 0.12290884405374528
Training loss on iteration 3600 = 0.11705826446413994
Training loss on iteration 3620 = 0.11765013337135315
Training loss on iteration 3640 = 0.11135084591805935
Training loss on iteration 3660 = 0.1134822878986597
Training loss on iteration 3680 = 0.10969167686998844
Training loss on iteration 3700 = 0.10369726344943046
Training loss on iteration 3720 = 0.11865402720868587
Training loss on iteration 3740 = 0.10816146060824394
Training loss on iteration 3760 = 0.11563014015555381
Training loss on iteration 3780 = 0.11443446353077888
Training loss on iteration 3800 = 0.11447072178125381
Training loss on iteration 3820 = 0.10915513299405574
Training loss on iteration 3840 = 0.12322537489235401
Training loss on iteration 3860 = 0.13007598444819451
Training loss on iteration 3880 = 0.11068458147346974
Training loss on iteration 3900 = 0.12328677959740161
Training loss on iteration 3920 = 0.11629796139895916
Training loss on iteration 3940 = 0.11520198583602906
Training loss on iteration 3960 = 0.11899205297231674
Training loss on iteration 3980 = 0.11109756380319595
Training loss on iteration 4000 = 0.11594891119748355
Training loss on iteration 4020 = 0.10302972197532653
Training loss on iteration 4040 = 0.11622565649449826
Training loss on iteration 4060 = 0.11249460093677044
Training loss on iteration 4080 = 0.1229072768241167
Training loss on iteration 4100 = 0.11212345026433468
Training loss on iteration 4120 = 0.11194787733256817
Training loss on iteration 4140 = 0.116143848747015
Training loss on iteration 4160 = 0.1015913937240839
Training loss on iteration 4180 = 0.11177743561565875
Training loss on iteration 4200 = 0.1108342595398426
Training loss on iteration 4220 = 0.10372875146567821
Training loss on iteration 4240 = 0.11668055653572082
Training loss on iteration 4260 = 0.11727664694190025
Training loss on iteration 4280 = 0.1153873547911644
Training loss on iteration 4300 = 0.10443007946014404
Training loss on iteration 4320 = 0.12448636665940285
Training loss on iteration 4340 = 0.11148252673447132
Training loss on iteration 4360 = 0.11082102283835411
Training loss on iteration 4380 = 0.1087166141718626
Training loss on iteration 4400 = 0.09545537866652012
Training loss on iteration 4420 = 0.10958939045667648
Training loss on iteration 4440 = 0.11500770635902882
Training loss on iteration 4460 = 0.10782898273319005
Training loss on iteration 4480 = 0.11781309582293034
Training loss on iteration 4500 = 0.12113903425633907
Training loss on iteration 4520 = 0.106914222240448
Training loss on iteration 4540 = 0.10660587195307017
Training loss on iteration 4560 = 0.1110119130462408
Training loss on iteration 4580 = 0.1157929852604866
Training loss on iteration 4600 = 0.12029095441102981
Training loss on iteration 4620 = 0.12441133186221123
Training loss on iteration 4640 = 0.2933634243905544
Training loss on iteration 4660 = 0.12064499035477638
Training loss on iteration 4680 = 0.11414046101272106
Training loss on iteration 4700 = 0.10784828402101994
Training loss on iteration 4720 = 0.12746813409030439
Training loss on iteration 4740 = 0.10218341164290905
Training loss on iteration 4760 = 0.10981185920536518
Training loss on iteration 4780 = 0.10994258988648653
Training loss on iteration 4800 = 0.11243733912706375
Training loss on iteration 4820 = 0.11519307605922222
Training loss on iteration 4840 = 0.1177902389317751
Training loss on iteration 4860 = 0.10616433545947075
Training loss on iteration 4880 = 0.10495519749820233
Training loss on iteration 4900 = 0.11021887697279453
Training loss on iteration 4920 = 0.11114205680787563
Training loss on iteration 4940 = 0.10745565332472325
Training loss on iteration 4960 = 0.11756973005831242
Training loss on iteration 4980 = 0.10825778655707836
Training loss on iteration 5000 = 0.11619072891771794
Training loss on iteration 5020 = 0.11459761634469032
Training loss on iteration 5040 = 0.12958815395832063
Training loss on iteration 5060 = 0.11377036292105913
Training loss on iteration 5080 = 0.10737942196428776
Training loss on iteration 5100 = 0.11156260222196579
Training loss on iteration 5120 = 0.1170478243380785
Training loss on iteration 5140 = 0.11900913529098034
Training loss on iteration 5160 = 0.11464370936155319
Training loss on iteration 5180 = 0.11348558142781258
Training loss on iteration 5200 = 0.11424916386604309
Training loss on iteration 5220 = 0.1195442259311676
Training loss on iteration 5240 = 0.11799036860466003
Training loss on iteration 5260 = 0.11461363732814789
Training loss on iteration 5280 = 0.1046801120042801
Training loss on iteration 5300 = 0.10019357800483704
Training loss on iteration 5320 = 0.11017709616571665
Training loss on iteration 5340 = 0.11403992623090745
Training loss on iteration 5360 = 0.11840621344745159
Training loss on iteration 5380 = 0.1303994618356228
Training loss on iteration 5400 = 0.11541551761329175
Training loss on iteration 5420 = 0.12333186939358712
Training loss on iteration 5440 = 0.11033035591244697
Training loss on iteration 5460 = 0.10124697200953961
Training loss on iteration 5480 = 0.11277412697672844
Training loss on iteration 5500 = 0.1155882639810443
Training loss on iteration 5520 = 0.1426228780299425
Training loss on iteration 5540 = 0.11553203389048576
Training loss on iteration 5560 = 0.11003925167024135
Training loss on iteration 5580 = 0.12236370518803596
Training loss on iteration 5600 = 0.12312673181295394
Training loss on iteration 5620 = 0.1272903736680746
Training loss on iteration 5640 = 0.117508514970541
Training loss on iteration 5660 = 0.11666857078671455
Training loss on iteration 5680 = 0.1235998947173357
Training loss on iteration 5700 = 0.1039048109203577
Training loss on iteration 5720 = 0.1047808650881052
Training loss on iteration 5740 = 0.11652214284986258
Training loss on iteration 5760 = 0.11211357936263085
Training loss on iteration 5780 = 0.12625452764332296
Training loss on iteration 5800 = 0.11854316182434559
Training loss on iteration 5820 = 0.11460798084735871
Training loss on iteration 5840 = 0.1161009855568409
Training loss on iteration 5860 = 0.1275502573698759
Training loss on iteration 5880 = 0.1213266473263502
Training loss on iteration 5900 = 0.11518516652286052
Training loss on iteration 5920 = 0.1062222447246313
Training loss on iteration 5940 = 0.11295009888708592
Training loss on iteration 5960 = 0.12008644118905068
Training loss on iteration 5980 = 0.11428248323500156
Training loss on iteration 6000 = 0.11714955158531666
Training loss on iteration 6020 = 0.11649824567139148
Training loss on iteration 6040 = 0.13047917261719705
Training loss on iteration 6060 = 0.125711577385664
Training loss on iteration 6080 = 0.12054677493870258
Training loss on iteration 6100 = 0.11568391509354115
Training loss on iteration 6120 = 0.10551533848047256
Training loss on iteration 6140 = 0.13587391637265683
Training loss on iteration 6160 = 0.1126072259619832
Training loss on iteration 6180 = 0.11919467207044362
Training loss on iteration 6200 = 0.1200624242424965
Training loss on iteration 6220 = 0.12452099099755287
Training loss on iteration 6240 = 0.10582834165543317
Training loss on iteration 6260 = 0.11540859118103981
Training loss on iteration 6280 = 0.12299910299479962
Training loss on iteration 6300 = 0.11584044098854065
Training loss on iteration 6320 = 0.11136836670339108
Training loss on iteration 6340 = 0.12036847211420536
Training loss on iteration 6360 = 0.10824787728488446
Training loss on iteration 6380 = 0.10938161686062813
Training loss on iteration 0 = 0.09047587215900421
Training loss on iteration 20 = 0.09847544729709626
Training loss on iteration 40 = 0.09838976114988326
Training loss on iteration 60 = 0.10304883904755116
Training loss on iteration 80 = 0.12451813574880362
Training loss on iteration 100 = 0.10395777467638254
Training loss on iteration 120 = 0.10119698084890842
Training loss on iteration 140 = 0.10282643176615239
Training loss on iteration 160 = 0.11085449010133744
Training loss on iteration 180 = 0.10247541740536689
Training loss on iteration 200 = 0.10678934529423714
Training loss on iteration 220 = 0.09521777015179396
Training loss on iteration 240 = 0.1071923054754734
Training loss on iteration 260 = 0.1013035535812378
Training loss on iteration 280 = 0.09713985715061427
Training loss on iteration 300 = 0.10572277680039406
Training loss on iteration 320 = 0.10697418302297593
Training loss on iteration 340 = 0.11882322989404201
Training loss on iteration 360 = 0.10633258670568466
Training loss on iteration 380 = 0.11413692273199558
Training loss on iteration 400 = 0.09989084340631962
Training loss on iteration 420 = 0.10072610788047313
Training loss on iteration 440 = 0.10751641839742661
Training loss on iteration 460 = 0.09761801529675722
Training loss on iteration 480 = 0.10206112936139107
Training loss on iteration 500 = 0.1095947464928031
Training loss on iteration 520 = 0.10644453540444374
Training loss on iteration 540 = 0.10111756473779679
Training loss on iteration 560 = 0.09243165384978055
Training loss on iteration 580 = 0.10471733398735524
Training loss on iteration 600 = 0.1058509785681963
Training loss on iteration 620 = 0.12367328628897667
Training loss on iteration 640 = 0.10487007591873407
Training loss on iteration 660 = 0.09978369250893593
Training loss on iteration 680 = 0.10946774948388338
Training loss on iteration 700 = 0.09623959381133318
Training loss on iteration 720 = 0.11945723704993724
Training loss on iteration 740 = 0.11298075616359711
Training loss on iteration 760 = 0.10362102165818214
Training loss on iteration 780 = 0.13894162885844707
Training loss on iteration 800 = 0.11054564192891121
Training loss on iteration 820 = 0.10508649051189423
Training loss on iteration 840 = 0.10686077438294887
Training loss on iteration 860 = 0.1015490373596549
Training loss on iteration 880 = 0.10969856679439545
Training loss on iteration 900 = 0.10800973810255528
Training loss on iteration 920 = 0.10680948160588741
Training loss on iteration 940 = 0.10026718936860561
Training loss on iteration 960 = 0.09633714277297259
Training loss on iteration 980 = 0.09422058891505003
Training loss on iteration 1000 = 0.1085667822510004
Training loss on iteration 1020 = 0.09627097249031066
Training loss on iteration 1040 = 0.10817918404936791
Training loss on iteration 1060 = 0.09486319087445735
Training loss on iteration 1080 = 0.12587002702057362
Training loss on iteration 1100 = 0.09391466807574034
Training loss on iteration 1120 = 0.10439416356384754
Training loss on iteration 1140 = 0.11046012900769711
Training loss on iteration 1160 = 0.11704982668161393
Training loss on iteration 1180 = 0.1064833540469408
Training loss on iteration 1200 = 0.10488106794655323
Training loss on iteration 1220 = 0.10768787320703269
Training loss on iteration 1240 = 0.11315768472850322
Training loss on iteration 1260 = 0.10289201810956002
Training loss on iteration 1280 = 0.10483451262116432
Training loss on iteration 1300 = 0.10232083089649677
Training loss on iteration 1320 = 0.10094871185719967
Training loss on iteration 1340 = 0.1002134546637535
Training loss on iteration 1360 = 0.17681904919445515
Training loss on iteration 1380 = 0.10772470086812973
Training loss on iteration 1400 = 0.10206211544573307
Training loss on iteration 1420 = 0.11236374080181122
Training loss on iteration 1440 = 0.11236899308860301
Training loss on iteration 1460 = 0.09762499984353781
Training loss on iteration 1480 = 0.10401551201939582
Training loss on iteration 1500 = 0.10506553016602993
Training loss on iteration 1520 = 0.09690344594419002
Training loss on iteration 1540 = 0.11202377006411553
Training loss on iteration 1560 = 0.10987447574734688
Training loss on iteration 1580 = 0.09923090562224388
Training loss on iteration 1600 = 0.10811984911561012
Training loss on iteration 1620 = 0.11901819705963135
Training loss on iteration 1640 = 0.11675619948655366
Training loss on iteration 1660 = 0.11183736100792885
Training loss on iteration 1680 = 0.12857124134898185
Training loss on iteration 1700 = 0.10406273789703846
Training loss on iteration 1720 = 0.11164917647838593
Training loss on iteration 1740 = 0.10668263472616672
Training loss on iteration 1760 = 0.10332883931696416
Training loss on iteration 1780 = 0.10420643612742424
Training loss on iteration 1800 = 0.10465192534029484
Training loss on iteration 1820 = 0.1186861015856266
Training loss on iteration 1840 = 0.10638564862310887
Training loss on iteration 1860 = 0.11200296450406313
Training loss on iteration 1880 = 0.10574660822749138
Training loss on iteration 1900 = 0.10714696943759919
Training loss on iteration 1920 = 0.11589680165052414
Training loss on iteration 1940 = 0.10642742794007062
Training loss on iteration 1960 = 0.11094296127557754
Training loss on iteration 1980 = 0.10739336460828781
Training loss on iteration 2000 = 0.10589663162827492
Training loss on iteration 2020 = 0.11230275463312864
Training loss on iteration 2040 = 0.10367664024233818
Training loss on iteration 2060 = 0.10138878412544727
Training loss on iteration 2080 = 0.12684981636703013
Training loss on iteration 2100 = 0.11435213051736355
Training loss on iteration 2120 = 0.09917775820940733
Training loss on iteration 2140 = 0.11353626437485217
Training loss on iteration 2160 = 0.09817533530294895
Training loss on iteration 2180 = 0.1100100789219141
Training loss on iteration 2200 = 0.09911985844373702
Training loss on iteration 2220 = 0.14180315323174
Training loss on iteration 2240 = 0.1058196710422635
Training loss on iteration 2260 = 0.09823984615504741
Training loss on iteration 2280 = 0.11538669504225255
Training loss on iteration 2300 = 0.11509173959493638
Training loss on iteration 2320 = 0.10291233025491238
Training loss on iteration 2340 = 0.2658627539873123
Training loss on iteration 2360 = 0.097404870018363
Training loss on iteration 2380 = 0.10834270101040602
Training loss on iteration 2400 = 0.10489335134625435
Training loss on iteration 2420 = 0.10385607331991195
Training loss on iteration 2440 = 0.10066292844712735
Training loss on iteration 2460 = 0.11141905412077904
Training loss on iteration 2480 = 0.11227496843785048
Training loss on iteration 2500 = 0.12305675037205219
Training loss on iteration 2520 = 0.1030381191521883
Training loss on iteration 2540 = 0.11168874576687812
Training loss on iteration 2560 = 0.11507724989205599
Training loss on iteration 2580 = 0.12247548662126065
Training loss on iteration 2600 = 0.10894368588924408
Training loss on iteration 2620 = 0.11055771633982658
Training loss on iteration 2640 = 0.1155545711517334
Training loss on iteration 2660 = 0.1066239058971405
Training loss on iteration 2680 = 0.10633114632219076
Training loss on iteration 2700 = 0.1151994165033102
Training loss on iteration 2720 = 0.11839794591069222
Training loss on iteration 2740 = 0.12031682319939137
Training loss on iteration 2760 = 0.09834357723593712
Training loss on iteration 2780 = 0.11874103508889675
Training loss on iteration 2800 = 0.11316885389387607
Training loss on iteration 2820 = 0.11593156885355711
Training loss on iteration 2840 = 0.11640273928642272
Training loss on iteration 2860 = 0.10920065455138683
Training loss on iteration 2880 = 0.12541949264705182
Training loss on iteration 2900 = 0.11721434406936168
Training loss on iteration 2920 = 0.1038843996822834
Training loss on iteration 2940 = 0.10355747938156128
Training loss on iteration 2960 = 0.09852869436144829
Training loss on iteration 2980 = 0.10839251317083835
Training loss on iteration 3000 = 0.10309646278619766
Training loss on iteration 3020 = 0.11569945551455021
Training loss on iteration 3040 = 0.11376354657113552
Training loss on iteration 3060 = 0.11488944031298161
Training loss on iteration 3080 = 0.09884723648428917
Training loss on iteration 3100 = 0.11019684039056302
Training loss on iteration 3120 = 0.11237887032330036
Training loss on iteration 3140 = 0.12621590457856655
Training loss on iteration 3160 = 0.1079863216727972
Training loss on iteration 3180 = 0.10399676691740752
Training loss on iteration 3200 = 0.11002410240471364
Training loss on iteration 3220 = 0.10454662367701531
Training loss on iteration 3240 = 0.10368318520486355
Training loss on iteration 3260 = 0.1100771602243185
Training loss on iteration 3280 = 0.11932143718004226
Training loss on iteration 3300 = 0.10605924017727375
Training loss on iteration 3320 = 0.11640118509531021
Training loss on iteration 3340 = 0.10360448211431503
Training loss on iteration 3360 = 0.10268708877265453
Training loss on iteration 3380 = 0.10394570454955102
Training loss on iteration 3400 = 0.11461267694830894
Training loss on iteration 3420 = 0.12256921008229256
Training loss on iteration 3440 = 0.11576633453369141
Training loss on iteration 3460 = 0.10788818970322608
Training loss on iteration 3480 = 0.10500346086919307
Training loss on iteration 3500 = 0.11534321382641792
Training loss on iteration 3520 = 0.11979526616632938
Training loss on iteration 3540 = 0.12779815085232257
Training loss on iteration 3560 = 0.11362467482686042
Training loss on iteration 3580 = 0.11428430639207363
Training loss on iteration 3600 = 0.11537121795117855
Training loss on iteration 3620 = 0.1400824051350355
Training loss on iteration 3640 = 0.11270860210061073
Training loss on iteration 3660 = 0.10678104199469089
Training loss on iteration 3680 = 0.1100433573126793
Training loss on iteration 3700 = 0.11013922281563282
Training loss on iteration 3720 = 0.10617607533931732
Training loss on iteration 3740 = 0.11224203146994113
Training loss on iteration 3760 = 0.09787031151354313
Training loss on iteration 3780 = 0.12024546917527915
Training loss on iteration 3800 = 0.10645346250385046
Training loss on iteration 3820 = 0.11047429591417313
Training loss on iteration 3840 = 0.10524638928472996
Training loss on iteration 3860 = 0.10592421218752861
Training loss on iteration 3880 = 0.10740470327436924
Training loss on iteration 3900 = 0.1322333011776209
Training loss on iteration 3920 = 0.11090841479599475
Training loss on iteration 3940 = 0.12119493074715137
Training loss on iteration 3960 = 0.10263815354555846
Training loss on iteration 3980 = 0.11020457446575165
Training loss on iteration 4000 = 0.11054200362414121
Training loss on iteration 4020 = 0.10200147554278374
Training loss on iteration 4040 = 0.1119865246117115
Training loss on iteration 4060 = 0.10912416316568851
Training loss on iteration 4080 = 0.12568364776670932
Training loss on iteration 4100 = 0.1200127113610506
Training loss on iteration 4120 = 0.10834381692111492
Training loss on iteration 4140 = 0.1031787347048521
Training loss on iteration 4160 = 0.11046247705817222
Training loss on iteration 4180 = 0.28008714988827704
Training loss on iteration 4200 = 0.11349314860999585
Training loss on iteration 4220 = 0.11380888186395169
Training loss on iteration 4240 = 0.12940496243536473
Training loss on iteration 4260 = 0.1134261341765523
Training loss on iteration 4280 = 0.12795692421495913
Training loss on iteration 4300 = 0.09828445538878441
Training loss on iteration 4320 = 0.1078891135752201
Training loss on iteration 4340 = 0.10843279734253883
Training loss on iteration 4360 = 0.12057022489607334
Training loss on iteration 4380 = 0.1383313402533531
Training loss on iteration 4400 = 0.11148411706089974
Training loss on iteration 4420 = 0.10734088979661464
Training loss on iteration 4440 = 0.10834101531654597
Training loss on iteration 4460 = 0.1480908539146185
Training loss on iteration 4480 = 0.11672101132571697
Training loss on iteration 4500 = 0.11423193551599979
Training loss on iteration 4520 = 0.11193683557212353
Training loss on iteration 4540 = 0.11861027963459492
Training loss on iteration 4560 = 0.10724658351391554
Training loss on iteration 4580 = 0.10751384869217873
Training loss on iteration 4600 = 0.11926476806402206
Training loss on iteration 4620 = 0.1195146057754755
Training loss on iteration 4640 = 0.11328160911798477
Training loss on iteration 4660 = 0.1069641437381506
Training loss on iteration 4680 = 0.11122605800628663
Training loss on iteration 4700 = 0.10594607666134834
Training loss on iteration 4720 = 0.10812699310481548
Training loss on iteration 4740 = 0.1191641267389059
Training loss on iteration 4760 = 0.11920384131371975
Training loss on iteration 4780 = 0.1054830826818943
Training loss on iteration 4800 = 0.11581796668469906
Training loss on iteration 4820 = 0.1103662472218275
Training loss on iteration 4840 = 0.11378150396049022
Training loss on iteration 4860 = 0.11532811410725116
Training loss on iteration 4880 = 0.101383850350976
Training loss on iteration 4900 = 0.10317625142633916
Training loss on iteration 4920 = 0.10729330480098724
Training loss on iteration 4940 = 0.10575567111372948
Training loss on iteration 4960 = 0.11484651751816273
Training loss on iteration 4980 = 0.1232000920921564
Training loss on iteration 5000 = 0.10545437671244144
Training loss on iteration 5020 = 0.11523841992020607
Training loss on iteration 5040 = 0.12148436307907104
Training loss on iteration 5060 = 0.10635529290884733
Training loss on iteration 5080 = 0.11136361621320248
Training loss on iteration 5100 = 0.12385150454938412
Training loss on iteration 5120 = 0.10534086748957634
Training loss on iteration 5140 = 0.10744606349617243
Training loss on iteration 5160 = 0.12424976862967015
Training loss on iteration 5180 = 0.11533652730286122
Training loss on iteration 5200 = 0.12151710838079452
Training loss on iteration 5220 = 0.1078177809715271
Training loss on iteration 5240 = 0.11145002283155918
Training loss on iteration 5260 = 0.11272082068026065
Training loss on iteration 5280 = 0.10948342159390449
Training loss on iteration 5300 = 0.1289740676060319
Training loss on iteration 5320 = 0.13562553003430367
Training loss on iteration 5340 = 0.11572036575525999
Training loss on iteration 5360 = 0.10386718362569809
Training loss on iteration 5380 = 0.10593676455318927
Training loss on iteration 5400 = 0.11286319307982921
Training loss on iteration 5420 = 0.10970621667802334
Training loss on iteration 5440 = 0.11850474029779434
Training loss on iteration 5460 = 0.09924122728407384
Training loss on iteration 5480 = 0.10749882645905018
Training loss on iteration 5500 = 0.11787068620324134
Training loss on iteration 5520 = 0.10158335082232953
Training loss on iteration 5540 = 0.1198429238051176
Training loss on iteration 5560 = 0.10239638723433017
Training loss on iteration 5580 = 0.11944409124553204
Training loss on iteration 5600 = 0.10892826914787293
Training loss on iteration 5620 = 0.11718528419733047
Training loss on iteration 5640 = 0.11040786616504192
Training loss on iteration 5660 = 0.1134545650333166
Training loss on iteration 5680 = 0.10390652753412724
Training loss on iteration 5700 = 0.12067198827862739
Training loss on iteration 5720 = 0.10918108373880386
Training loss on iteration 5740 = 0.11219349391758442
Training loss on iteration 5760 = 0.11730212867259979
Training loss on iteration 5780 = 0.1050954107195139
Training loss on iteration 5800 = 0.1117335855960846
Training loss on iteration 5820 = 0.12329027242958546
Training loss on iteration 5840 = 0.13577020764350892
Training loss on iteration 5860 = 0.11329524479806423
Training loss on iteration 5880 = 0.12345157228410245
Training loss on iteration 5900 = 0.11097164042294025
Training loss on iteration 5920 = 0.1035263977944851
Training loss on iteration 5940 = 0.1146534126251936
Training loss on iteration 5960 = 0.10988895446062089
Training loss on iteration 5980 = 0.10589617155492306
Training loss on iteration 6000 = 0.12073274105787277
Training loss on iteration 6020 = 0.12482677437365056
Training loss on iteration 6040 = 0.11774137802422047
Training loss on iteration 6060 = 0.10797378420829773
Training loss on iteration 6080 = 0.12205680422484874
Training loss on iteration 6100 = 0.10727472007274627
Training loss on iteration 6120 = 0.11615334078669548
Training loss on iteration 6140 = 0.1113926786929369
Training loss on iteration 6160 = 0.1104067038744688
Training loss on iteration 6180 = 0.11951499991118908
Training loss on iteration 6200 = 0.10954581536352634
Training loss on iteration 6220 = 0.12271281890571117
Training loss on iteration 6240 = 0.10539274495095015
Training loss on iteration 6260 = 0.11439711600542068
Training loss on iteration 6280 = 0.11970105580985546
Training loss on iteration 6300 = 0.10861162543296814
Training loss on iteration 6320 = 0.10776805430650711
Training loss on iteration 6340 = 0.11136718653142452
Training loss on iteration 6360 = 0.12391591928899288
Training loss on iteration 6380 = 0.12090279534459114
Training loss on iteration 0 = 0.14309528470039368
Training loss on iteration 20 = 0.10389666445553303
Training loss on iteration 40 = 0.10831377506256104
Training loss on iteration 60 = 0.10009348094463348
Training loss on iteration 80 = 0.12704668678343295
Training loss on iteration 100 = 0.10525935664772987
Training loss on iteration 120 = 0.11454725079238415
Training loss on iteration 140 = 0.10273212101310492
Training loss on iteration 160 = 0.09755347110331059
Training loss on iteration 180 = 0.11468770280480385
Training loss on iteration 200 = 0.2595469832420349
Training loss on iteration 220 = 0.10850740987807513
Training loss on iteration 240 = 0.0938777880743146
Training loss on iteration 260 = 0.09975636638700962
Training loss on iteration 280 = 0.1023487389087677
Training loss on iteration 300 = 0.1051816426217556
Training loss on iteration 320 = 0.09547070767730474
Training loss on iteration 340 = 0.09007792733609676
Training loss on iteration 360 = 0.09976911544799805
Training loss on iteration 380 = 0.0942656897008419
Training loss on iteration 400 = 0.09676342271268368
Training loss on iteration 420 = 0.09823192823678255
Training loss on iteration 440 = 0.08693096563220024
Training loss on iteration 460 = 0.10184547044336796
Training loss on iteration 480 = 0.10236407332122326
Training loss on iteration 500 = 0.10292932502925396
Training loss on iteration 520 = 0.09755566418170929
Training loss on iteration 540 = 0.09553552940487861
Training loss on iteration 560 = 0.10663745999336242
Training loss on iteration 580 = 0.10042351111769676
Training loss on iteration 600 = 0.09940431993454694
Training loss on iteration 620 = 0.09385764375329017
Training loss on iteration 640 = 0.09670640230178833
Training loss on iteration 660 = 0.1014151431620121
Training loss on iteration 680 = 0.10668763909488917
Training loss on iteration 700 = 0.09956726133823394
Training loss on iteration 720 = 0.1011316142976284
Training loss on iteration 740 = 0.09291842970997095
Training loss on iteration 760 = 0.1009399589151144
Training loss on iteration 780 = 0.10641248039901256
Training loss on iteration 800 = 0.0892212551087141
Training loss on iteration 820 = 0.10507508292794228
Training loss on iteration 840 = 0.09511840399354696
Training loss on iteration 860 = 0.11012418791651726
Training loss on iteration 880 = 0.1014313567429781
Training loss on iteration 900 = 0.09953534565865993
Training loss on iteration 920 = 0.09591608308255672
Training loss on iteration 940 = 0.10078211948275566
Training loss on iteration 960 = 0.12484256811439991
Training loss on iteration 980 = 0.11114857736974955
Training loss on iteration 1000 = 0.09524309020489455
Training loss on iteration 1020 = 0.09643101450055838
Training loss on iteration 1040 = 0.10092810392379761
Training loss on iteration 1060 = 0.10698943100869655
Training loss on iteration 1080 = 0.09334793258458376
Training loss on iteration 1100 = 0.10209754668176174
Training loss on iteration 1120 = 0.09924218393862247
Training loss on iteration 1140 = 0.11754777170717716
Training loss on iteration 1160 = 0.10210444200783968
Training loss on iteration 1180 = 0.1036532748490572
Training loss on iteration 1200 = 0.10308300331234932
Training loss on iteration 1220 = 0.10221370328217745
Training loss on iteration 1240 = 0.10109783075749874
Training loss on iteration 1260 = 0.11373218037188053
Training loss on iteration 1280 = 0.10462253838777542
Training loss on iteration 1300 = 0.1179839514195919
Training loss on iteration 1320 = 0.10059209391474724
Training loss on iteration 1340 = 0.11909798346459866
Training loss on iteration 1360 = 0.10384764056652784
Training loss on iteration 1380 = 0.10888072028756142
Training loss on iteration 1400 = 0.09538288228213787
Training loss on iteration 1420 = 0.10770287811756134
Training loss on iteration 1440 = 0.09748036451637745
Training loss on iteration 1460 = 0.10570894666016102
Training loss on iteration 1480 = 0.10044067911803722
Training loss on iteration 1500 = 0.0994619220495224
Training loss on iteration 1520 = 0.10139833725988864
Training loss on iteration 1540 = 0.09933269768953323
Training loss on iteration 1560 = 0.10321165174245835
Training loss on iteration 1580 = 0.10845227502286434
Training loss on iteration 1600 = 0.09131016284227371
Training loss on iteration 1620 = 0.10371768660843372
Training loss on iteration 1640 = 0.09726274982094765
Training loss on iteration 1660 = 0.10995358936488628
Training loss on iteration 1680 = 0.10240140818059444
Training loss on iteration 1700 = 0.09697220548987388
Training loss on iteration 1720 = 0.11284681521356106
Training loss on iteration 1740 = 0.09870274923741817
Training loss on iteration 1760 = 0.1281706688925624
Training loss on iteration 1780 = 0.12347402311861515
Training loss on iteration 1800 = 0.10814256779849529
Training loss on iteration 1820 = 0.11256894506514073
Training loss on iteration 1840 = 0.1059885997325182
Training loss on iteration 1860 = 0.10685681104660034
Training loss on iteration 1880 = 0.1010602667927742
Training loss on iteration 1900 = 0.10999276209622622
Training loss on iteration 1920 = 0.10501565355807543
Training loss on iteration 1940 = 0.10730549097061157
Training loss on iteration 1960 = 0.11027931608259678
Training loss on iteration 1980 = 0.1141173005104065
Training loss on iteration 2000 = 0.10099712796509266
Training loss on iteration 2020 = 0.10915416739881038
Training loss on iteration 2040 = 0.11053234413266182
Training loss on iteration 2060 = 0.11206528693437576
Training loss on iteration 2080 = 0.09748073257505893
Training loss on iteration 2100 = 0.10837849676609039
Training loss on iteration 2120 = 0.10069162137806416
Training loss on iteration 2140 = 0.13126214407384396
Training loss on iteration 2160 = 0.11032497435808182
Training loss on iteration 2180 = 0.1066119197756052
Training loss on iteration 2200 = 0.10783569775521755
Training loss on iteration 2220 = 0.10726286098361015
Training loss on iteration 2240 = 0.09220407884567976
Training loss on iteration 2260 = 0.10614093784242869
Training loss on iteration 2280 = 0.10637553706765175
Training loss on iteration 2300 = 0.10681165773421526
Training loss on iteration 2320 = 0.10549242310225963
Training loss on iteration 2340 = 0.10571366790682077
Training loss on iteration 2360 = 0.12640287540853024
Training loss on iteration 2380 = 0.10428617745637894
Training loss on iteration 2400 = 0.10643954537808895
Training loss on iteration 2420 = 0.11525149419903755
Training loss on iteration 2440 = 0.11149947270751
Training loss on iteration 2460 = 0.10612152218818664
Training loss on iteration 2480 = 0.10563930757343769
Training loss on iteration 2500 = 0.1108751617372036
Training loss on iteration 2520 = 0.09929460808634757
Training loss on iteration 2540 = 0.10555567126721144
Training loss on iteration 2560 = 0.11246860027313232
Training loss on iteration 2580 = 0.10529317371547223
Training loss on iteration 2600 = 0.10060112103819847
Training loss on iteration 2620 = 0.11105217505246401
Training loss on iteration 2640 = 0.10646413415670394
Training loss on iteration 2660 = 0.1138344556093216
Training loss on iteration 2680 = 0.11134110372513532
Training loss on iteration 2700 = 0.10633783265948296
Training loss on iteration 2720 = 0.11156028099358081
Training loss on iteration 2740 = 0.10870398990809918
Training loss on iteration 2760 = 0.11186262555420398
Training loss on iteration 2780 = 0.10138656869530678
Training loss on iteration 2800 = 0.11716371141374111
Training loss on iteration 2820 = 0.10472413804382086
Training loss on iteration 2840 = 0.11012994162738324
Training loss on iteration 2860 = 0.11302653625607491
Training loss on iteration 2880 = 0.11077097244560719
Training loss on iteration 2900 = 0.10309492144733667
Training loss on iteration 2920 = 0.09010368995368481
Training loss on iteration 2940 = 0.11916155889630317
Training loss on iteration 2960 = 0.09466966986656189
Training loss on iteration 2980 = 0.10770154390484095
Training loss on iteration 3000 = 0.09526109155267477
Training loss on iteration 3020 = 0.1118103839457035
Training loss on iteration 3040 = 0.10340158976614475
Training loss on iteration 3060 = 0.1077115524560213
Training loss on iteration 3080 = 0.10619404837489128
Training loss on iteration 3100 = 0.1266276143491268
Training loss on iteration 3120 = 0.10070372764021158
Training loss on iteration 3140 = 0.11796040125191212
Training loss on iteration 3160 = 0.11223538722842932
Training loss on iteration 3180 = 0.104772955365479
Training loss on iteration 3200 = 0.10695399641990662
Training loss on iteration 3220 = 0.10292915478348733
Training loss on iteration 3240 = 0.12686384040862322
Training loss on iteration 3260 = 0.11490055248141288
Training loss on iteration 3280 = 0.1163598995655775
Training loss on iteration 3300 = 0.12133366949856281
Training loss on iteration 3320 = 0.1119297355413437
Training loss on iteration 3340 = 0.09747527614235878
Training loss on iteration 3360 = 0.10099298357963563
Training loss on iteration 3380 = 0.1270783755928278
Training loss on iteration 3400 = 0.11850272081792354
Training loss on iteration 3420 = 0.11481888443231583
Training loss on iteration 3440 = 0.11105156168341637
Training loss on iteration 3460 = 0.10090167373418808
Training loss on iteration 3480 = 0.11823178976774215
Training loss on iteration 3500 = 0.10711693651974201
Training loss on iteration 3520 = 0.11626906469464302
Training loss on iteration 3540 = 0.1118716498836875
Training loss on iteration 3560 = 0.10916232205927372
Training loss on iteration 3580 = 0.10607041530311108
Training loss on iteration 3600 = 0.1054043360054493
Training loss on iteration 3620 = 0.11842328999191523
Training loss on iteration 3640 = 0.11880855970084667
Training loss on iteration 3660 = 0.10328944623470307
Training loss on iteration 3680 = 0.105032623372972
Training loss on iteration 3700 = 0.10042345114052295
Training loss on iteration 3720 = 0.11736851371824741
Training loss on iteration 3740 = 0.10897072069346905
Training loss on iteration 3760 = 0.11115722581744195
Training loss on iteration 3780 = 0.10377259105443955
Training loss on iteration 3800 = 0.11141705363988877
Training loss on iteration 3820 = 0.13243323788046837
Training loss on iteration 3840 = 0.11419954299926757
Training loss on iteration 3860 = 0.09597342796623706
Training loss on iteration 3880 = 0.12090583145618439
Training loss on iteration 3900 = 0.1014223039150238
Training loss on iteration 3920 = 0.10651972293853759
Training loss on iteration 3940 = 0.11379442475736141
Training loss on iteration 3960 = 0.11735905073583126
Training loss on iteration 3980 = 0.1226918175816536
Training loss on iteration 4000 = 0.12182565182447433
Training loss on iteration 4020 = 0.13975097350776194
Training loss on iteration 4040 = 0.11204000003635883
Training loss on iteration 4060 = 0.11800974383950233
Training loss on iteration 4080 = 0.12555133290588855
Training loss on iteration 4100 = 0.11518011167645455
Training loss on iteration 4120 = 0.11084910705685616
Training loss on iteration 4140 = 0.11243363209068775
Training loss on iteration 4160 = 0.10944416709244251
Training loss on iteration 4180 = 0.10559739731252193
Training loss on iteration 4200 = 0.10640393979847432
Training loss on iteration 4220 = 0.1263545874506235
Training loss on iteration 4240 = 0.11528879664838314
Training loss on iteration 4260 = 0.10988714322447776
Training loss on iteration 4280 = 0.11067221686244011
Training loss on iteration 4300 = 0.1066969906911254
Training loss on iteration 4320 = 0.10819377154111862
Training loss on iteration 4340 = 0.10896281991153955
Training loss on iteration 4360 = 0.09680574424564839
Training loss on iteration 4380 = 0.11613784581422806
Training loss on iteration 4400 = 0.1323959670960903
Training loss on iteration 4420 = 0.11383253820240498
Training loss on iteration 4440 = 0.09583356752991676
Training loss on iteration 4460 = 0.10283885858952999
Training loss on iteration 4480 = 0.12112462744116784
Training loss on iteration 4500 = 0.10580452755093575
Training loss on iteration 4520 = 0.10638547874987125
Training loss on iteration 4540 = 0.5905516143888235
Training loss on iteration 4560 = 0.11092436239123345
Training loss on iteration 4580 = 0.11546806693077087
Training loss on iteration 4600 = 0.11227312907576562
Training loss on iteration 4620 = 0.12487736046314239
Training loss on iteration 4640 = 0.1000561872497201
Training loss on iteration 4660 = 0.1112022865563631
Training loss on iteration 4680 = 0.10333028398454189
Training loss on iteration 4700 = 0.11625489499419928
Training loss on iteration 4720 = 0.10309860035777092
Training loss on iteration 4740 = 0.12201514244079589
Training loss on iteration 4760 = 0.10773804262280465
Training loss on iteration 4780 = 0.10900569558143616
Training loss on iteration 4800 = 0.09895745180547237
Training loss on iteration 4820 = 0.1126590583473444
Training loss on iteration 4840 = 0.12761223707348107
Training loss on iteration 4860 = 0.10567753352224826
Training loss on iteration 4880 = 0.09744791239500046
Training loss on iteration 4900 = 0.11262398436665536
Training loss on iteration 4920 = 0.10597321502864361
Training loss on iteration 4940 = 0.11078668236732483
Training loss on iteration 4960 = 0.10981468968093396
Training loss on iteration 4980 = 0.09606508444994688
Training loss on iteration 5000 = 0.11091558076441288
Training loss on iteration 5020 = 0.11588537059724331
Training loss on iteration 5040 = 0.11846857406198978
Training loss on iteration 5060 = 0.11374225541949272
Training loss on iteration 5080 = 0.09898653700947761
Training loss on iteration 5100 = 0.11465919576585293
Training loss on iteration 5120 = 0.26780201159417627
Training loss on iteration 5140 = 0.1479704950004816
Training loss on iteration 5160 = 0.1094312034547329
Training loss on iteration 5180 = 0.1151485376060009
Training loss on iteration 5200 = 0.10843513831496239
Training loss on iteration 5220 = 0.11763727739453315
Training loss on iteration 5240 = 0.10130215063691139
Training loss on iteration 5260 = 0.10682448782026768
Training loss on iteration 5280 = 0.1133379977196455
Training loss on iteration 5300 = 0.10791555009782314
Training loss on iteration 5320 = 0.1101074293255806
Training loss on iteration 5340 = 0.11074702814221382
Training loss on iteration 5360 = 0.1030329879373312
Training loss on iteration 5380 = 0.12168084792792797
Training loss on iteration 5400 = 0.10410768240690231
Training loss on iteration 5420 = 0.1053769413381815
Training loss on iteration 5440 = 0.1155348002910614
Training loss on iteration 5460 = 0.10990170650184154
Training loss on iteration 5480 = 0.10544620230793952
Training loss on iteration 5500 = 0.1048059869557619
Training loss on iteration 5520 = 0.11782869808375836
Training loss on iteration 5540 = 0.12541124355047942
Training loss on iteration 5560 = 0.09818522483110428
Training loss on iteration 5580 = 0.11102698408067227
Training loss on iteration 5600 = 0.10700054802000522
Training loss on iteration 5620 = 0.11665828954428434
Training loss on iteration 5640 = 0.09537338502705098
Training loss on iteration 5660 = 0.11742732971906662
Training loss on iteration 5680 = 0.10716303288936616
Training loss on iteration 5700 = 0.10689489170908928
Training loss on iteration 5720 = 0.11508451849222183
Training loss on iteration 5740 = 0.10318034924566746
Training loss on iteration 5760 = 0.11936121657490731
Training loss on iteration 5780 = 0.10861113294959068
Training loss on iteration 5800 = 0.10930665358901023
Training loss on iteration 5820 = 0.18804177418351173
Training loss on iteration 5840 = 0.10252534076571465
Training loss on iteration 5860 = 0.11469439268112183
Training loss on iteration 5880 = 0.1128566300496459
Training loss on iteration 5900 = 0.10402325559407473
Training loss on iteration 5920 = 0.11252476833760738
Training loss on iteration 5940 = 0.11295435465872287
Training loss on iteration 5960 = 0.11518236510455608
Training loss on iteration 5980 = 0.1085530374199152
Training loss on iteration 6000 = 0.10499055851250887
Training loss on iteration 6020 = 0.12336458638310432
Training loss on iteration 6040 = 0.10470276102423667
Training loss on iteration 6060 = 0.10835345052182674
Training loss on iteration 6080 = 0.10916833914816379
Training loss on iteration 6100 = 0.10772316604852676
Training loss on iteration 6120 = 0.106522286683321
Training loss on iteration 6140 = 0.11834855787456036
Training loss on iteration 6160 = 0.11489503681659699
Training loss on iteration 6180 = 0.10847977064549923
Training loss on iteration 6200 = 0.11258556246757508
Training loss on iteration 6220 = 0.11231765076518059
Training loss on iteration 6240 = 0.11975983716547489
Training loss on iteration 6260 = 0.11297453790903092
Training loss on iteration 6280 = 0.10528185553848743
Training loss on iteration 6300 = 0.1086927019059658
Training loss on iteration 6320 = 0.10231039579957724
Training loss on iteration 6340 = 0.105764339864254
Training loss on iteration 6360 = 0.10550985410809517
Training loss on iteration 6380 = 0.11350862234830857
Training loss on iteration 0 = 0.09993281215429306
Training loss on iteration 20 = 0.09408637899905443
Training loss on iteration 40 = 0.09465850293636321
Training loss on iteration 60 = 0.08807339202612638
Training loss on iteration 80 = 0.09538252931088209
Training loss on iteration 100 = 0.09691224321722984
Training loss on iteration 120 = 0.13259386345744134
Training loss on iteration 140 = 0.09775166232138872
Training loss on iteration 160 = 0.10761859789490699
Training loss on iteration 180 = 0.10086990967392921
Training loss on iteration 200 = 0.09444052260369062
Training loss on iteration 220 = 0.10486837178468704
Training loss on iteration 240 = 0.11174189168959855
Training loss on iteration 260 = 0.10501730740070343
Training loss on iteration 280 = 0.10833722241222858
Training loss on iteration 300 = 0.09921604804694653
Training loss on iteration 320 = 0.0981903277337551
Training loss on iteration 340 = 0.10415913574397564
Training loss on iteration 360 = 0.10650584846735
Training loss on iteration 380 = 0.1112880127504468
Training loss on iteration 400 = 0.10684358086436987
Training loss on iteration 420 = 0.0951933853328228
Training loss on iteration 440 = 0.10617781095206738
Training loss on iteration 460 = 0.09735645707696676
Training loss on iteration 480 = 0.10045750886201858
Training loss on iteration 500 = 0.09841773249208927
Training loss on iteration 520 = 0.11800877377390862
Training loss on iteration 540 = 0.09889817852526903
Training loss on iteration 560 = 0.10174809861928225
Training loss on iteration 580 = 0.12908915970474483
Training loss on iteration 600 = 0.10505260527133942
Training loss on iteration 620 = 0.10813663136214018
Training loss on iteration 640 = 0.09499550089240075
Training loss on iteration 660 = 0.09333231784403324
Training loss on iteration 680 = 0.16198116149753333
Training loss on iteration 700 = 0.10212258622050285
Training loss on iteration 720 = 0.09692983701825142
Training loss on iteration 740 = 0.09753436185419559
Training loss on iteration 760 = 0.11491899862885475
Training loss on iteration 780 = 0.12487191744148732
Training loss on iteration 800 = 0.09736241512000561
Training loss on iteration 820 = 0.09906055852770805
Training loss on iteration 840 = 0.10129637606441974
Training loss on iteration 860 = 0.11040982194244861
Training loss on iteration 880 = 0.10749109461903572
Training loss on iteration 900 = 0.09556548465043306
Training loss on iteration 920 = 0.10202979948371649
Training loss on iteration 940 = 0.0994358230382204
Training loss on iteration 960 = 0.09436637833714485
Training loss on iteration 980 = 0.10592279750853777
Training loss on iteration 1000 = 0.1030346293002367
Training loss on iteration 1020 = 0.12730374969542027
Training loss on iteration 1040 = 0.09223266094923019
Training loss on iteration 1060 = 0.10761424601078033
Training loss on iteration 1080 = 0.10693093873560429
Training loss on iteration 1100 = 0.10689386613667011
Training loss on iteration 1120 = 0.11172820143401622
Training loss on iteration 1140 = 0.08881756886839867
Training loss on iteration 1160 = 0.09562544245272875
Training loss on iteration 1180 = 0.10435512326657773
Training loss on iteration 1200 = 0.10494976975023747
Training loss on iteration 1220 = 0.09984322804957628
Training loss on iteration 1240 = 0.09917443580925464
Training loss on iteration 1260 = 0.09584756568074226
Training loss on iteration 1280 = 0.09960689563304186
Training loss on iteration 1300 = 0.1013983029872179
Training loss on iteration 1320 = 0.09599813614040613
Training loss on iteration 1340 = 0.11814993172883988
Training loss on iteration 1360 = 0.10956255123019218
Training loss on iteration 1380 = 0.10399401746690273
Training loss on iteration 1400 = 0.10951067917048932
Training loss on iteration 1420 = 0.10121124275028706
Training loss on iteration 1440 = 0.10249425396323204
Training loss on iteration 1460 = 0.09475265108048916
Training loss on iteration 1480 = 0.09713728241622448
Training loss on iteration 1500 = 0.11019780151546002
Training loss on iteration 1520 = 0.08588942550122738
Training loss on iteration 1540 = 0.1038537498563528
Training loss on iteration 1560 = 0.10945986155420542
Training loss on iteration 1580 = 0.09631064403802156
Training loss on iteration 1600 = 0.11989660821855068
Training loss on iteration 1620 = 0.09348143599927425
Training loss on iteration 1640 = 0.10012314617633819
Training loss on iteration 1660 = 0.09591068159788847
Training loss on iteration 1680 = 0.09624046999961137
Training loss on iteration 1700 = 0.10428593680262566
Training loss on iteration 1720 = 0.09504682272672653
Training loss on iteration 1740 = 0.10208045337349177
Training loss on iteration 1760 = 0.10496797617524863
Training loss on iteration 1780 = 0.11499653570353985
Training loss on iteration 1800 = 0.09977630376815796
Training loss on iteration 1820 = 0.09608135838061571
Training loss on iteration 1840 = 0.10513435080647468
Training loss on iteration 1860 = 0.12034438401460648
Training loss on iteration 1880 = 0.09785583689808845
Training loss on iteration 1900 = 0.09577825702726842
Training loss on iteration 1920 = 0.09980165995657445
Training loss on iteration 1940 = 0.11686569396406413
Training loss on iteration 1960 = 0.09746534563601017
Training loss on iteration 1980 = 0.10242898799479008
Training loss on iteration 2000 = 0.10458935648202897
Training loss on iteration 2020 = 0.10378756150603294
Training loss on iteration 2040 = 0.10879711955785751
Training loss on iteration 2060 = 0.11322989389300346
Training loss on iteration 2080 = 0.09317232668399811
Training loss on iteration 2100 = 0.1050523653626442
Training loss on iteration 2120 = 0.10427755210548639
Training loss on iteration 2140 = 0.10178126953542233
Training loss on iteration 2160 = 0.10810181684792042
Training loss on iteration 2180 = 0.09961815364658833
Training loss on iteration 2200 = 0.09817690551280975
Training loss on iteration 2220 = 0.10580282993614673
Training loss on iteration 2240 = 0.09772563893347978
Training loss on iteration 2260 = 0.10286729298532009
Training loss on iteration 2280 = 0.10229455437511206
Training loss on iteration 2300 = 0.09768865108489991
Training loss on iteration 2320 = 0.09504707828164101
Training loss on iteration 2340 = 0.10100541263818741
Training loss on iteration 2360 = 0.10807416997849942
Training loss on iteration 2380 = 0.09591629989445209
Training loss on iteration 2400 = 0.10458607226610184
Training loss on iteration 2420 = 0.10686066150665283
Training loss on iteration 2440 = 0.10715685188770294
Training loss on iteration 2460 = 0.10556852966547012
Training loss on iteration 2480 = 0.10090115927159786
Training loss on iteration 2500 = 0.10208119861781598
Training loss on iteration 2520 = 0.1115212868899107
Training loss on iteration 2540 = 0.1109564419835806
Training loss on iteration 2560 = 0.10487103685736657
Training loss on iteration 2580 = 0.09463937599211932
Training loss on iteration 2600 = 0.10128211565315723
Training loss on iteration 2620 = 0.09553752597421408
Training loss on iteration 2640 = 0.10003498531877994
Training loss on iteration 2660 = 0.09821387752890587
Training loss on iteration 2680 = 0.1029580645263195
Training loss on iteration 2700 = 0.10658830106258392
Training loss on iteration 2720 = 0.10242784768342972
Training loss on iteration 2740 = 0.10011727698147296
Training loss on iteration 2760 = 0.10994261074811221
Training loss on iteration 2780 = 0.1056071650236845
Training loss on iteration 2800 = 0.10754738058894872
Training loss on iteration 2820 = 0.11228557229042054
Training loss on iteration 2840 = 0.10295431464910507
Training loss on iteration 2860 = 0.10290384739637375
Training loss on iteration 2880 = 0.10965646468102933
Training loss on iteration 2900 = 0.10634533204138279
Training loss on iteration 2920 = 0.11053663566708564
Training loss on iteration 2940 = 0.09494131673127412
Training loss on iteration 2960 = 0.11038383245468139
Training loss on iteration 2980 = 0.1099395252764225
Training loss on iteration 3000 = 0.09728457666933536
Training loss on iteration 3020 = 0.09876718930900097
Training loss on iteration 3040 = 0.10660403892397881
Training loss on iteration 3060 = 0.10623083226382732
Training loss on iteration 3080 = 0.11319608949124813
Training loss on iteration 3100 = 0.10257428623735905
Training loss on iteration 3120 = 0.11071597281843423
Training loss on iteration 3140 = 0.11492133364081383
Training loss on iteration 3160 = 0.10210257209837437
Training loss on iteration 3180 = 0.09555290564894676
Training loss on iteration 3200 = 0.10112087316811084
Training loss on iteration 3220 = 0.11174936033785343
Training loss on iteration 3240 = 0.10427263267338276
Training loss on iteration 3260 = 0.1102967295795679
Training loss on iteration 3280 = 0.1144849058240652
Training loss on iteration 3300 = 0.10568308494985104
Training loss on iteration 3320 = 0.09444594718515872
Training loss on iteration 3340 = 0.10253713484853506
Training loss on iteration 3360 = 0.11623172909021377
Training loss on iteration 3380 = 0.1044796161353588
Training loss on iteration 3400 = 0.11206631064414978
Training loss on iteration 3420 = 0.10452404953539371
Training loss on iteration 3440 = 0.1294557934626937
Training loss on iteration 3460 = 0.10376251507550478
Training loss on iteration 3480 = 0.12392358258366584
Training loss on iteration 3500 = 0.09768213145434856
Training loss on iteration 3520 = 0.10535157658159733
Training loss on iteration 3540 = 0.09938363023102284
Training loss on iteration 3560 = 0.10731839872896672
Training loss on iteration 3580 = 0.11212016735225916
Training loss on iteration 3600 = 0.10169405844062566
Training loss on iteration 3620 = 0.10302920117974282
Training loss on iteration 3640 = 0.116069246083498
Training loss on iteration 3660 = 0.11209608800709248
Training loss on iteration 3680 = 0.09849264957010746
Training loss on iteration 3700 = 0.10809001438319683
Training loss on iteration 3720 = 0.10268063135445119
Training loss on iteration 3740 = 0.2565358757972717
Training loss on iteration 3760 = 0.2775453232228756
Training loss on iteration 3780 = 0.10166742019355297
Training loss on iteration 3800 = 0.12065489981323481
Training loss on iteration 3820 = 0.12129119522869587
Training loss on iteration 3840 = 0.10599710587412119
Training loss on iteration 3860 = 0.11339885760098696
Training loss on iteration 3880 = 0.1128319852054119
Training loss on iteration 3900 = 0.10199353210628033
Training loss on iteration 3920 = 0.0997862895950675
Training loss on iteration 3940 = 0.10090383179485798
Training loss on iteration 3960 = 0.10837069377303124
Training loss on iteration 3980 = 0.1100185263901949
Training loss on iteration 4000 = 0.1163195550441742
Training loss on iteration 4020 = 0.10198832526803017
Training loss on iteration 4040 = 0.10328401476144791
Training loss on iteration 4060 = 0.0923831719905138
Training loss on iteration 4080 = 0.10707245860248804
Training loss on iteration 4100 = 0.11336955726146698
Training loss on iteration 4120 = 0.10570824071764946
Training loss on iteration 4140 = 0.12359333075582982
Training loss on iteration 4160 = 0.10349977016448975
Training loss on iteration 4180 = 0.11698945350944996
Training loss on iteration 4200 = 0.12002987675368786
Training loss on iteration 4220 = 0.11356421262025833
Training loss on iteration 4240 = 0.10190462470054626
Training loss on iteration 4260 = 0.10484019443392753
Training loss on iteration 4280 = 0.10196265652775764
Training loss on iteration 4300 = 0.116610973700881
Training loss on iteration 4320 = 0.10904387831687927
Training loss on iteration 4340 = 0.11060370653867721
Training loss on iteration 4360 = 0.11088095419108868
Training loss on iteration 4380 = 0.10956473983824253
Training loss on iteration 4400 = 0.10243638344109059
Training loss on iteration 4420 = 0.11578521616756916
Training loss on iteration 4440 = 0.11487270481884479
Training loss on iteration 4460 = 0.11410096660256386
Training loss on iteration 4480 = 0.11443172078579664
Training loss on iteration 4500 = 0.0935033418238163
Training loss on iteration 4520 = 0.10514465272426606
Training loss on iteration 4540 = 0.10641236752271652
Training loss on iteration 4560 = 0.09459549840539694
Training loss on iteration 4580 = 0.09988129325211048
Training loss on iteration 4600 = 0.1135959044098854
Training loss on iteration 4620 = 0.1071461196988821
Training loss on iteration 4640 = 0.09498908631503582
Training loss on iteration 4660 = 0.16726563237607478
Training loss on iteration 4680 = 0.10859654098749161
Training loss on iteration 4700 = 0.1355771992355585
Training loss on iteration 4720 = 0.10658892542123795
Training loss on iteration 4740 = 0.09759354777634144
Training loss on iteration 4760 = 0.09364597704261542
Training loss on iteration 4780 = 0.10439607296139002
Training loss on iteration 4800 = 0.10849094577133656
Training loss on iteration 4820 = 0.12151651792228221
Training loss on iteration 4840 = 0.09634620733559132
Training loss on iteration 4860 = 0.10289654023945331
Training loss on iteration 4880 = 0.10821690745651721
Training loss on iteration 4900 = 0.1111387174576521
Training loss on iteration 4920 = 0.10490495841950179
Training loss on iteration 4940 = 0.10487044677138328
Training loss on iteration 4960 = 0.10643651634454727
Training loss on iteration 4980 = 0.0966653898358345
Training loss on iteration 5000 = 0.10198733173310756
Training loss on iteration 5020 = 0.12074741385877133
Training loss on iteration 5040 = 0.10184898301959038
Training loss on iteration 5060 = 0.1172065407037735
Training loss on iteration 5080 = 0.11081060767173767
Training loss on iteration 5100 = 0.09841670654714108
Training loss on iteration 5120 = 0.1120529880747199
Training loss on iteration 5140 = 0.10934585016220807
Training loss on iteration 5160 = 0.09626586474478245
Training loss on iteration 5180 = 0.10492947474122047
Training loss on iteration 5200 = 0.11444655992090702
Training loss on iteration 5220 = 0.11635035648941994
Training loss on iteration 5240 = 0.11310187689960002
Training loss on iteration 5260 = 0.10890331212431192
Training loss on iteration 5280 = 0.11564480774104595
Training loss on iteration 5300 = 0.11214762441813945
Training loss on iteration 5320 = 0.11142161022871733
Training loss on iteration 5340 = 0.09162945970892906
Training loss on iteration 5360 = 0.09854893200099468
Training loss on iteration 5380 = 0.10152934528887272
Training loss on iteration 5400 = 0.1234904769808054
Training loss on iteration 5420 = 0.12419777698814868
Training loss on iteration 5440 = 0.10231935754418373
Training loss on iteration 5460 = 0.10858270525932312
Training loss on iteration 5480 = 0.11950303837656975
Training loss on iteration 5500 = 0.1048507196828723
Training loss on iteration 5520 = 0.10294083673506975
Training loss on iteration 5540 = 0.10636595711112022
Training loss on iteration 5560 = 0.1131619319319725
Training loss on iteration 5580 = 0.11123135574162006
Training loss on iteration 5600 = 0.09018417447805405
Training loss on iteration 5620 = 0.11347024701535702
Training loss on iteration 5640 = 0.10657689180225134
Training loss on iteration 5660 = 0.09841192364692689
Training loss on iteration 5680 = 0.11261598393321037
Training loss on iteration 5700 = 0.10038616806268692
Training loss on iteration 5720 = 0.10555537305772304
Training loss on iteration 5740 = 0.10148823074996471
Training loss on iteration 5760 = 0.10693934410810471
Training loss on iteration 5780 = 0.10123623162508011
Training loss on iteration 5800 = 0.10342033375054598
Training loss on iteration 5820 = 0.09221227951347828
Training loss on iteration 5840 = 0.11136014126241207
Training loss on iteration 5860 = 0.10380209181457759
Training loss on iteration 5880 = 0.11323385760188102
Training loss on iteration 5900 = 0.11813695281744004
Training loss on iteration 5920 = 0.1058784168213606
Training loss on iteration 5940 = 0.10562997311353683
Training loss on iteration 5960 = 0.10587093904614449
Training loss on iteration 5980 = 0.11238146014511585
Training loss on iteration 6000 = 0.10660452730953693
Training loss on iteration 6020 = 0.11359953470528125
Training loss on iteration 6040 = 0.11091412380337715
Training loss on iteration 6060 = 0.09651541914790869
Training loss on iteration 6080 = 0.10944549664855004
Training loss on iteration 6100 = 0.11574283912777901
Training loss on iteration 6120 = 0.09737067222595215
Training loss on iteration 6140 = 0.11692652907222509
Training loss on iteration 6160 = 0.10500213988125325
Training loss on iteration 6180 = 0.10385192483663559
Training loss on iteration 6200 = 0.10921208746731281
Training loss on iteration 6220 = 0.11277253963053227
Training loss on iteration 6240 = 0.09655788280069828
Training loss on iteration 6260 = 0.10523774288594723
Training loss on iteration 6280 = 0.10716392137110234
Training loss on iteration 6300 = 0.10789897702634335
Training loss on iteration 6320 = 0.11054753921926022
Training loss on iteration 6340 = 0.10834459513425827
Training loss on iteration 6360 = 0.10925723090767861
Training loss on iteration 6380 = 0.12621486261487008
Training loss on iteration 0 = 0.08026524633169174
Training loss on iteration 20 = 0.09367528855800629
Training loss on iteration 40 = 0.08707653600722551
Training loss on iteration 60 = 0.09805901497602462
Training loss on iteration 80 = 0.10508638955652713
Training loss on iteration 100 = 0.09737119153141975
Training loss on iteration 120 = 0.09264969192445278
Training loss on iteration 140 = 0.09211571421474218
Training loss on iteration 160 = 0.091965901106596
Training loss on iteration 180 = 0.10230547394603491
Training loss on iteration 200 = 0.10011118855327368
Training loss on iteration 220 = 0.10914099626243115
Training loss on iteration 240 = 0.08993228096514941
Training loss on iteration 260 = 0.09887492638081312
Training loss on iteration 280 = 0.09420403689146042
Training loss on iteration 300 = 0.09241836965084076
Training loss on iteration 320 = 0.09829784221947194
Training loss on iteration 340 = 0.09986461289227008
Training loss on iteration 360 = 0.10423971563577653
Training loss on iteration 380 = 0.10375306718051433
Training loss on iteration 400 = 0.094510362111032
Training loss on iteration 420 = 0.10723472740501165
Training loss on iteration 440 = 0.10143563747406006
Training loss on iteration 460 = 0.09862798489630223
Training loss on iteration 480 = 0.08778247311711311
Training loss on iteration 500 = 0.10734757222235203
Training loss on iteration 520 = 0.09341995567083358
Training loss on iteration 540 = 0.11976174041628837
Training loss on iteration 560 = 0.09519017860293388
Training loss on iteration 580 = 0.11332927905023098
Training loss on iteration 600 = 0.09348524771630765
Training loss on iteration 620 = 0.09843092635273934
Training loss on iteration 640 = 0.0952499482780695
Training loss on iteration 660 = 0.08875818978995084
Training loss on iteration 680 = 0.10116706192493438
Training loss on iteration 700 = 0.10231677014380694
Training loss on iteration 720 = 0.09143057484179735
Training loss on iteration 740 = 0.09986277222633362
Training loss on iteration 760 = 0.10309944376349449
Training loss on iteration 780 = 0.09425050169229507
Training loss on iteration 800 = 0.10023429244756699
Training loss on iteration 820 = 0.10087536890059709
Training loss on iteration 840 = 0.09659963417798281
Training loss on iteration 860 = 0.09480928517878055
Training loss on iteration 880 = 0.09478781670331955
Training loss on iteration 900 = 0.09730914905667305
Training loss on iteration 920 = 0.09948666226118803
Training loss on iteration 940 = 0.09533863347023726
Training loss on iteration 960 = 0.10012734308838844
Training loss on iteration 980 = 0.1046172471717
Training loss on iteration 1000 = 0.0969238456338644
Training loss on iteration 1020 = 0.09484262838959694
Training loss on iteration 1040 = 0.10036912113428116
Training loss on iteration 1060 = 0.09623887650668621
Training loss on iteration 1080 = 0.09969553351402283
Training loss on iteration 1100 = 0.10115010403096676
Training loss on iteration 1120 = 0.09548417404294014
Training loss on iteration 1140 = 0.09919761903584004
Training loss on iteration 1160 = 0.10053732227534055
Training loss on iteration 1180 = 0.10798893310129642
Training loss on iteration 1200 = 0.10445213057100773
Training loss on iteration 1220 = 0.09915009737014771
Training loss on iteration 1240 = 0.09334826674312353
Training loss on iteration 1260 = 0.09528757892549038
Training loss on iteration 1280 = 0.09968435317277909
Training loss on iteration 1300 = 0.0889915743842721
Training loss on iteration 1320 = 0.1284035922959447
Training loss on iteration 1340 = 0.09956802688539028
Training loss on iteration 1360 = 0.09305400345474482
Training loss on iteration 1380 = 0.12947780210524798
Training loss on iteration 1400 = 0.10367529541254043
Training loss on iteration 1420 = 0.10769129022955895
Training loss on iteration 1440 = 0.0991776742041111
Training loss on iteration 1460 = 0.08818689174950123
Training loss on iteration 1480 = 0.10256269536912441
Training loss on iteration 1500 = 0.09936483688652516
Training loss on iteration 1520 = 0.0921963781118393
Training loss on iteration 1540 = 0.0949440075084567
Training loss on iteration 1560 = 0.10450109876692296
Training loss on iteration 1580 = 0.10125949494540691
Training loss on iteration 1600 = 0.09542672224342823
Training loss on iteration 1620 = 0.10038608349859715
Training loss on iteration 1640 = 0.09730730131268502
Training loss on iteration 1660 = 0.10110736228525638
Training loss on iteration 1680 = 0.11603732146322727
Training loss on iteration 1700 = 0.11826796066015959
Training loss on iteration 1720 = 0.09822811745107174
Training loss on iteration 1740 = 0.10249824933707714
Training loss on iteration 1760 = 0.09817774184048175
Training loss on iteration 1780 = 0.08997400887310505
Training loss on iteration 1800 = 0.09424400851130485
Training loss on iteration 1820 = 0.11146614998579026
Training loss on iteration 1840 = 0.105313665792346
Training loss on iteration 1860 = 0.11367422249168158
Training loss on iteration 1880 = 0.10050026457756758
Training loss on iteration 1900 = 0.10621400028467179
Training loss on iteration 1920 = 0.0919803911820054
Training loss on iteration 1940 = 0.09762878865003585
Training loss on iteration 1960 = 0.10180167146027089
Training loss on iteration 1980 = 0.09887739531695842
Training loss on iteration 2000 = 0.09834809973835945
Training loss on iteration 2020 = 0.10213726870715618
Training loss on iteration 2040 = 0.0969119317829609
Training loss on iteration 2060 = 0.10614642277359962
Training loss on iteration 2080 = 0.11104253083467483
Training loss on iteration 2100 = 0.10368782915174961
Training loss on iteration 2120 = 0.10223388634622096
Training loss on iteration 2140 = 0.09508397467434407
Training loss on iteration 2160 = 0.10242399349808692
Training loss on iteration 2180 = 0.10348105505108833
Training loss on iteration 2200 = 0.1014654714614153
Training loss on iteration 2220 = 0.11301360540091991
Training loss on iteration 2240 = 0.09827823005616665
Training loss on iteration 2260 = 0.10530419833958149
Training loss on iteration 2280 = 0.09984502829611301
Training loss on iteration 2300 = 0.10726717226207257
Training loss on iteration 2320 = 0.09537951108068228
Training loss on iteration 2340 = 0.13434839360415934
Training loss on iteration 2360 = 0.10056753680109978
Training loss on iteration 2380 = 0.10261885952204466
Training loss on iteration 2400 = 0.10268577523529529
Training loss on iteration 2420 = 0.1079752691090107
Training loss on iteration 2440 = 0.09808282777667046
Training loss on iteration 2460 = 0.10352375879883766
Training loss on iteration 2480 = 0.12635586895048617
Training loss on iteration 2500 = 0.1038750883191824
Training loss on iteration 2520 = 0.09754871651530266
Training loss on iteration 2540 = 0.09649652130901813
Training loss on iteration 2560 = 0.10078335590660573
Training loss on iteration 2580 = 0.09110445901751518
Training loss on iteration 2600 = 0.10642805360257626
Training loss on iteration 2620 = 0.10482894778251647
Training loss on iteration 2640 = 0.09611911792308092
Training loss on iteration 2660 = 0.09447158370167016
Training loss on iteration 2680 = 0.09465327896177769
Training loss on iteration 2700 = 0.10527009777724743
Training loss on iteration 2720 = 0.1328297145664692
Training loss on iteration 2740 = 0.10376028381288052
Training loss on iteration 2760 = 0.10282537061721087
Training loss on iteration 2780 = 0.11388929598033429
Training loss on iteration 2800 = 0.10313128866255283
Training loss on iteration 2820 = 0.10872795153409243
Training loss on iteration 2840 = 0.10772565566003323
Training loss on iteration 2860 = 0.1106285635381937
Training loss on iteration 2880 = 0.10228735357522964
Training loss on iteration 2900 = 0.1021890688687563
Training loss on iteration 2920 = 0.10920449644327164
Training loss on iteration 2940 = 0.09668562803417444
Training loss on iteration 2960 = 0.10774901770055294
Training loss on iteration 2980 = 0.10383226200938225
Training loss on iteration 3000 = 0.11596024669706821
Training loss on iteration 3020 = 0.11817970983684063
Training loss on iteration 3040 = 0.10059950016438961
Training loss on iteration 3060 = 0.10299353320151568
Training loss on iteration 3080 = 0.10432085804641247
Training loss on iteration 3100 = 0.10295679941773414
Training loss on iteration 3120 = 0.10146395824849605
Training loss on iteration 3140 = 0.10281018316745758
Training loss on iteration 3160 = 0.10131095051765442
Training loss on iteration 3180 = 0.10774244330823421
Training loss on iteration 3200 = 0.1022454135119915
Training loss on iteration 3220 = 0.10132502671331167
Training loss on iteration 3240 = 0.1061542771756649
Training loss on iteration 3260 = 0.10370896439999341
Training loss on iteration 3280 = 0.10053025577217341
Training loss on iteration 3300 = 0.09811479113996029
Training loss on iteration 3320 = 0.11032513212412595
Training loss on iteration 3340 = 0.09991508908569813
Training loss on iteration 3360 = 0.11023409496992827
Training loss on iteration 3380 = 0.09925909861922264
Training loss on iteration 3400 = 0.12035612687468529
Training loss on iteration 3420 = 0.10580637101083994
Training loss on iteration 3440 = 0.10016078501939774
Training loss on iteration 3460 = 0.10044382400810718
Training loss on iteration 3480 = 0.09371110685169697
Training loss on iteration 3500 = 0.09942977987229824
Training loss on iteration 3520 = 0.10302219353616238
Training loss on iteration 3540 = 0.10273009538650513
Training loss on iteration 3560 = 0.09776091948151588
Training loss on iteration 3580 = 0.11616180054843425
Training loss on iteration 3600 = 0.0990162257105112
Training loss on iteration 3620 = 0.11187837943434716
Training loss on iteration 3640 = 0.10089764036238194
Training loss on iteration 3660 = 0.1051892288029194
Training loss on iteration 3680 = 0.10508513115346432
Training loss on iteration 3700 = 0.0890356507152319
Training loss on iteration 3720 = 0.10442054606974124
Training loss on iteration 3740 = 0.16556682847440243
Training loss on iteration 3760 = 0.11895992364734412
Training loss on iteration 3780 = 0.09664023518562317
Training loss on iteration 3800 = 0.10270450096577406
Training loss on iteration 3820 = 0.0962395690381527
Training loss on iteration 3840 = 0.10476402342319488
Training loss on iteration 3860 = 0.1012886930257082
Training loss on iteration 3880 = 0.1040363848209381
Training loss on iteration 3900 = 0.10372344143688679
Training loss on iteration 3920 = 0.10385632030665874
Training loss on iteration 3940 = 0.09913494102656842
Training loss on iteration 3960 = 0.10615694131702184
Training loss on iteration 3980 = 0.11027144305408002
Training loss on iteration 4000 = 0.11292758993804455
Training loss on iteration 4020 = 0.09761210381984711
Training loss on iteration 4040 = 0.11266519874334335
Training loss on iteration 4060 = 0.10483213905245066
Training loss on iteration 4080 = 0.12063863724470139
Training loss on iteration 4100 = 0.11139825209975243
Training loss on iteration 4120 = 0.10455396473407745
Training loss on iteration 4140 = 0.10550106503069401
Training loss on iteration 4160 = 0.1376009240746498
Training loss on iteration 4180 = 0.10318294577300549
Training loss on iteration 4200 = 0.1329592488706112
Training loss on iteration 4220 = 0.10523717552423477
Training loss on iteration 4240 = 0.0997538086026907
Training loss on iteration 4260 = 0.12059452682733536
Training loss on iteration 4280 = 0.10007481388747692
Training loss on iteration 4300 = 0.0985194493085146
Training loss on iteration 4320 = 0.10102308914065361
Training loss on iteration 4340 = 0.0996124155819416
Training loss on iteration 4360 = 0.10682501532137394
Training loss on iteration 4380 = 0.2535956636071205
Training loss on iteration 4400 = 0.0936323707923293
Training loss on iteration 4420 = 0.10672586299479007
Training loss on iteration 4440 = 0.11433561965823173
Training loss on iteration 4460 = 0.10757734701037407
Training loss on iteration 4480 = 0.10864964537322522
Training loss on iteration 4500 = 0.10087907630950213
Training loss on iteration 4520 = 0.11181202419102192
Training loss on iteration 4540 = 0.11853734329342842
Training loss on iteration 4560 = 0.10569470897316932
Training loss on iteration 4580 = 0.09730455912649631
Training loss on iteration 4600 = 0.1000854630023241
Training loss on iteration 4620 = 0.09386453330516815
Training loss on iteration 4640 = 0.10242281071841716
Training loss on iteration 4660 = 0.09977968130260706
Training loss on iteration 4680 = 0.10192026663571596
Training loss on iteration 4700 = 0.11688938327133655
Training loss on iteration 4720 = 0.11136126257479191
Training loss on iteration 4740 = 0.10596691220998763
Training loss on iteration 4760 = 0.1016014140099287
Training loss on iteration 4780 = 0.11212683543562889
Training loss on iteration 4800 = 0.09325176440179347
Training loss on iteration 4820 = 0.10612815711647272
Training loss on iteration 4840 = 0.11091023571789264
Training loss on iteration 4860 = 0.0975094310939312
Training loss on iteration 4880 = 0.11288665533065796
Training loss on iteration 4900 = 0.10930424258112907
Training loss on iteration 4920 = 0.10708527415990829
Training loss on iteration 4940 = 0.1309872679412365
Training loss on iteration 4960 = 0.11892478093504906
Training loss on iteration 4980 = 0.10491616167128086
Training loss on iteration 5000 = 0.0987865936011076
Training loss on iteration 5020 = 0.1053814172744751
Training loss on iteration 5040 = 0.11893141008913517
Training loss on iteration 5060 = 0.09074186366051436
Training loss on iteration 5080 = 0.10664037447422743
Training loss on iteration 5100 = 0.10664858743548393
Training loss on iteration 5120 = 0.09446790013462306
Training loss on iteration 5140 = 0.10756080858409404
Training loss on iteration 5160 = 0.1077264178544283
Training loss on iteration 5180 = 0.1033765759319067
Training loss on iteration 5200 = 0.10842175967991352
Training loss on iteration 5220 = 0.10532651953399182
Training loss on iteration 5240 = 0.10209588930010796
Training loss on iteration 5260 = 0.11271881423890591
Training loss on iteration 5280 = 0.10890238806605339
Training loss on iteration 5300 = 0.12203263286501169
Training loss on iteration 5320 = 0.09413328152149916
Training loss on iteration 5340 = 0.09149704203009605
Training loss on iteration 5360 = 0.1058828067034483
Training loss on iteration 5380 = 0.11084545403718948
Training loss on iteration 5400 = 0.10806627683341503
Training loss on iteration 5420 = 0.12472143024206161
Training loss on iteration 5440 = 0.10469071324914694
Training loss on iteration 5460 = 0.11260300017893314
Training loss on iteration 5480 = 0.10232063494622708
Training loss on iteration 5500 = 0.11893955767154693
Training loss on iteration 5520 = 0.09746223390102386
Training loss on iteration 5540 = 0.10700180828571319
Training loss on iteration 5560 = 0.10380642376840114
Training loss on iteration 5580 = 0.10578934773802758
Training loss on iteration 5600 = 0.10481171794235707
Training loss on iteration 5620 = 0.10411665327847004
Training loss on iteration 5640 = 0.10035397000610828
Training loss on iteration 5660 = 0.09769566375762224
Training loss on iteration 5680 = 0.10162550434470177
Training loss on iteration 5700 = 0.10178834199905396
Training loss on iteration 5720 = 0.10718709044158459
Training loss on iteration 5740 = 0.1102323517203331
Training loss on iteration 5760 = 0.09758376255631447
Training loss on iteration 5780 = 0.10524769648909568
Training loss on iteration 5800 = 0.10119095407426357
Training loss on iteration 5820 = 0.10138316042721271
Training loss on iteration 5840 = 0.1018395908176899
Training loss on iteration 5860 = 0.11093897148966789
Training loss on iteration 5880 = 0.10847516320645809
Training loss on iteration 5900 = 0.1069024283438921
Training loss on iteration 5920 = 0.10620779916644096
Training loss on iteration 5940 = 0.10472937822341918
Training loss on iteration 5960 = 0.10122708398848772
Training loss on iteration 5980 = 0.10341669879853725
Training loss on iteration 6000 = 0.10817834679037333
Training loss on iteration 6020 = 0.1161248568445444
Training loss on iteration 6040 = 0.120345439016819
Training loss on iteration 6060 = 0.10282723382115364
Training loss on iteration 6080 = 0.09965099431574345
Training loss on iteration 6100 = 0.11298087425529957
Training loss on iteration 6120 = 0.107461491599679
Training loss on iteration 6140 = 0.11230368167161942
Training loss on iteration 6160 = 0.10250602774322033
Training loss on iteration 6180 = 0.10571572631597519
Training loss on iteration 6200 = 0.10341924093663693
Training loss on iteration 6220 = 0.10944692119956016
Training loss on iteration 6240 = 0.11504034213721752
Training loss on iteration 6260 = 0.11972142159938812
Training loss on iteration 6280 = 0.09472152553498744
Training loss on iteration 6300 = 0.10325565934181213
Training loss on iteration 6320 = 0.12676523812115192
Training loss on iteration 6340 = 0.11312415301799775
Training loss on iteration 6360 = 0.2684716012328863
Training loss on iteration 6380 = 0.11058305352926254
Training loss on iteration 0 = 0.0726744681596756
Training loss on iteration 20 = 0.09995749145746231
Training loss on iteration 40 = 0.10510794688016176
Training loss on iteration 60 = 0.09502393994480371
Training loss on iteration 80 = 0.0895145857706666
Training loss on iteration 100 = 0.09275438170880079
Training loss on iteration 120 = 0.09244517646729947
Training loss on iteration 140 = 0.09273085054010152
Training loss on iteration 160 = 0.08849305901676416
Training loss on iteration 180 = 0.09664745610207319
Training loss on iteration 200 = 0.09668798409402371
Training loss on iteration 220 = 0.09160788226872682
Training loss on iteration 240 = 0.09850596077740192
Training loss on iteration 260 = 0.08967964146286249
Training loss on iteration 280 = 0.08778945654630661
Training loss on iteration 300 = 0.09164112526923418
Training loss on iteration 320 = 0.09168667439371347
Training loss on iteration 340 = 0.08421184495091438
Training loss on iteration 360 = 0.08639584295451641
Training loss on iteration 380 = 0.09525860976427794
Training loss on iteration 400 = 0.09445542693138123
Training loss on iteration 420 = 0.09484222512692213
Training loss on iteration 440 = 0.0972689401358366
Training loss on iteration 460 = 0.0998649973422289
Training loss on iteration 480 = 0.08916338831186295
Training loss on iteration 500 = 0.08981197364628316
Training loss on iteration 520 = 0.10009881779551506
Training loss on iteration 540 = 0.09391707889735698
Training loss on iteration 560 = 0.09169981647282839
Training loss on iteration 580 = 0.09212951548397541
Training loss on iteration 600 = 0.08850153498351573
Training loss on iteration 620 = 0.08269222062081098
Training loss on iteration 640 = 0.08656815178692341
Training loss on iteration 660 = 0.0892962146550417
Training loss on iteration 680 = 0.08447609283030033
Training loss on iteration 700 = 0.09258756879717112
Training loss on iteration 720 = 0.10555654969066382
Training loss on iteration 740 = 0.11935532465577126
Training loss on iteration 760 = 0.0913589458912611
Training loss on iteration 780 = 0.08855432756245137
Training loss on iteration 800 = 0.0951263602823019
Training loss on iteration 820 = 0.09068237245082855
Training loss on iteration 840 = 0.08354596123099327
Training loss on iteration 860 = 0.09266632869839668
Training loss on iteration 880 = 0.09518509935587645
Training loss on iteration 900 = 0.08684734273701906
Training loss on iteration 920 = 0.09029629863798619
Training loss on iteration 940 = 0.08847687337547541
Training loss on iteration 960 = 0.0819260623306036
Training loss on iteration 980 = 0.09932685047388076
Training loss on iteration 1000 = 0.08985446989536286
Training loss on iteration 1020 = 0.0806263903155923
Training loss on iteration 1040 = 0.1025988768786192
Training loss on iteration 1060 = 0.09278171397745609
Training loss on iteration 1080 = 0.0958476135507226
Training loss on iteration 1100 = 0.08743810821324587
Training loss on iteration 1120 = 0.10169172063469886
Training loss on iteration 1140 = 0.09315469600260258
Training loss on iteration 1160 = 0.09873343911021948
Training loss on iteration 1180 = 0.08320248741656541
Training loss on iteration 1200 = 0.09417345561087132
Training loss on iteration 1220 = 0.09468161948025226
Training loss on iteration 1240 = 0.08952908981591463
Training loss on iteration 1260 = 0.08836459200829268
Training loss on iteration 1280 = 0.0896495345979929
Training loss on iteration 1300 = 0.08989029750227928
Training loss on iteration 1320 = 0.11845620349049568
Training loss on iteration 1340 = 0.09354928452521563
Training loss on iteration 1360 = 0.08834253139793873
Training loss on iteration 1380 = 0.09102449305355549
Training loss on iteration 1400 = 0.0948940183967352
Training loss on iteration 1420 = 0.10331462621688843
Training loss on iteration 1440 = 0.09898134302347898
Training loss on iteration 1460 = 0.09010654259473086
Training loss on iteration 1480 = 0.08380963038653136
Training loss on iteration 1500 = 0.09127881471067667
Training loss on iteration 1520 = 0.09844933524727821
Training loss on iteration 1540 = 0.08693421222269535
Training loss on iteration 1560 = 0.10997816361486912
Training loss on iteration 1580 = 0.10112464167177677
Training loss on iteration 1600 = 0.09043721482157707
Training loss on iteration 1620 = 0.09753692112863063
Training loss on iteration 1640 = 0.08519491087645292
Training loss on iteration 1660 = 0.08815040178596974
Training loss on iteration 1680 = 0.09558882378041744
Training loss on iteration 1700 = 0.10193627215921879
Training loss on iteration 1720 = 0.09996767994016409
Training loss on iteration 1740 = 0.08924999721348285
Training loss on iteration 1760 = 0.0901047607883811
Training loss on iteration 1780 = 0.09088175687938929
Training loss on iteration 1800 = 0.1011483572423458
Training loss on iteration 1820 = 0.09215730484575033
Training loss on iteration 1840 = 0.08587298151105642
Training loss on iteration 1860 = 0.08401382025331258
Training loss on iteration 1880 = 0.09966960027813912
Training loss on iteration 1900 = 0.10337597504258156
Training loss on iteration 1920 = 0.0955279616639018
Training loss on iteration 1940 = 0.1038265112787485
Training loss on iteration 1960 = 0.08734723068773746
Training loss on iteration 1980 = 0.10199532471597195
Training loss on iteration 2000 = 0.09445775039494038
Training loss on iteration 2020 = 0.09668982177972793
Training loss on iteration 2040 = 0.09139195717871189
Training loss on iteration 2060 = 0.09038051627576352
Training loss on iteration 2080 = 0.09494806304574013
Training loss on iteration 2100 = 0.10132835526019335
Training loss on iteration 2120 = 0.09602116253226996
Training loss on iteration 2140 = 0.09848529621958732
Training loss on iteration 2160 = 0.09577403869479895
Training loss on iteration 2180 = 0.09414793960750104
Training loss on iteration 2200 = 0.10918840430676938
Training loss on iteration 2220 = 0.10211228094995022
Training loss on iteration 2240 = 0.0949150113388896
Training loss on iteration 2260 = 0.09382484462112188
Training loss on iteration 2280 = 0.09657827131450177
Training loss on iteration 2300 = 0.09966362621635198
Training loss on iteration 2320 = 0.09783764965832234
Training loss on iteration 2340 = 0.09087367635220289
Training loss on iteration 2360 = 0.10450253263115883
Training loss on iteration 2380 = 0.09080039560794831
Training loss on iteration 2400 = 0.08935408294200897
Training loss on iteration 2420 = 0.09328555688261986
Training loss on iteration 2440 = 0.102094666659832
Training loss on iteration 2460 = 0.0924971915781498
Training loss on iteration 2480 = 0.08905918039381504
Training loss on iteration 2500 = 0.09278631303459406
Training loss on iteration 2520 = 0.11586389616131783
Training loss on iteration 2540 = 0.08784366995096207
Training loss on iteration 2560 = 0.08747265264391899
Training loss on iteration 2580 = 0.10811401829123497
Training loss on iteration 2600 = 0.1019090550020337
Training loss on iteration 2620 = 0.09481804501265287
Training loss on iteration 2640 = 0.09420525692403317
Training loss on iteration 2660 = 0.08569624908268451
Training loss on iteration 2680 = 0.08864303976297379
Training loss on iteration 2700 = 0.08678313754498959
Training loss on iteration 2720 = 0.09329694472253322
Training loss on iteration 2740 = 0.09515086188912392
Training loss on iteration 2760 = 0.09637162853032351
Training loss on iteration 2780 = 0.08831491488963365
Training loss on iteration 2800 = 0.24557336736470461
Training loss on iteration 2820 = 0.09310224112123251
Training loss on iteration 2840 = 0.09059701655060053
Training loss on iteration 2860 = 0.09202622752636672
Training loss on iteration 2880 = 0.08892524167895317
Training loss on iteration 2900 = 0.09116141498088837
Training loss on iteration 2920 = 0.08610948510468006
Training loss on iteration 2940 = 0.08581059090793133
Training loss on iteration 2960 = 0.09678942020982503
Training loss on iteration 2980 = 0.09946949519217015
Training loss on iteration 3000 = 0.10583721324801446
Training loss on iteration 3020 = 0.09754546396434308
Training loss on iteration 3040 = 0.09276248831301928
Training loss on iteration 3060 = 0.09296806044876575
Training loss on iteration 3080 = 0.09341725651174784
Training loss on iteration 3100 = 0.09783583581447601
Training loss on iteration 3120 = 0.09495733436197043
Training loss on iteration 3140 = 0.10026952121406793
Training loss on iteration 3160 = 0.08664751816540957
Training loss on iteration 3180 = 0.10931595861911773
Training loss on iteration 3200 = 0.10857452750205994
Training loss on iteration 3220 = 0.09869198240339756
Training loss on iteration 3240 = 0.09282598625868559
Training loss on iteration 3260 = 0.08985655941069126
Training loss on iteration 3280 = 0.08495823722332715
Training loss on iteration 3300 = 0.0906833192333579
Training loss on iteration 3320 = 0.0839092567563057
Training loss on iteration 3340 = 0.09528202563524246
Training loss on iteration 3360 = 0.09625909347087144
Training loss on iteration 3380 = 0.09935116190463304
Training loss on iteration 3400 = 0.08802311420440674
Training loss on iteration 3420 = 0.10775842629373074
Training loss on iteration 3440 = 0.09755968935787677
Training loss on iteration 3460 = 0.09667831808328628
Training loss on iteration 3480 = 0.10022312086075544
Training loss on iteration 3500 = 0.09904112033545971
Training loss on iteration 3520 = 0.09000631161034107
Training loss on iteration 3540 = 0.08652784675359726
Training loss on iteration 3560 = 0.08791084531694651
Training loss on iteration 3580 = 0.08976137936115265
Training loss on iteration 3600 = 0.10311520211398602
Training loss on iteration 3620 = 0.09007760006934404
Training loss on iteration 3640 = 0.08090909291058779
Training loss on iteration 3660 = 0.09351763688027859
Training loss on iteration 3680 = 0.0903168436139822
Training loss on iteration 3700 = 0.08405120503157378
Training loss on iteration 3720 = 0.09850557595491409
Training loss on iteration 3740 = 0.0904729824513197
Training loss on iteration 3760 = 0.09798166751861573
Training loss on iteration 3780 = 0.10883718393743039
Training loss on iteration 3800 = 0.0882528156042099
Training loss on iteration 3820 = 0.09384375251829624
Training loss on iteration 3840 = 0.10117002204060555
Training loss on iteration 3860 = 0.08933972399681807
Training loss on iteration 3880 = 0.08368779737502337
Training loss on iteration 3900 = 0.09739492200314999
Training loss on iteration 3920 = 0.0896682845428586
Training loss on iteration 3940 = 0.09954003058373928
Training loss on iteration 3960 = 0.09378106743097306
Training loss on iteration 3980 = 0.09054558277130127
Training loss on iteration 4000 = 0.08691701181232929
Training loss on iteration 4020 = 0.08358287215232849
Training loss on iteration 4040 = 0.08387753572314978
Training loss on iteration 4060 = 0.09930606633424759
Training loss on iteration 4080 = 0.09606094937771559
Training loss on iteration 4100 = 0.092088483273983
Training loss on iteration 4120 = 0.09610312562435866
Training loss on iteration 4140 = 0.08859797194600105
Training loss on iteration 4160 = 0.09709318988025188
Training loss on iteration 4180 = 0.08545430917292833
Training loss on iteration 4200 = 0.09819013141095638
Training loss on iteration 4220 = 0.08854254819452763
Training loss on iteration 4240 = 0.09034695774316788
Training loss on iteration 4260 = 0.08867745287716389
Training loss on iteration 4280 = 0.1073704918846488
Training loss on iteration 4300 = 0.10044186916202306
Training loss on iteration 4320 = 0.08775145318359137
Training loss on iteration 4340 = 0.09324575699865818
Training loss on iteration 4360 = 0.09080401752144099
Training loss on iteration 4380 = 0.09467600174248218
Training loss on iteration 4400 = 0.2534488994628191
Training loss on iteration 4420 = 0.09325230326503516
Training loss on iteration 4440 = 0.08881823401898145
Training loss on iteration 4460 = 0.09287136755883693
Training loss on iteration 4480 = 0.09284516517072916
Training loss on iteration 4500 = 0.16455886401236058
Training loss on iteration 4520 = 0.09308150000870227
Training loss on iteration 4540 = 0.09706466756761074
Training loss on iteration 4560 = 0.08520664256066084
Training loss on iteration 4580 = 0.09904052410274744
Training loss on iteration 4600 = 0.10367452204227448
Training loss on iteration 4620 = 0.08789151348173618
Training loss on iteration 4640 = 0.09111442603170872
Training loss on iteration 4660 = 0.08689152263104916
Training loss on iteration 4680 = 0.1135230703279376
Training loss on iteration 4700 = 0.09807346947491169
Training loss on iteration 4720 = 0.09746674373745919
Training loss on iteration 4740 = 0.09886087253689765
Training loss on iteration 4760 = 0.09727951996028424
Training loss on iteration 4780 = 0.09365004878491164
Training loss on iteration 4800 = 0.08667702805250883
Training loss on iteration 4820 = 0.08416996188461781
Training loss on iteration 4840 = 0.10237213075160981
Training loss on iteration 4860 = 0.09350831266492605
Training loss on iteration 4880 = 0.09477584324777126
Training loss on iteration 4900 = 0.09394120872020721
Training loss on iteration 4920 = 0.09548971485346555
Training loss on iteration 4940 = 0.09005563855171203
Training loss on iteration 4960 = 0.08219539728015661
Training loss on iteration 4980 = 0.09904785696417093
Training loss on iteration 5000 = 0.09239620026201009
Training loss on iteration 5020 = 0.10085884239524603
Training loss on iteration 5040 = 0.09124312214553357
Training loss on iteration 5060 = 0.10570925157517194
Training loss on iteration 5080 = 0.09023751113563776
Training loss on iteration 5100 = 0.09303564038127661
Training loss on iteration 5120 = 0.1012962032109499
Training loss on iteration 5140 = 0.08436482772231102
Training loss on iteration 5160 = 0.10804584547877312
Training loss on iteration 5180 = 0.1006492555141449
Training loss on iteration 5200 = 0.10120871029794216
Training loss on iteration 5220 = 0.08902670051902532
Training loss on iteration 5240 = 0.0932190515100956
Training loss on iteration 5260 = 0.09370315913110971
Training loss on iteration 5280 = 0.08950301893055439
Training loss on iteration 5300 = 0.09122960418462753
Training loss on iteration 5320 = 0.08881649393588305
Training loss on iteration 5340 = 0.09570917896926404
Training loss on iteration 5360 = 0.09765932038426399
Training loss on iteration 5380 = 0.0956909243017435
Training loss on iteration 5400 = 0.09431078173220157
Training loss on iteration 5420 = 0.09638914372771978
Training loss on iteration 5440 = 0.08406389653682708
Training loss on iteration 5460 = 0.10740706957876682
Training loss on iteration 5480 = 0.08485090993344784
Training loss on iteration 5500 = 0.09115027617663145
Training loss on iteration 5520 = 0.09064668267965317
Training loss on iteration 5540 = 0.08457603361457586
Training loss on iteration 5560 = 0.08716962952166796
Training loss on iteration 5580 = 0.10060022622346879
Training loss on iteration 5600 = 0.08886664360761642
Training loss on iteration 5620 = 0.0894614391028881
Training loss on iteration 5640 = 0.08376385439187288
Training loss on iteration 5660 = 0.08909327704459429
Training loss on iteration 5680 = 0.09650137647986412
Training loss on iteration 5700 = 0.0940860765054822
Training loss on iteration 5720 = 0.09629240967333316
Training loss on iteration 5740 = 0.09687213841825723
Training loss on iteration 5760 = 0.09504336304962635
Training loss on iteration 5780 = 0.08728472739458085
Training loss on iteration 5800 = 0.09890286456793547
Training loss on iteration 5820 = 0.08454286828637123
Training loss on iteration 5840 = 0.08662799578160048
Training loss on iteration 5860 = 0.11051186639815569
Training loss on iteration 5880 = 0.09314350560307502
Training loss on iteration 5900 = 0.1136500459164381
Training loss on iteration 5920 = 0.09861323684453964
Training loss on iteration 5940 = 0.09566673040390014
Training loss on iteration 5960 = 0.08985533826053142
Training loss on iteration 5980 = 0.08037225343286991
Training loss on iteration 6000 = 0.09044716656208038
Training loss on iteration 6020 = 0.08969389200210572
Training loss on iteration 6040 = 0.09555532280355691
Training loss on iteration 6060 = 0.10829225555062294
Training loss on iteration 6080 = 0.09060993585735559
Training loss on iteration 6100 = 0.0963540991768241
Training loss on iteration 6120 = 0.09343974236398936
Training loss on iteration 6140 = 0.09490948803722858
Training loss on iteration 6160 = 0.08885308783501386
Training loss on iteration 6180 = 0.09014877993613482
Training loss on iteration 6200 = 0.0906243596225977
Training loss on iteration 6220 = 0.0858050886541605
Training loss on iteration 6240 = 0.09971710499376059
Training loss on iteration 6260 = 0.09414056856185198
Training loss on iteration 6280 = 0.09276338666677475
Training loss on iteration 6300 = 0.1324614629149437
Training loss on iteration 6320 = 0.0944078229367733
Training loss on iteration 6340 = 0.09699795506894589
Training loss on iteration 6360 = 0.09151124637573957
Training loss on iteration 6380 = 0.10101560410112143
Training loss on iteration 0 = 0.1197136715054512
Training loss on iteration 20 = 0.08847056068480015
Training loss on iteration 40 = 0.08872812669724225
Training loss on iteration 60 = 0.08859492056071758
Training loss on iteration 80 = 0.08960239123553038
Training loss on iteration 100 = 0.08922177590429783
Training loss on iteration 120 = 0.08179593551903963
Training loss on iteration 140 = 0.08940444402396679
Training loss on iteration 160 = 0.08449187129735947
Training loss on iteration 180 = 0.1084438083693385
Training loss on iteration 200 = 0.08470693286508321
Training loss on iteration 220 = 0.08416025023907422
Training loss on iteration 240 = 0.08335427884012461
Training loss on iteration 260 = 0.09777866937220096
Training loss on iteration 280 = 0.08617751915007829
Training loss on iteration 300 = 0.0838907128199935
Training loss on iteration 320 = 0.08965370375663043
Training loss on iteration 340 = 0.08125343602150678
Training loss on iteration 360 = 0.0938112623989582
Training loss on iteration 380 = 0.08149403259158135
Training loss on iteration 400 = 0.08516097888350486
Training loss on iteration 420 = 0.08982226196676493
Training loss on iteration 440 = 0.0842474315315485
Training loss on iteration 460 = 0.0885745819658041
Training loss on iteration 480 = 0.08466987889260054
Training loss on iteration 500 = 0.08648757077753544
Training loss on iteration 520 = 0.08941115848720074
Training loss on iteration 540 = 0.0856011776253581
Training loss on iteration 560 = 0.08557642810046673
Training loss on iteration 580 = 0.08763549719005823
Training loss on iteration 600 = 0.08723647072911263
Training loss on iteration 620 = 0.08464198932051659
Training loss on iteration 640 = 0.08852652460336685
Training loss on iteration 660 = 0.0898171653971076
Training loss on iteration 680 = 0.08750548027455807
Training loss on iteration 700 = 0.08904252275824547
Training loss on iteration 720 = 0.0926699848845601
Training loss on iteration 740 = 0.09664419293403625
Training loss on iteration 760 = 0.08135210406035184
Training loss on iteration 780 = 0.08951082192361355
Training loss on iteration 800 = 0.09744855053722859
Training loss on iteration 820 = 0.09442811757326126
Training loss on iteration 840 = 0.09282956328243017
Training loss on iteration 860 = 0.08997535798698664
Training loss on iteration 880 = 0.09903758130967617
Training loss on iteration 900 = 0.09318147525191307
Training loss on iteration 920 = 0.08961822204291821
Training loss on iteration 940 = 0.0947537038475275
Training loss on iteration 960 = 0.09468677304685116
Training loss on iteration 980 = 0.08300589416176081
Training loss on iteration 1000 = 0.08405779004096985
Training loss on iteration 1020 = 0.08730541877448558
Training loss on iteration 1040 = 0.0885783851146698
Training loss on iteration 1060 = 0.08965812791138887
Training loss on iteration 1080 = 0.09091860745102168
Training loss on iteration 1100 = 0.0832558697089553
Training loss on iteration 1120 = 0.08482821881771088
Training loss on iteration 1140 = 0.08005658108741046
Training loss on iteration 1160 = 0.08891450371593237
Training loss on iteration 1180 = 0.07781911622732877
Training loss on iteration 1200 = 0.09158062860369683
Training loss on iteration 1220 = 0.08668673038482666
Training loss on iteration 1240 = 0.07890240754932165
Training loss on iteration 1260 = 0.09085271544754506
Training loss on iteration 1280 = 0.0858306061476469
Training loss on iteration 1300 = 0.08301348090171815
Training loss on iteration 1320 = 0.0903453290462494
Training loss on iteration 1340 = 0.08740091547369958
Training loss on iteration 1360 = 0.09289611391723156
Training loss on iteration 1380 = 0.0893113749101758
Training loss on iteration 1400 = 0.0834599619731307
Training loss on iteration 1420 = 0.0926686579361558
Training loss on iteration 1440 = 0.09421098064631224
Training loss on iteration 1460 = 0.08201093319803476
Training loss on iteration 1480 = 0.09999023750424385
Training loss on iteration 1500 = 0.08450507409870625
Training loss on iteration 1520 = 0.1455135392025113
Training loss on iteration 1540 = 0.0922439482063055
Training loss on iteration 1560 = 0.08380396403372288
Training loss on iteration 1580 = 0.08487225007265806
Training loss on iteration 1600 = 0.09400176294147969
Training loss on iteration 1620 = 0.09317337386310101
Training loss on iteration 1640 = 0.0919131513684988
Training loss on iteration 1660 = 0.08209187351167202
Training loss on iteration 1680 = 0.09641130622476339
Training loss on iteration 1700 = 0.08836615197360516
Training loss on iteration 1720 = 0.09806058015674353
Training loss on iteration 1740 = 0.08727629370987415
Training loss on iteration 1760 = 0.0946610614657402
Training loss on iteration 1780 = 0.08810720797628165
Training loss on iteration 1800 = 0.08903683181852103
Training loss on iteration 1820 = 0.08733446560800076
Training loss on iteration 1840 = 0.09046146925538778
Training loss on iteration 1860 = 0.09141970761120319
Training loss on iteration 1880 = 0.09176158122718334
Training loss on iteration 1900 = 0.118543965741992
Training loss on iteration 1920 = 0.08672322407364845
Training loss on iteration 1940 = 0.09493807367980481
Training loss on iteration 1960 = 0.09144133981317282
Training loss on iteration 1980 = 0.09892089068889617
Training loss on iteration 2000 = 0.08200478572398424
Training loss on iteration 2020 = 0.09422353878617287
Training loss on iteration 2040 = 0.08753607776015997
Training loss on iteration 2060 = 0.08527428042143584
Training loss on iteration 2080 = 0.09373296909034252
Training loss on iteration 2100 = 0.09186304248869419
Training loss on iteration 2120 = 0.11246738471090793
Training loss on iteration 2140 = 0.08736730925738811
Training loss on iteration 2160 = 0.0819174537435174
Training loss on iteration 2180 = 0.08581258710473776
Training loss on iteration 2200 = 0.09431982897222042
Training loss on iteration 2220 = 0.08495041597634553
Training loss on iteration 2240 = 0.0954689409583807
Training loss on iteration 2260 = 0.08537871818989515
Training loss on iteration 2280 = 0.08722607363015414
Training loss on iteration 2300 = 0.08875677604228258
Training loss on iteration 2320 = 0.08221885357052088
Training loss on iteration 2340 = 0.08465307857841253
Training loss on iteration 2360 = 0.0793897470459342
Training loss on iteration 2380 = 0.10496146660298108
Training loss on iteration 2400 = 0.09034186974167824
Training loss on iteration 2420 = 0.09177025426179171
Training loss on iteration 2440 = 0.07611963804811239
Training loss on iteration 2460 = 0.09928841423243284
Training loss on iteration 2480 = 0.08699827119708062
Training loss on iteration 2500 = 0.08523245360702277
Training loss on iteration 2520 = 0.0976006805896759
Training loss on iteration 2540 = 0.24975653402507306
Training loss on iteration 2560 = 0.08805088847875595
Training loss on iteration 2580 = 0.09755972288548946
Training loss on iteration 2600 = 0.09103059507906437
Training loss on iteration 2620 = 0.09293751027435064
Training loss on iteration 2640 = 0.08607623800635338
Training loss on iteration 2660 = 0.07900255508720874
Training loss on iteration 2680 = 0.08221822530031205
Training loss on iteration 2700 = 0.08487900104373694
Training loss on iteration 2720 = 0.08291234895586967
Training loss on iteration 2740 = 0.09130985327064992
Training loss on iteration 2760 = 0.11156528703868389
Training loss on iteration 2780 = 0.09389183335006238
Training loss on iteration 2800 = 0.08717465903609992
Training loss on iteration 2820 = 0.084992097876966
Training loss on iteration 2840 = 0.09406145066022872
Training loss on iteration 2860 = 0.09978243261575699
Training loss on iteration 2880 = 0.09479681756347418
Training loss on iteration 2900 = 0.09184406083077193
Training loss on iteration 2920 = 0.08948078546673059
Training loss on iteration 2940 = 0.09837084282189608
Training loss on iteration 2960 = 0.08375584296882152
Training loss on iteration 2980 = 0.08553446866571904
Training loss on iteration 3000 = 0.11534364353865385
Training loss on iteration 3020 = 0.0901365477591753
Training loss on iteration 3040 = 0.09188329763710498
Training loss on iteration 3060 = 0.09930020086467266
Training loss on iteration 3080 = 0.09098382033407688
Training loss on iteration 3100 = 0.08983828704804182
Training loss on iteration 3120 = 0.08351642787456512
Training loss on iteration 3140 = 0.09192728102207184
Training loss on iteration 3160 = 0.09145882427692413
Training loss on iteration 3180 = 0.09392441250383854
Training loss on iteration 3200 = 0.08645058237016201
Training loss on iteration 3220 = 0.08930261861532926
Training loss on iteration 3240 = 0.08523498810827732
Training loss on iteration 3260 = 0.08456739448010922
Training loss on iteration 3280 = 0.09349981788545847
Training loss on iteration 3300 = 0.08920051604509353
Training loss on iteration 3320 = 0.09973015245050192
Training loss on iteration 3340 = 0.08836480900645256
Training loss on iteration 3360 = 0.11093101687729359
Training loss on iteration 3380 = 0.08612328302115202
Training loss on iteration 3400 = 0.08660795092582703
Training loss on iteration 3420 = 0.08731343634426594
Training loss on iteration 3440 = 0.09084005858749152
Training loss on iteration 3460 = 0.09393176306039094
Training loss on iteration 3480 = 0.08905612714588643
Training loss on iteration 3500 = 0.08657230623066425
Training loss on iteration 3520 = 0.08905540592968464
Training loss on iteration 3540 = 0.10124179273843766
Training loss on iteration 3560 = 0.08257478047162295
Training loss on iteration 3580 = 0.08912297170609236
Training loss on iteration 3600 = 0.08882741276174784
Training loss on iteration 3620 = 0.10417269729077816
Training loss on iteration 3640 = 0.23642264660447837
Training loss on iteration 3660 = 0.09119785707443953
Training loss on iteration 3680 = 0.0877293910831213
Training loss on iteration 3700 = 0.09919272363185883
Training loss on iteration 3720 = 0.08590145334601403
Training loss on iteration 3740 = 0.09422371871769428
Training loss on iteration 3760 = 0.09410659857094288
Training loss on iteration 3780 = 0.07956970911473035
Training loss on iteration 3800 = 0.11164150293916464
Training loss on iteration 3820 = 0.0965852227061987
Training loss on iteration 3840 = 0.09840772375464439
Training loss on iteration 3860 = 0.08310444951057434
Training loss on iteration 3880 = 0.1025817234069109
Training loss on iteration 3900 = 0.10054103750735521
Training loss on iteration 3920 = 0.10011893827468157
Training loss on iteration 3940 = 0.09425469078123569
Training loss on iteration 3960 = 0.09155826922506094
Training loss on iteration 3980 = 0.08574588168412448
Training loss on iteration 4000 = 0.08934505507349969
Training loss on iteration 4020 = 0.08908608797937631
Training loss on iteration 4040 = 0.09670434631407261
Training loss on iteration 4060 = 0.09606520067900419
Training loss on iteration 4080 = 0.089372543618083
Training loss on iteration 4100 = 0.10247569270431996
Training loss on iteration 4120 = 0.10222876481711865
Training loss on iteration 4140 = 0.09003022816032172
Training loss on iteration 4160 = 0.0960548497736454
Training loss on iteration 4180 = 0.09409124795347452
Training loss on iteration 4200 = 0.10160458348691463
Training loss on iteration 4220 = 0.09058523625135421
Training loss on iteration 4240 = 0.09943134151399136
Training loss on iteration 4260 = 0.09247665964066983
Training loss on iteration 4280 = 0.09138013292104005
Training loss on iteration 4300 = 0.08973958529531956
Training loss on iteration 4320 = 0.08669110294431448
Training loss on iteration 4340 = 0.0951229304075241
Training loss on iteration 4360 = 0.0866161786019802
Training loss on iteration 4380 = 0.09688500426709652
Training loss on iteration 4400 = 0.08776552900671959
Training loss on iteration 4420 = 0.09574977681040764
Training loss on iteration 4440 = 0.10978509522974492
Training loss on iteration 4460 = 0.10794438347220421
Training loss on iteration 4480 = 0.09562450870871544
Training loss on iteration 4500 = 0.09086913429200649
Training loss on iteration 4520 = 0.08964609280228615
Training loss on iteration 4540 = 0.09768447056412696
Training loss on iteration 4560 = 0.09394780285656452
Training loss on iteration 4580 = 0.0838839776813984
Training loss on iteration 4600 = 0.0901388356462121
Training loss on iteration 4620 = 0.08138298671692609
Training loss on iteration 4640 = 0.08304814714938402
Training loss on iteration 4660 = 0.0931532496586442
Training loss on iteration 4680 = 0.09445480592548847
Training loss on iteration 4700 = 0.08896197453141212
Training loss on iteration 4720 = 0.095915262773633
Training loss on iteration 4740 = 0.08815952613949776
Training loss on iteration 4760 = 0.0936868816614151
Training loss on iteration 4780 = 0.08921492174267769
Training loss on iteration 4800 = 0.09559926073998212
Training loss on iteration 4820 = 0.09296459313482046
Training loss on iteration 4840 = 0.09543372876942158
Training loss on iteration 4860 = 0.08830164186656475
Training loss on iteration 4880 = 0.09273135699331761
Training loss on iteration 4900 = 0.09141234699636698
Training loss on iteration 4920 = 0.10124797523021697
Training loss on iteration 4940 = 0.09103911183774471
Training loss on iteration 4960 = 0.10488859266042709
Training loss on iteration 4980 = 0.08646077383309603
Training loss on iteration 5000 = 0.09984480366110801
Training loss on iteration 5020 = 0.11175404228270054
Training loss on iteration 5040 = 0.08551635295152664
Training loss on iteration 5060 = 0.09999030996114015
Training loss on iteration 5080 = 0.09235154166817665
Training loss on iteration 5100 = 0.08435012362897396
Training loss on iteration 5120 = 0.09565397519618273
Training loss on iteration 5140 = 0.0969780620187521
Training loss on iteration 5160 = 0.09161265101283789
Training loss on iteration 5180 = 0.0951048107817769
Training loss on iteration 5200 = 0.09471850618720054
Training loss on iteration 5220 = 0.09540842827409506
Training loss on iteration 5240 = 0.09385824091732502
Training loss on iteration 5260 = 0.11600682623684407
Training loss on iteration 5280 = 0.09634155798703432
Training loss on iteration 5300 = 0.08355286233127117
Training loss on iteration 5320 = 0.0964487336575985
Training loss on iteration 5340 = 0.08638273179531097
Training loss on iteration 5360 = 0.08978628646582365
Training loss on iteration 5380 = 0.09258466362953185
Training loss on iteration 5400 = 0.09205127637833357
Training loss on iteration 5420 = 0.09959278795868158
Training loss on iteration 5440 = 0.09154332317411899
Training loss on iteration 5460 = 0.09331526011228561
Training loss on iteration 5480 = 0.09515517614781857
Training loss on iteration 5500 = 0.08556320685893297
Training loss on iteration 5520 = 0.08079140223562717
Training loss on iteration 5540 = 0.1028745960444212
Training loss on iteration 5560 = 0.0905150182545185
Training loss on iteration 5580 = 0.09072374999523163
Training loss on iteration 5600 = 0.09475174266844988
Training loss on iteration 5620 = 0.10059195198118687
Training loss on iteration 5640 = 0.08453566450625657
Training loss on iteration 5660 = 0.09163464549928904
Training loss on iteration 5680 = 0.13165861554443836
Training loss on iteration 5700 = 0.09609046615660191
Training loss on iteration 5720 = 0.08713677022606134
Training loss on iteration 5740 = 0.09555097930133342
Training loss on iteration 5760 = 0.10255854651331901
Training loss on iteration 5780 = 0.08944901712238788
Training loss on iteration 5800 = 0.08606040012091398
Training loss on iteration 5820 = 0.09998333491384984
Training loss on iteration 5840 = 0.09460171833634376
Training loss on iteration 5860 = 0.09216459132730961
Training loss on iteration 5880 = 0.09126446954905987
Training loss on iteration 5900 = 0.08566611204296351
Training loss on iteration 5920 = 0.1027679719030857
Training loss on iteration 5940 = 0.09579592794179917
Training loss on iteration 5960 = 0.10986246727406979
Training loss on iteration 5980 = 0.0885996351018548
Training loss on iteration 6000 = 0.08894450813531876
Training loss on iteration 6020 = 0.10160425193607807
Training loss on iteration 6040 = 0.08725821170955897
Training loss on iteration 6060 = 0.09910461753606796
Training loss on iteration 6080 = 0.08991924691945315
Training loss on iteration 6100 = 0.08224534690380096
Training loss on iteration 6120 = 0.09988458231091499
Training loss on iteration 6140 = 0.09507805556058883
Training loss on iteration 6160 = 0.09974216297268867
Training loss on iteration 6180 = 0.09167742021381856
Training loss on iteration 6200 = 0.09701990708708763
Training loss on iteration 6220 = 0.08672717101871967
Training loss on iteration 6240 = 0.08903050236403942
Training loss on iteration 6260 = 0.1094158235937357
Training loss on iteration 6280 = 0.09530156943947077
Training loss on iteration 6300 = 0.09211871717125178
Training loss on iteration 6320 = 0.08675876930356026
Training loss on iteration 6340 = 0.08977573867887259
Training loss on iteration 6360 = 0.09818244744092226
Training loss on iteration 6380 = 0.0852139288559556
Training loss on iteration 0 = 0.11859854310750961
Training loss on iteration 20 = 0.08098219875246286
Training loss on iteration 40 = 0.09200759306550026
Training loss on iteration 60 = 0.09133436512202024
Training loss on iteration 80 = 0.08797520101070404
Training loss on iteration 100 = 0.085005122423172
Training loss on iteration 120 = 0.08648784346878528
Training loss on iteration 140 = 0.08146976698189974
Training loss on iteration 160 = 0.08259629886597394
Training loss on iteration 180 = 0.08526082430034876
Training loss on iteration 200 = 0.0879860932007432
Training loss on iteration 220 = 0.08755972255021334
Training loss on iteration 240 = 0.08331796955317258
Training loss on iteration 260 = 0.08573210388422012
Training loss on iteration 280 = 0.09114502985030412
Training loss on iteration 300 = 0.09001572094857693
Training loss on iteration 320 = 0.08079041317105293
Training loss on iteration 340 = 0.08485779333859682
Training loss on iteration 360 = 0.08816592320799828
Training loss on iteration 380 = 0.09756110515445471
Training loss on iteration 400 = 0.08304241187870502
Training loss on iteration 420 = 0.08436884693801402
Training loss on iteration 440 = 0.10322070978581906
Training loss on iteration 460 = 0.08741691559553147
Training loss on iteration 480 = 0.08648736663162708
Training loss on iteration 500 = 0.08632956594228744
Training loss on iteration 520 = 0.09204989112913609
Training loss on iteration 540 = 0.09375361688435077
Training loss on iteration 560 = 0.09368782211095095
Training loss on iteration 580 = 0.08818243183195591
Training loss on iteration 600 = 0.09062100984156132
Training loss on iteration 620 = 0.08740057684481144
Training loss on iteration 640 = 0.0914932657033205
Training loss on iteration 660 = 0.08488728981465102
Training loss on iteration 680 = 0.09352338630706072
Training loss on iteration 700 = 0.08593302685767412
Training loss on iteration 720 = 0.0902811985462904
Training loss on iteration 740 = 0.09229932613670826
Training loss on iteration 760 = 0.08877023905515671
Training loss on iteration 780 = 0.08548778872936964
Training loss on iteration 800 = 0.09122432805597783
Training loss on iteration 820 = 0.08566587064415217
Training loss on iteration 840 = 0.08458526786416769
Training loss on iteration 860 = 0.09544063080102205
Training loss on iteration 880 = 0.08105510622262954
Training loss on iteration 900 = 0.09244456198066472
Training loss on iteration 920 = 0.08248351886868477
Training loss on iteration 940 = 0.08374338094145059
Training loss on iteration 960 = 0.08272035848349332
Training loss on iteration 980 = 0.08349720593541861
Training loss on iteration 1000 = 0.08556963000446557
Training loss on iteration 1020 = 0.08974556662142277
Training loss on iteration 1040 = 0.08950134553015232
Training loss on iteration 1060 = 0.0830839505419135
Training loss on iteration 1080 = 0.10620854180306197
Training loss on iteration 1100 = 0.0896210303530097
Training loss on iteration 1120 = 0.09179542139172554
Training loss on iteration 1140 = 0.08667350355535745
Training loss on iteration 1160 = 0.09324016813188792
Training loss on iteration 1180 = 0.08312728926539421
Training loss on iteration 1200 = 0.07871750220656396
Training loss on iteration 1220 = 0.11897300332784652
Training loss on iteration 1240 = 0.08630187232047319
Training loss on iteration 1260 = 0.08525260314345359
Training loss on iteration 1280 = 0.08384526949375867
Training loss on iteration 1300 = 0.08126738872379065
Training loss on iteration 1320 = 0.08557251803576946
Training loss on iteration 1340 = 0.0831648763269186
Training loss on iteration 1360 = 0.09103990197181702
Training loss on iteration 1380 = 0.08704467993229628
Training loss on iteration 1400 = 0.09595349188894034
Training loss on iteration 1420 = 0.09511581361293793
Training loss on iteration 1440 = 0.08608678318560123
Training loss on iteration 1460 = 0.0835636455565691
Training loss on iteration 1480 = 0.08579621985554695
Training loss on iteration 1500 = 0.09046812690794467
Training loss on iteration 1520 = 0.09317521657794714
Training loss on iteration 1540 = 0.08284499105066061
Training loss on iteration 1560 = 0.08395070452243089
Training loss on iteration 1580 = 0.09053830076009035
Training loss on iteration 1600 = 0.08886023666709661
Training loss on iteration 1620 = 0.10238817017525434
Training loss on iteration 1640 = 0.08612634222954511
Training loss on iteration 1660 = 0.0879919471219182
Training loss on iteration 1680 = 0.08614828661084176
Training loss on iteration 1700 = 0.09592436384409667
Training loss on iteration 1720 = 0.08339704684913159
Training loss on iteration 1740 = 0.08737639226019382
Training loss on iteration 1760 = 0.0870481375604868
Training loss on iteration 1780 = 0.09642847497016191
Training loss on iteration 1800 = 0.09028886277228594
Training loss on iteration 1820 = 0.08536869063973426
Training loss on iteration 1840 = 0.08506926856935024
Training loss on iteration 1860 = 0.08721461184322835
Training loss on iteration 1880 = 0.09244330022484064
Training loss on iteration 1900 = 0.0845288135111332
Training loss on iteration 1920 = 0.08165891617536544
Training loss on iteration 1940 = 0.08698034174740314
Training loss on iteration 1960 = 0.09570823945105075
Training loss on iteration 1980 = 0.08515870627015829
Training loss on iteration 2000 = 0.08190526720136404
Training loss on iteration 2020 = 0.08497763555496932
Training loss on iteration 2040 = 0.09077098220586777
Training loss on iteration 2060 = 0.08971115946769714
Training loss on iteration 2080 = 0.08072102926671505
Training loss on iteration 2100 = 0.08900574259459973
Training loss on iteration 2120 = 0.08592803627252579
Training loss on iteration 2140 = 0.10551869105547666
Training loss on iteration 2160 = 0.09070841446518899
Training loss on iteration 2180 = 0.09392164796590804
Training loss on iteration 2200 = 0.08151269666850566
Training loss on iteration 2220 = 0.08338660392910242
Training loss on iteration 2240 = 0.0925224045291543
Training loss on iteration 2260 = 0.0805112786591053
Training loss on iteration 2280 = 0.09604318104684353
Training loss on iteration 2300 = 0.09483946785330773
Training loss on iteration 2320 = 0.08943692613393069
Training loss on iteration 2340 = 0.09655637033283711
Training loss on iteration 2360 = 0.15209976099431516
Training loss on iteration 2380 = 0.09762527737766505
Training loss on iteration 2400 = 0.08777529634535312
Training loss on iteration 2420 = 0.09774669855833054
Training loss on iteration 2440 = 0.0804287964478135
Training loss on iteration 2460 = 0.08987987414002419
Training loss on iteration 2480 = 0.08773243520408869
Training loss on iteration 2500 = 0.09810114316642285
Training loss on iteration 2520 = 0.1017029870301485
Training loss on iteration 2540 = 0.246611363068223
Training loss on iteration 2560 = 0.08173514120280742
Training loss on iteration 2580 = 0.08910336084663868
Training loss on iteration 2600 = 0.08897882997989655
Training loss on iteration 2620 = 0.08126415405422449
Training loss on iteration 2640 = 0.09320822339504957
Training loss on iteration 2660 = 0.08716982360929251
Training loss on iteration 2680 = 0.09203274212777615
Training loss on iteration 2700 = 0.09019330888986588
Training loss on iteration 2720 = 0.08510783705860377
Training loss on iteration 2740 = 0.08312357664108276
Training loss on iteration 2760 = 0.09242834839969874
Training loss on iteration 2780 = 0.09785432126373053
Training loss on iteration 2800 = 0.08998474087566137
Training loss on iteration 2820 = 0.0815285224467516
Training loss on iteration 2840 = 0.09523288570344449
Training loss on iteration 2860 = 0.09214727357029914
Training loss on iteration 2880 = 0.09043193720281124
Training loss on iteration 2900 = 0.08240038752555848
Training loss on iteration 2920 = 0.08537084702402353
Training loss on iteration 2940 = 0.08407373242080211
Training loss on iteration 2960 = 0.08451055996119976
Training loss on iteration 2980 = 0.0857257692143321
Training loss on iteration 3000 = 0.09039037618786097
Training loss on iteration 3020 = 0.0876130050048232
Training loss on iteration 3040 = 0.10420045331120491
Training loss on iteration 3060 = 0.09114971943199635
Training loss on iteration 3080 = 0.09110305160284042
Training loss on iteration 3100 = 0.10142206288874149
Training loss on iteration 3120 = 0.09306276999413968
Training loss on iteration 3140 = 0.09297810662537813
Training loss on iteration 3160 = 0.08174246568232775
Training loss on iteration 3180 = 0.08921278789639472
Training loss on iteration 3200 = 0.09378372207283973
Training loss on iteration 3220 = 0.09099386669695378
Training loss on iteration 3240 = 0.09374419916421176
Training loss on iteration 3260 = 0.11746307872235776
Training loss on iteration 3280 = 0.09794188942760229
Training loss on iteration 3300 = 0.09258712697774171
Training loss on iteration 3320 = 0.08850306048989295
Training loss on iteration 3340 = 0.08554105777293444
Training loss on iteration 3360 = 0.08550166934728623
Training loss on iteration 3380 = 0.08977933526039124
Training loss on iteration 3400 = 0.0836252961307764
Training loss on iteration 3420 = 0.11846776474267244
Training loss on iteration 3440 = 0.1200542189180851
Training loss on iteration 3460 = 0.09102699887007475
Training loss on iteration 3480 = 0.08204149510711431
Training loss on iteration 3500 = 0.0849316144362092
Training loss on iteration 3520 = 0.12552151679992676
Training loss on iteration 3540 = 0.0868681037798524
Training loss on iteration 3560 = 0.09337496962398291
Training loss on iteration 3580 = 0.08954304493963719
Training loss on iteration 3600 = 0.08872130904346705
Training loss on iteration 3620 = 0.09865435883402825
Training loss on iteration 3640 = 0.08889507483690977
Training loss on iteration 3660 = 0.08451957702636718
Training loss on iteration 3680 = 0.08107891101390123
Training loss on iteration 3700 = 0.08980060555040836
Training loss on iteration 3720 = 0.08483613077551126
Training loss on iteration 3740 = 0.1124480701982975
Training loss on iteration 3760 = 0.0882692713290453
Training loss on iteration 3780 = 0.08630319833755493
Training loss on iteration 3800 = 0.08584670387208462
Training loss on iteration 3820 = 0.09185365159064532
Training loss on iteration 3840 = 0.0902580838650465
Training loss on iteration 3860 = 0.09219042100012302
Training loss on iteration 3880 = 0.09456650875508785
Training loss on iteration 3900 = 0.08889773339033127
Training loss on iteration 3920 = 0.09474019333720207
Training loss on iteration 3940 = 0.09141894206404685
Training loss on iteration 3960 = 0.10324627198278905
Training loss on iteration 3980 = 0.08724678978323937
Training loss on iteration 4000 = 0.09089590106159448
Training loss on iteration 4020 = 0.0888226192444563
Training loss on iteration 4040 = 0.08792265653610229
Training loss on iteration 4060 = 0.08499481230974197
Training loss on iteration 4080 = 0.09316069930791855
Training loss on iteration 4100 = 0.09704761318862438
Training loss on iteration 4120 = 0.08259719703346491
Training loss on iteration 4140 = 0.1014235969632864
Training loss on iteration 4160 = 0.08505427222698927
Training loss on iteration 4180 = 0.08604704290628433
Training loss on iteration 4200 = 0.08990648854523897
Training loss on iteration 4220 = 0.09380428474396467
Training loss on iteration 4240 = 0.09250356499105691
Training loss on iteration 4260 = 0.08731517661362886
Training loss on iteration 4280 = 0.08955674730241299
Training loss on iteration 4300 = 0.09099188521504402
Training loss on iteration 4320 = 0.10108764544129371
Training loss on iteration 4340 = 0.08308612257242202
Training loss on iteration 4360 = 0.08768489304929972
Training loss on iteration 4380 = 0.08694970030337572
Training loss on iteration 4400 = 0.09330108109861612
Training loss on iteration 4420 = 0.09038823693990708
Training loss on iteration 4440 = 0.09298435896635056
Training loss on iteration 4460 = 0.09917788561433553
Training loss on iteration 4480 = 0.0844321869313717
Training loss on iteration 4500 = 0.08464551940560341
Training loss on iteration 4520 = 0.09045356679707765
Training loss on iteration 4540 = 0.10026189424097538
Training loss on iteration 4560 = 0.08885452523827553
Training loss on iteration 4580 = 0.09184378758072853
Training loss on iteration 4600 = 0.09505486525595189
Training loss on iteration 4620 = 0.08769778180867434
Training loss on iteration 4640 = 0.08297872934490443
Training loss on iteration 4660 = 0.10394300110638141
Training loss on iteration 4680 = 0.09498748891055583
Training loss on iteration 4700 = 0.08658096920698881
Training loss on iteration 4720 = 0.0830400800332427
Training loss on iteration 4740 = 0.09392390623688698
Training loss on iteration 4760 = 0.09483889415860176
Training loss on iteration 4780 = 0.0899102371186018
Training loss on iteration 4800 = 0.08877029027789832
Training loss on iteration 4820 = 0.08241047263145447
Training loss on iteration 4840 = 0.0995409220457077
Training loss on iteration 4860 = 0.09171456005424261
Training loss on iteration 4880 = 0.0818884588778019
Training loss on iteration 4900 = 0.09746589008718728
Training loss on iteration 4920 = 0.08585957456380129
Training loss on iteration 4940 = 0.09214420393109321
Training loss on iteration 4960 = 0.09464028850197792
Training loss on iteration 4980 = 0.08835702762007713
Training loss on iteration 5000 = 0.09685342404991389
Training loss on iteration 5020 = 0.08719596285372973
Training loss on iteration 5040 = 0.08617117516696453
Training loss on iteration 5060 = 0.0808333095163107
Training loss on iteration 5080 = 0.08153346106410027
Training loss on iteration 5100 = 0.0982847822830081
Training loss on iteration 5120 = 0.09171814639121294
Training loss on iteration 5140 = 0.08513826373964548
Training loss on iteration 5160 = 0.08986042011529208
Training loss on iteration 5180 = 0.09467512518167495
Training loss on iteration 5200 = 0.09593546371906995
Training loss on iteration 5220 = 0.08451915178447962
Training loss on iteration 5240 = 0.09397314935922622
Training loss on iteration 5260 = 0.07475849837064744
Training loss on iteration 5280 = 0.08714415859431028
Training loss on iteration 5300 = 0.08940112069249154
Training loss on iteration 5320 = 0.10155158787965775
Training loss on iteration 5340 = 0.09727555084973574
Training loss on iteration 5360 = 0.08994999080896378
Training loss on iteration 5380 = 0.08669040706008672
Training loss on iteration 5400 = 0.09091274179518223
Training loss on iteration 5420 = 0.07990096155554056
Training loss on iteration 5440 = 0.08106308057904243
Training loss on iteration 5460 = 0.10082513298839331
Training loss on iteration 5480 = 0.09831191338598728
Training loss on iteration 5500 = 0.09382726959884166
Training loss on iteration 5520 = 0.08527965079993009
Training loss on iteration 5540 = 0.08635551631450653
Training loss on iteration 5560 = 0.09184413254261017
Training loss on iteration 5580 = 0.09333682656288148
Training loss on iteration 5600 = 0.09403494615107774
Training loss on iteration 5620 = 0.08806060813367367
Training loss on iteration 5640 = 0.07888686116784811
Training loss on iteration 5660 = 0.09416313041001559
Training loss on iteration 5680 = 0.0912005688995123
Training loss on iteration 5700 = 0.09635390043258667
Training loss on iteration 5720 = 0.1049849146977067
Training loss on iteration 5740 = 0.10081272386014462
Training loss on iteration 5760 = 0.08512140288949013
Training loss on iteration 5780 = 0.08935340922325849
Training loss on iteration 5800 = 0.09140572547912598
Training loss on iteration 5820 = 0.08294618017971515
Training loss on iteration 5840 = 0.08663606066256761
Training loss on iteration 5860 = 0.10144497267901897
Training loss on iteration 5880 = 0.08410932701081038
Training loss on iteration 5900 = 0.2563291281461716
Training loss on iteration 5920 = 0.09061457831412553
Training loss on iteration 5940 = 0.07986704204231501
Training loss on iteration 5960 = 0.09855799749493599
Training loss on iteration 5980 = 0.08327545318752527
Training loss on iteration 6000 = 0.08874231334775687
Training loss on iteration 6020 = 0.10839914232492447
Training loss on iteration 6040 = 0.08827789593487978
Training loss on iteration 6060 = 0.0970029341056943
Training loss on iteration 6080 = 0.09329430423676968
Training loss on iteration 6100 = 0.09296574238687753
Training loss on iteration 6120 = 0.0822889419272542
Training loss on iteration 6140 = 0.09126808345317841
Training loss on iteration 6160 = 0.08459883909672498
Training loss on iteration 6180 = 0.08911846540868282
Training loss on iteration 6200 = 0.0931261220946908
Training loss on iteration 6220 = 0.09727467279881238
Training loss on iteration 6240 = 0.1076176194474101
Training loss on iteration 6260 = 0.0854314286261797
Training loss on iteration 6280 = 0.09251891188323498
Training loss on iteration 6300 = 0.09037386272102595
Training loss on iteration 6320 = 0.09593445789068937
Training loss on iteration 6340 = 0.08901618234813213
Training loss on iteration 6360 = 0.10209380462765694
Training loss on iteration 6380 = 0.0931919526308775
Training loss on iteration 0 = 0.08320391178131104
Training loss on iteration 20 = 0.11553464978933334
Training loss on iteration 40 = 0.08870301097631454
Training loss on iteration 60 = 0.08330552447587251
Training loss on iteration 80 = 0.08425795510411263
Training loss on iteration 100 = 0.077012057043612
Training loss on iteration 120 = 0.0860199136659503
Training loss on iteration 140 = 0.07818702720105648
Training loss on iteration 160 = 0.0838029783219099
Training loss on iteration 180 = 0.08023496139794588
Training loss on iteration 200 = 0.07979804407805205
Training loss on iteration 220 = 0.0917277555912733
Training loss on iteration 240 = 0.08549769818782807
Training loss on iteration 260 = 0.0815558711066842
Training loss on iteration 280 = 0.08055748473852872
Training loss on iteration 300 = 0.08038986306637526
Training loss on iteration 320 = 0.0882673416286707
Training loss on iteration 340 = 0.08845937028527259
Training loss on iteration 360 = 0.08191647548228502
Training loss on iteration 380 = 0.07643219139426946
Training loss on iteration 400 = 0.08409136198461056
Training loss on iteration 420 = 0.08940313998609781
Training loss on iteration 440 = 0.0871669478714466
Training loss on iteration 460 = 0.09328699335455895
Training loss on iteration 480 = 0.09022722877562046
Training loss on iteration 500 = 0.08334051165729761
Training loss on iteration 520 = 0.11797643434256315
Training loss on iteration 540 = 0.08594448454678058
Training loss on iteration 560 = 0.08744587618857622
Training loss on iteration 580 = 0.09827662091702223
Training loss on iteration 600 = 0.08542752675712109
Training loss on iteration 620 = 0.08493222128599882
Training loss on iteration 640 = 0.097504423558712
Training loss on iteration 660 = 0.09321981109678745
Training loss on iteration 680 = 0.10158600620925426
Training loss on iteration 700 = 0.07353262864053249
Training loss on iteration 720 = 0.09419492743909359
Training loss on iteration 740 = 0.08874985054135323
Training loss on iteration 760 = 0.0792995523661375
Training loss on iteration 780 = 0.087975131906569
Training loss on iteration 800 = 0.08669450078159571
Training loss on iteration 820 = 0.09705538377165794
Training loss on iteration 840 = 0.08807200342416763
Training loss on iteration 860 = 0.08103032391518354
Training loss on iteration 880 = 0.08312642313539982
Training loss on iteration 900 = 0.08839219193905593
Training loss on iteration 920 = 0.08315233811736107
Training loss on iteration 940 = 0.08012776616960764
Training loss on iteration 960 = 0.08332564607262612
Training loss on iteration 980 = 0.07853508684784175
Training loss on iteration 1000 = 0.0804702254012227
Training loss on iteration 1020 = 0.08232939559966326
Training loss on iteration 1040 = 0.08871669471263885
Training loss on iteration 1060 = 0.08670505583286285
Training loss on iteration 1080 = 0.08225406147539616
Training loss on iteration 1100 = 0.08559392485767603
Training loss on iteration 1120 = 0.09069013688713312
Training loss on iteration 1140 = 0.08543923273682594
Training loss on iteration 1160 = 0.08365239072591066
Training loss on iteration 1180 = 0.08410873711109161
Training loss on iteration 1200 = 0.08780514430254698
Training loss on iteration 1220 = 0.08748535308986902
Training loss on iteration 1240 = 0.08191096428781748
Training loss on iteration 1260 = 0.08595101684331893
Training loss on iteration 1280 = 0.08740571811795235
Training loss on iteration 1300 = 0.08618282992392778
Training loss on iteration 1320 = 0.09070608392357826
Training loss on iteration 1340 = 0.08333655633032322
Training loss on iteration 1360 = 0.08168484177440405
Training loss on iteration 1380 = 0.07863271851092576
Training loss on iteration 1400 = 0.09558074995875358
Training loss on iteration 1420 = 0.079467511177063
Training loss on iteration 1440 = 0.08430428616702557
Training loss on iteration 1460 = 0.09767132960259914
Training loss on iteration 1480 = 0.08067353088408709
Training loss on iteration 1500 = 0.0778657391667366
Training loss on iteration 1520 = 0.10761724580079317
Training loss on iteration 1540 = 0.09317184910178185
Training loss on iteration 1560 = 0.09003328792750835
Training loss on iteration 1580 = 0.08301073890179396
Training loss on iteration 1600 = 0.08303518109023571
Training loss on iteration 1620 = 0.08444786295294762
Training loss on iteration 1640 = 0.08752144407480955
Training loss on iteration 1660 = 0.09779981076717377
Training loss on iteration 1680 = 0.09170634765177965
Training loss on iteration 1700 = 0.08757738266140222
Training loss on iteration 1720 = 0.08436244279146195
Training loss on iteration 1740 = 0.08587978072464467
Training loss on iteration 1760 = 0.0854465352371335
Training loss on iteration 1780 = 0.088709238730371
Training loss on iteration 1800 = 0.07902938351035119
Training loss on iteration 1820 = 0.08513057697564363
Training loss on iteration 1840 = 0.08652365896850825
Training loss on iteration 1860 = 0.08217396214604378
Training loss on iteration 1880 = 0.11271087676286698
Training loss on iteration 1900 = 0.0900132393464446
Training loss on iteration 1920 = 0.09429192822426558
Training loss on iteration 1940 = 0.08774682190269231
Training loss on iteration 1960 = 0.08789340145885945
Training loss on iteration 1980 = 0.08403744995594024
Training loss on iteration 2000 = 0.2463316798210144
Training loss on iteration 2020 = 0.08697511218488216
Training loss on iteration 2040 = 0.09125615991652011
Training loss on iteration 2060 = 0.11456663273274899
Training loss on iteration 2080 = 0.09278092887252569
Training loss on iteration 2100 = 0.08387465868145227
Training loss on iteration 2120 = 0.08899000156670808
Training loss on iteration 2140 = 0.090873471647501
Training loss on iteration 2160 = 0.08884305097162723
Training loss on iteration 2180 = 0.08873814474791289
Training loss on iteration 2200 = 0.23540784306824208
Training loss on iteration 2220 = 0.08270319551229477
Training loss on iteration 2240 = 0.087367076985538
Training loss on iteration 2260 = 0.08066510111093521
Training loss on iteration 2280 = 0.08049374837428332
Training loss on iteration 2300 = 0.08150958493351937
Training loss on iteration 2320 = 0.08580647110939026
Training loss on iteration 2340 = 0.08883271273225546
Training loss on iteration 2360 = 0.08718975000083447
Training loss on iteration 2380 = 0.08252873700112104
Training loss on iteration 2400 = 0.08255603574216366
Training loss on iteration 2420 = 0.08357078339904547
Training loss on iteration 2440 = 0.08570197001099586
Training loss on iteration 2460 = 0.08366492781788111
Training loss on iteration 2480 = 0.08506995402276515
Training loss on iteration 2500 = 0.0922604575753212
Training loss on iteration 2520 = 0.08133107498288154
Training loss on iteration 2540 = 0.0871536998078227
Training loss on iteration 2560 = 0.07729303818196058
Training loss on iteration 2580 = 0.08749669641256333
Training loss on iteration 2600 = 0.08453127481043339
Training loss on iteration 2620 = 0.08134655375033617
Training loss on iteration 2640 = 0.07969154715538025
Training loss on iteration 2660 = 0.08717341516166925
Training loss on iteration 2680 = 0.09183041024953127
Training loss on iteration 2700 = 0.08255648091435433
Training loss on iteration 2720 = 0.09210598394274712
Training loss on iteration 2740 = 0.09403135478496552
Training loss on iteration 2760 = 0.08162546027451753
Training loss on iteration 2780 = 0.08057401925325394
Training loss on iteration 2800 = 0.09323017485439777
Training loss on iteration 2820 = 0.0934569377452135
Training loss on iteration 2840 = 0.08705542236566544
Training loss on iteration 2860 = 0.0862210962921381
Training loss on iteration 2880 = 0.08895818907767535
Training loss on iteration 2900 = 0.08619724605232477
Training loss on iteration 2920 = 0.10472824461758137
Training loss on iteration 2940 = 0.08993421290069818
Training loss on iteration 2960 = 0.08098547887057066
Training loss on iteration 2980 = 0.08696478847414255
Training loss on iteration 3000 = 0.08457997422665357
Training loss on iteration 3020 = 0.0818023269996047
Training loss on iteration 3040 = 0.09084123354405164
Training loss on iteration 3060 = 0.08410297222435474
Training loss on iteration 3080 = 0.09020628780126572
Training loss on iteration 3100 = 0.08871575407683849
Training loss on iteration 3120 = 0.08603917565196753
Training loss on iteration 3140 = 0.08745393771678209
Training loss on iteration 3160 = 0.08764487821608782
Training loss on iteration 3180 = 0.08032328207045794
Training loss on iteration 3200 = 0.0866430750116706
Training loss on iteration 3220 = 0.08561336845159531
Training loss on iteration 3240 = 0.0901863569393754
Training loss on iteration 3260 = 0.08874649833887815
Training loss on iteration 3280 = 0.08709285892546177
Training loss on iteration 3300 = 0.09093901384621858
Training loss on iteration 3320 = 0.07786571346223355
Training loss on iteration 3340 = 0.08218118418008089
Training loss on iteration 3360 = 0.09212509356439114
Training loss on iteration 3380 = 0.08886353243142367
Training loss on iteration 3400 = 0.09647532105445862
Training loss on iteration 3420 = 0.08735085502266884
Training loss on iteration 3440 = 0.08104248829185963
Training loss on iteration 3460 = 0.08950901422649622
Training loss on iteration 3480 = 0.08822346627712249
Training loss on iteration 3500 = 0.10124187208712102
Training loss on iteration 3520 = 0.0825561374425888
Training loss on iteration 3540 = 0.09823862779885531
Training loss on iteration 3560 = 0.08916440643370152
Training loss on iteration 3580 = 0.08751292526721954
Training loss on iteration 3600 = 0.08384885508567094
Training loss on iteration 3620 = 0.08854171317070722
Training loss on iteration 3640 = 0.09144571349024773
Training loss on iteration 3660 = 0.09350185245275497
Training loss on iteration 3680 = 0.08200878221541644
Training loss on iteration 3700 = 0.08727960642427206
Training loss on iteration 3720 = 0.09894156940281391
Training loss on iteration 3740 = 0.08877013120800256
Training loss on iteration 3760 = 0.09607324283570051
Training loss on iteration 3780 = 0.08908512834459544
Training loss on iteration 3800 = 0.08721318971365691
Training loss on iteration 3820 = 0.09682448878884316
Training loss on iteration 3840 = 0.08691991306841373
Training loss on iteration 3860 = 0.09869509562849998
Training loss on iteration 3880 = 0.08339686244726181
Training loss on iteration 3900 = 0.0877560943365097
Training loss on iteration 3920 = 0.09537958949804307
Training loss on iteration 3940 = 0.08873336631804704
Training loss on iteration 3960 = 0.0894603632390499
Training loss on iteration 3980 = 0.08510883320122957
Training loss on iteration 4000 = 0.08578964322805405
Training loss on iteration 4020 = 0.08337324112653732
Training loss on iteration 4040 = 0.09771033562719822
Training loss on iteration 4060 = 0.08971406072378159
Training loss on iteration 4080 = 0.08672976735979318
Training loss on iteration 4100 = 0.09090708792209626
Training loss on iteration 4120 = 0.08753362707793713
Training loss on iteration 4140 = 0.09255129564553499
Training loss on iteration 4160 = 0.09894815031439066
Training loss on iteration 4180 = 0.0869184585288167
Training loss on iteration 4200 = 0.0961673779413104
Training loss on iteration 4220 = 0.08450914211571217
Training loss on iteration 4240 = 0.09045250918716193
Training loss on iteration 4260 = 0.08859850279986858
Training loss on iteration 4280 = 0.08964562695473433
Training loss on iteration 4300 = 0.11197702325880528
Training loss on iteration 4320 = 0.09007120467722415
Training loss on iteration 4340 = 0.08361357152462005
Training loss on iteration 4360 = 0.08341797813773155
Training loss on iteration 4380 = 0.08709894344210625
Training loss on iteration 4400 = 0.09430331569164992
Training loss on iteration 4420 = 0.08347197882831096
Training loss on iteration 4440 = 0.09267949126660824
Training loss on iteration 4460 = 0.09102331679314375
Training loss on iteration 4480 = 0.09261084571480752
Training loss on iteration 4500 = 0.09446417838335038
Training loss on iteration 4520 = 0.10059701316058636
Training loss on iteration 4540 = 0.09425951316952705
Training loss on iteration 4560 = 0.0938262052834034
Training loss on iteration 4580 = 0.09037300981581212
Training loss on iteration 4600 = 0.09005622882395983
Training loss on iteration 4620 = 0.0956606563180685
Training loss on iteration 4640 = 0.09417215995490551
Training loss on iteration 4660 = 0.08356183767318726
Training loss on iteration 4680 = 0.09105433765798807
Training loss on iteration 4700 = 0.09780378267168999
Training loss on iteration 4720 = 0.09193535521626472
Training loss on iteration 4740 = 0.08860746324062348
Training loss on iteration 4760 = 0.0988374238833785
Training loss on iteration 4780 = 0.09777870923280715
Training loss on iteration 4800 = 0.08498718962073326
Training loss on iteration 4820 = 0.08694483358412981
Training loss on iteration 4840 = 0.09207681585103274
Training loss on iteration 4860 = 0.0874810865148902
Training loss on iteration 4880 = 0.0914481958374381
Training loss on iteration 4900 = 0.09453299064189195
Training loss on iteration 4920 = 0.08597885742783547
Training loss on iteration 4940 = 0.08406827617436648
Training loss on iteration 4960 = 0.08878323584794998
Training loss on iteration 4980 = 0.09656851515173911
Training loss on iteration 5000 = 0.093130555562675
Training loss on iteration 5020 = 0.09203452598303556
Training loss on iteration 5040 = 0.10187353454530239
Training loss on iteration 5060 = 0.08877764418721198
Training loss on iteration 5080 = 0.08788204062730073
Training loss on iteration 5100 = 0.0886799281463027
Training loss on iteration 5120 = 0.084991174377501
Training loss on iteration 5140 = 0.08942143023014068
Training loss on iteration 5160 = 0.08470898140221834
Training loss on iteration 5180 = 0.08287768438458443
Training loss on iteration 5200 = 0.09927936363965273
Training loss on iteration 5220 = 0.08532843813300132
Training loss on iteration 5240 = 0.09026208855211734
Training loss on iteration 5260 = 0.08975799083709717
Training loss on iteration 5280 = 0.08430313058197499
Training loss on iteration 5300 = 0.0907433893531561
Training loss on iteration 5320 = 0.1448059927672148
Training loss on iteration 5340 = 0.08451429884880782
Training loss on iteration 5360 = 0.07937904596328735
Training loss on iteration 5380 = 0.09273790512233973
Training loss on iteration 5400 = 0.08782468680292368
Training loss on iteration 5420 = 0.0844994468614459
Training loss on iteration 5440 = 0.09261557180434465
Training loss on iteration 5460 = 0.08847293127328157
Training loss on iteration 5480 = 0.08895451575517654
Training loss on iteration 5500 = 0.10113074146211147
Training loss on iteration 5520 = 0.08343955967575312
Training loss on iteration 5540 = 0.08128427024930715
Training loss on iteration 5560 = 0.09272402059286833
Training loss on iteration 5580 = 0.09887079540640116
Training loss on iteration 5600 = 0.09057574085891247
Training loss on iteration 5620 = 0.09466673992574215
Training loss on iteration 5640 = 0.09557333625853062
Training loss on iteration 5660 = 0.08492656666785478
Training loss on iteration 5680 = 0.08799708783626556
Training loss on iteration 5700 = 0.08813038617372512
Training loss on iteration 5720 = 0.09391291793435812
Training loss on iteration 5740 = 0.0900035822764039
Training loss on iteration 5760 = 0.08468810245394706
Training loss on iteration 5780 = 0.09047654196619988
Training loss on iteration 5800 = 0.09142382759600878
Training loss on iteration 5820 = 0.08943936116993427
Training loss on iteration 5840 = 0.0897721979767084
Training loss on iteration 5860 = 0.08672942370176315
Training loss on iteration 5880 = 0.08121378254145384
Training loss on iteration 5900 = 0.09506638795137405
Training loss on iteration 5920 = 0.08449219148606062
Training loss on iteration 5940 = 0.11683867573738098
Training loss on iteration 5960 = 0.11564083639532327
Training loss on iteration 5980 = 0.08684925734996796
Training loss on iteration 6000 = 0.08554851710796356
Training loss on iteration 6020 = 0.09681197069585323
Training loss on iteration 6040 = 0.1035462699830532
Training loss on iteration 6060 = 0.09321691561490297
Training loss on iteration 6080 = 0.09134051986038685
Training loss on iteration 6100 = 0.08755163215100766
Training loss on iteration 6120 = 0.09371718633919954
Training loss on iteration 6140 = 0.08773715700954199
Training loss on iteration 6160 = 0.09589618258178234
Training loss on iteration 6180 = 0.08578337654471398
Training loss on iteration 6200 = 0.0957279171794653
Training loss on iteration 6220 = 0.09201300796121359
Training loss on iteration 6240 = 0.09164596647024155
Training loss on iteration 6260 = 0.09676468912512064
Training loss on iteration 6280 = 0.08665504511445761
Training loss on iteration 6300 = 0.10725161172449589
Training loss on iteration 6320 = 0.10401716586202384
Training loss on iteration 6340 = 0.09193138144910336
Training loss on iteration 6360 = 0.09138905219733715
Training loss on iteration 6380 = 0.08963741958141327
Training loss on iteration 0 = 0.0763130784034729
Training loss on iteration 20 = 0.09729892499744892
Training loss on iteration 40 = 0.08910209983587265
Training loss on iteration 60 = 0.08803042285144329
Training loss on iteration 80 = 0.08200807590037584
Training loss on iteration 100 = 0.08700264208018779
Training loss on iteration 120 = 0.07953862696886063
Training loss on iteration 140 = 0.0970578171312809
Training loss on iteration 160 = 0.08889968357980252
Training loss on iteration 180 = 0.08315815962851048
Training loss on iteration 200 = 0.08247275836765766
Training loss on iteration 220 = 0.08634127303957939
Training loss on iteration 240 = 0.09348948989063502
Training loss on iteration 260 = 0.08733766954392194
Training loss on iteration 280 = 0.08115601036697626
Training loss on iteration 300 = 0.09285430442541838
Training loss on iteration 320 = 0.08518397230654955
Training loss on iteration 340 = 0.074933722615242
Training loss on iteration 360 = 0.07529678512364626
Training loss on iteration 380 = 0.08338742479681968
Training loss on iteration 400 = 0.1076380243524909
Training loss on iteration 420 = 0.09407998900860548
Training loss on iteration 440 = 0.08311130870133639
Training loss on iteration 460 = 0.10814880765974522
Training loss on iteration 480 = 0.08277673590928317
Training loss on iteration 500 = 0.0848487077280879
Training loss on iteration 520 = 0.08683118484914303
Training loss on iteration 540 = 0.08222536630928516
Training loss on iteration 560 = 0.08544574398547411
Training loss on iteration 580 = 0.0800500601530075
Training loss on iteration 600 = 0.08112574648112059
Training loss on iteration 620 = 0.0905485101044178
Training loss on iteration 640 = 0.08299023378640413
Training loss on iteration 660 = 0.08166616261005402
Training loss on iteration 680 = 0.08263848554342985
Training loss on iteration 700 = 0.09187619443982839
Training loss on iteration 720 = 0.08610910791903734
Training loss on iteration 740 = 0.08384568020701408
Training loss on iteration 760 = 0.08337879180908203
Training loss on iteration 780 = 0.07892662473022938
Training loss on iteration 800 = 0.08560204375535249
Training loss on iteration 820 = 0.07637266032397746
Training loss on iteration 840 = 0.08947376552969218
Training loss on iteration 860 = 0.08138183634728194
Training loss on iteration 880 = 0.09146625138819217
Training loss on iteration 900 = 0.0910740103572607
Training loss on iteration 920 = 0.08009059838950634
Training loss on iteration 940 = 0.08229471612721681
Training loss on iteration 960 = 0.0750667829066515
Training loss on iteration 980 = 0.09583800956606865
Training loss on iteration 1000 = 0.08551958166062831
Training loss on iteration 1020 = 0.07791463658213615
Training loss on iteration 1040 = 0.07911301143467427
Training loss on iteration 1060 = 0.0876020746305585
Training loss on iteration 1080 = 0.07790009193122387
Training loss on iteration 1100 = 0.07751671168953181
Training loss on iteration 1120 = 0.08333431407809258
Training loss on iteration 1140 = 0.07330725360661745
Training loss on iteration 1160 = 0.08082100190222263
Training loss on iteration 1180 = 0.08174928408116103
Training loss on iteration 1200 = 0.07935137189924717
Training loss on iteration 1220 = 0.0863066790625453
Training loss on iteration 1240 = 0.08082219157367945
Training loss on iteration 1260 = 0.08553405832499265
Training loss on iteration 1280 = 0.0947983117774129
Training loss on iteration 1300 = 0.08189106974750757
Training loss on iteration 1320 = 0.08929186910390854
Training loss on iteration 1340 = 0.08719882778823376
Training loss on iteration 1360 = 0.08241659160703421
Training loss on iteration 1380 = 0.0775052947923541
Training loss on iteration 1400 = 0.08375136572867632
Training loss on iteration 1420 = 0.08827181309461593
Training loss on iteration 1440 = 0.07761974204331637
Training loss on iteration 1460 = 0.08304085358977317
Training loss on iteration 1480 = 0.07940164562314748
Training loss on iteration 1500 = 0.09724447131156921
Training loss on iteration 1520 = 0.09392461758106947
Training loss on iteration 1540 = 0.09802863337099552
Training loss on iteration 1560 = 0.08348039574921132
Training loss on iteration 1580 = 0.07835429143160581
Training loss on iteration 1600 = 0.0832820625975728
Training loss on iteration 1620 = 0.07857524659484624
Training loss on iteration 1640 = 0.08284075371921062
Training loss on iteration 1660 = 0.09364865198731423
Training loss on iteration 1680 = 0.09276934619992971
Training loss on iteration 1700 = 0.08990656025707722
Training loss on iteration 1720 = 0.09592158868908882
Training loss on iteration 1740 = 0.08744609002023936
Training loss on iteration 1760 = 0.08174022864550352
Training loss on iteration 1780 = 0.07987076975405216
Training loss on iteration 1800 = 0.08126362599432468
Training loss on iteration 1820 = 0.0896570060402155
Training loss on iteration 1840 = 0.079566116258502
Training loss on iteration 1860 = 0.0830596450716257
Training loss on iteration 1880 = 0.0874461717903614
Training loss on iteration 1900 = 0.08168876264244318
Training loss on iteration 1920 = 0.0932276027277112
Training loss on iteration 1940 = 0.08254726305603981
Training loss on iteration 1960 = 0.083811304718256
Training loss on iteration 1980 = 0.08734982740134001
Training loss on iteration 2000 = 0.0822518864646554
Training loss on iteration 2020 = 0.08401195611804724
Training loss on iteration 2040 = 0.08250699732452631
Training loss on iteration 2060 = 0.08172301277518272
Training loss on iteration 2080 = 0.08789058364927768
Training loss on iteration 2100 = 0.07913661301136017
Training loss on iteration 2120 = 0.08830168321728707
Training loss on iteration 2140 = 0.10492447912693023
Training loss on iteration 2160 = 0.08777914419770241
Training loss on iteration 2180 = 0.08288763854652643
Training loss on iteration 2200 = 0.08481637220829726
Training loss on iteration 2220 = 0.08936687186360359
Training loss on iteration 2240 = 0.08334721755236388
Training loss on iteration 2260 = 0.08392698671668768
Training loss on iteration 2280 = 0.11597424559295177
Training loss on iteration 2300 = 0.09888959340751172
Training loss on iteration 2320 = 0.0863133618608117
Training loss on iteration 2340 = 0.09081351067870855
Training loss on iteration 2360 = 0.08306353986263275
Training loss on iteration 2380 = 0.09011898934841156
Training loss on iteration 2400 = 0.0898220770061016
Training loss on iteration 2420 = 0.10054759494960308
Training loss on iteration 2440 = 0.09359508100897074
Training loss on iteration 2460 = 0.09245575852692127
Training loss on iteration 2480 = 0.10883578881621361
Training loss on iteration 2500 = 0.10481204725801944
Training loss on iteration 2520 = 0.08414293006062508
Training loss on iteration 2540 = 0.07964473329484463
Training loss on iteration 2560 = 0.0802114924415946
Training loss on iteration 2580 = 0.08871003221720457
Training loss on iteration 2600 = 0.08912371788173915
Training loss on iteration 2620 = 0.10089465156197548
Training loss on iteration 2640 = 0.07739646788686513
Training loss on iteration 2660 = 0.08611923344433307
Training loss on iteration 2680 = 0.08318964261561632
Training loss on iteration 2700 = 0.0892679838463664
Training loss on iteration 2720 = 0.08441202361136675
Training loss on iteration 2740 = 0.08961538169533015
Training loss on iteration 2760 = 0.09085959941148758
Training loss on iteration 2780 = 0.09329838529229165
Training loss on iteration 2800 = 0.09038531649857759
Training loss on iteration 2820 = 0.08460098579525947
Training loss on iteration 2840 = 0.107104317471385
Training loss on iteration 2860 = 0.09386984035372734
Training loss on iteration 2880 = 0.09049418959766627
Training loss on iteration 2900 = 0.08253500536084175
Training loss on iteration 2920 = 0.10988819897174835
Training loss on iteration 2940 = 0.08608205392956733
Training loss on iteration 2960 = 0.08473006002604962
Training loss on iteration 2980 = 0.08926373422145843
Training loss on iteration 3000 = 0.08637271914631128
Training loss on iteration 3020 = 0.08162109665572644
Training loss on iteration 3040 = 0.0857156191021204
Training loss on iteration 3060 = 0.08654694277793169
Training loss on iteration 3080 = 0.08264406751841306
Training loss on iteration 3100 = 0.10371026434004307
Training loss on iteration 3120 = 0.07767337691038848
Training loss on iteration 3140 = 0.08319564145058393
Training loss on iteration 3160 = 0.08997147027403116
Training loss on iteration 3180 = 0.08496307134628296
Training loss on iteration 3200 = 0.08568968363106251
Training loss on iteration 3220 = 0.08237171526998281
Training loss on iteration 3240 = 0.08313515614718199
Training loss on iteration 3260 = 0.10006798040121793
Training loss on iteration 3280 = 0.0805470522493124
Training loss on iteration 3300 = 0.09220102187246085
Training loss on iteration 3320 = 0.08239674922078848
Training loss on iteration 3340 = 0.08152747619897127
Training loss on iteration 3360 = 0.09856496304273606
Training loss on iteration 3380 = 0.0960045225918293
Training loss on iteration 3400 = 0.08266329932957887
Training loss on iteration 3420 = 0.0875758146867156
Training loss on iteration 3440 = 0.08670934159308671
Training loss on iteration 3460 = 0.09166075494140387
Training loss on iteration 3480 = 0.0868715824559331
Training loss on iteration 3500 = 0.0994672104716301
Training loss on iteration 3520 = 0.09075703620910644
Training loss on iteration 3540 = 0.08470027726143599
Training loss on iteration 3560 = 0.09007720034569502
Training loss on iteration 3580 = 0.07989930883049964
Training loss on iteration 3600 = 0.0920012028887868
Training loss on iteration 3620 = 0.08940059822052718
Training loss on iteration 3640 = 0.07840338163077831
Training loss on iteration 3660 = 0.08873440027236938
Training loss on iteration 3680 = 0.08505942542105913
Training loss on iteration 3700 = 0.09380500093102455
Training loss on iteration 3720 = 0.0774620009586215
Training loss on iteration 3740 = 0.08787044938653707
Training loss on iteration 3760 = 0.08067359980195761
Training loss on iteration 3780 = 0.08486067652702331
Training loss on iteration 3800 = 0.08536849301308394
Training loss on iteration 3820 = 0.09157700203359127
Training loss on iteration 3840 = 0.11200494673103094
Training loss on iteration 3860 = 0.09101654756814241
Training loss on iteration 3880 = 0.09054236747324466
Training loss on iteration 3900 = 0.0885881220921874
Training loss on iteration 3920 = 0.08488049264997244
Training loss on iteration 3940 = 0.08874847013503313
Training loss on iteration 3960 = 0.0932343153283
Training loss on iteration 3980 = 0.08788305968046188
Training loss on iteration 4000 = 0.09234254956245422
Training loss on iteration 4020 = 0.08414572458714246
Training loss on iteration 4040 = 0.07974559254944324
Training loss on iteration 4060 = 0.0814275711774826
Training loss on iteration 4080 = 0.08290353268384934
Training loss on iteration 4100 = 0.09260792843997478
Training loss on iteration 4120 = 0.09624878652393817
Training loss on iteration 4140 = 0.10392695367336273
Training loss on iteration 4160 = 0.088111761957407
Training loss on iteration 4180 = 0.08863542843610048
Training loss on iteration 4200 = 0.09452839214354754
Training loss on iteration 4220 = 0.09266562201082706
Training loss on iteration 4240 = 0.08257871121168137
Training loss on iteration 4260 = 0.08660595566034317
Training loss on iteration 4280 = 0.09275048281997442
Training loss on iteration 4300 = 0.08745772559195757
Training loss on iteration 4320 = 0.08224895652383565
Training loss on iteration 4340 = 0.08177148215472699
Training loss on iteration 4360 = 0.07985904142260551
Training loss on iteration 4380 = 0.08590480890125037
Training loss on iteration 4400 = 0.08872307017445565
Training loss on iteration 4420 = 0.09737172462046147
Training loss on iteration 4440 = 0.08414900619536639
Training loss on iteration 4460 = 0.08646728601306677
Training loss on iteration 4480 = 0.08384085036814212
Training loss on iteration 4500 = 0.08791022337973117
Training loss on iteration 4520 = 0.08679879866540433
Training loss on iteration 4540 = 0.08175187036395073
Training loss on iteration 4560 = 0.08404241558164358
Training loss on iteration 4580 = 0.08888141699135303
Training loss on iteration 4600 = 0.09261505343019963
Training loss on iteration 4620 = 0.08986302670091391
Training loss on iteration 4640 = 0.0813504682853818
Training loss on iteration 4660 = 0.08388372287154197
Training loss on iteration 4680 = 0.08603547979146242
Training loss on iteration 4700 = 0.08394602090120315
Training loss on iteration 4720 = 0.09897050634026527
Training loss on iteration 4740 = 0.08082404583692551
Training loss on iteration 4760 = 0.08146167267113924
Training loss on iteration 4780 = 0.0815737422555685
Training loss on iteration 4800 = 0.09204328320920467
Training loss on iteration 4820 = 0.24258731864392757
Training loss on iteration 4840 = 0.09383542835712433
Training loss on iteration 4860 = 0.09217687994241715
Training loss on iteration 4880 = 0.09214450400322675
Training loss on iteration 4900 = 0.08555488325655461
Training loss on iteration 4920 = 0.09292071871459484
Training loss on iteration 4940 = 0.23514430839568376
Training loss on iteration 4960 = 0.0857169859111309
Training loss on iteration 4980 = 0.09805754236876965
Training loss on iteration 5000 = 0.08972050566226245
Training loss on iteration 5020 = 0.09626009166240693
Training loss on iteration 5040 = 0.09568054229021072
Training loss on iteration 5060 = 0.15020259004086256
Training loss on iteration 5080 = 0.08755331933498382
Training loss on iteration 5100 = 0.0882099460810423
Training loss on iteration 5120 = 0.08729937318712473
Training loss on iteration 5140 = 0.09480679184198379
Training loss on iteration 5160 = 0.09203420579433441
Training loss on iteration 5180 = 0.09546622987836599
Training loss on iteration 5200 = 0.10007841307669878
Training loss on iteration 5220 = 0.09137444961816073
Training loss on iteration 5240 = 0.08485987931489944
Training loss on iteration 5260 = 0.0874988678842783
Training loss on iteration 5280 = 0.09569109715521336
Training loss on iteration 5300 = 0.0866183066740632
Training loss on iteration 5320 = 0.08965677171945571
Training loss on iteration 5340 = 0.09218979068100452
Training loss on iteration 5360 = 0.10766501240432262
Training loss on iteration 5380 = 0.0904424674808979
Training loss on iteration 5400 = 0.09030597694218159
Training loss on iteration 5420 = 0.08995862547308206
Training loss on iteration 5440 = 0.09374467618763446
Training loss on iteration 5460 = 0.08813497517257929
Training loss on iteration 5480 = 0.08395928237587214
Training loss on iteration 5500 = 0.11374078560620546
Training loss on iteration 5520 = 0.08739179857075215
Training loss on iteration 5540 = 0.07985069844871759
Training loss on iteration 5560 = 0.08764056880027056
Training loss on iteration 5580 = 0.08424653057008982
Training loss on iteration 5600 = 0.0923159472644329
Training loss on iteration 5620 = 0.09179519936442375
Training loss on iteration 5640 = 0.07886762581765652
Training loss on iteration 5660 = 0.08665937390178442
Training loss on iteration 5680 = 0.11156002543866635
Training loss on iteration 5700 = 0.09398756921291351
Training loss on iteration 5720 = 0.09417464397847652
Training loss on iteration 5740 = 0.10120479837059974
Training loss on iteration 5760 = 0.08036558739840985
Training loss on iteration 5780 = 0.08339330442249775
Training loss on iteration 5800 = 0.08114068303257227
Training loss on iteration 5820 = 0.08867096211761236
Training loss on iteration 5840 = 0.0926045075058937
Training loss on iteration 5860 = 0.09208765663206578
Training loss on iteration 5880 = 0.08643897604197263
Training loss on iteration 5900 = 0.09373892452567816
Training loss on iteration 5920 = 0.08983150850981474
Training loss on iteration 5940 = 0.0892263524234295
Training loss on iteration 5960 = 0.08875634297728538
Training loss on iteration 5980 = 0.08653542418032885
Training loss on iteration 6000 = 0.08429861981421709
Training loss on iteration 6020 = 0.0936294760555029
Training loss on iteration 6040 = 0.08336861096322537
Training loss on iteration 6060 = 0.08717832826077938
Training loss on iteration 6080 = 0.08274590112268924
Training loss on iteration 6100 = 0.08687704987823963
Training loss on iteration 6120 = 0.08460337817668914
Training loss on iteration 6140 = 0.08816378023475409
Training loss on iteration 6160 = 0.07957868687808514
Training loss on iteration 6180 = 0.08651362340897321
Training loss on iteration 6200 = 0.10159671865403652
Training loss on iteration 6220 = 0.09042770247906447
Training loss on iteration 6240 = 0.08452801406383514
Training loss on iteration 6260 = 0.08693327568471432
Training loss on iteration 6280 = 0.094084451533854
Training loss on iteration 6300 = 0.08624793104827404
Training loss on iteration 6320 = 0.08894401546567679
Training loss on iteration 6340 = 0.0914506796747446
Training loss on iteration 6360 = 0.08519780188798905
Training loss on iteration 6380 = 0.09094406217336655
Training loss on iteration 0 = 0.0728684812784195
Training loss on iteration 20 = 0.0712169747799635
Training loss on iteration 40 = 0.07697400078177452
Training loss on iteration 60 = 0.07784888166934252
Training loss on iteration 80 = 0.08569440245628357
Training loss on iteration 100 = 0.07723255977034568
Training loss on iteration 120 = 0.08309114556759596
Training loss on iteration 140 = 0.08458775989711284
Training loss on iteration 160 = 0.08422012086957693
Training loss on iteration 180 = 0.08115913458168507
Training loss on iteration 200 = 0.0882722869515419
Training loss on iteration 220 = 0.08761404771357775
Training loss on iteration 240 = 0.08370338417589665
Training loss on iteration 260 = 0.0816862091422081
Training loss on iteration 280 = 0.09276404771953821
Training loss on iteration 300 = 0.0795575788244605
Training loss on iteration 320 = 0.08754389975219964
Training loss on iteration 340 = 0.07789638675749302
Training loss on iteration 360 = 0.08498483821749687
Training loss on iteration 380 = 0.08264463897794486
Training loss on iteration 400 = 0.08239929787814618
Training loss on iteration 420 = 0.07938960269093513
Training loss on iteration 440 = 0.0877680318430066
Training loss on iteration 460 = 0.08484371919184923
Training loss on iteration 480 = 0.07616448532789946
Training loss on iteration 500 = 0.07880004234611988
Training loss on iteration 520 = 0.08001361824572087
Training loss on iteration 540 = 0.08363214489072561
Training loss on iteration 560 = 0.08783391956239939
Training loss on iteration 580 = 0.08418100662529468
Training loss on iteration 600 = 0.10314814969897271
Training loss on iteration 620 = 0.08449620697647334
Training loss on iteration 640 = 0.08833627253770829
Training loss on iteration 660 = 0.0855095386505127
Training loss on iteration 680 = 0.07903045415878296
Training loss on iteration 700 = 0.07841912433505058
Training loss on iteration 720 = 0.08383977636694909
Training loss on iteration 740 = 0.08841807562857866
Training loss on iteration 760 = 0.08397632893174886
Training loss on iteration 780 = 0.08789125289767981
Training loss on iteration 800 = 0.08008851762861013
Training loss on iteration 820 = 0.08908015619963408
Training loss on iteration 840 = 0.09103188123553992
Training loss on iteration 860 = 0.07934013903141021
Training loss on iteration 880 = 0.0874445877969265
Training loss on iteration 900 = 0.0821239162236452
Training loss on iteration 920 = 0.08146011866629124
Training loss on iteration 940 = 0.08024378940463066
Training loss on iteration 960 = 0.2349197318777442
Training loss on iteration 980 = 0.08974827900528907
Training loss on iteration 1000 = 0.08456099312752485
Training loss on iteration 1020 = 0.08291722815483808
Training loss on iteration 1040 = 0.08006058018654585
Training loss on iteration 1060 = 0.08172796443104743
Training loss on iteration 1080 = 0.0857519431039691
Training loss on iteration 1100 = 0.07960349228233099
Training loss on iteration 1120 = 0.08281952627003193
Training loss on iteration 1140 = 0.07899740021675825
Training loss on iteration 1160 = 0.07944533359259368
Training loss on iteration 1180 = 0.08213589955121278
Training loss on iteration 1200 = 0.08244745582342147
Training loss on iteration 1220 = 0.07721643950790166
Training loss on iteration 1240 = 0.08233668636530637
Training loss on iteration 1260 = 0.07478471640497446
Training loss on iteration 1280 = 0.0908518498763442
Training loss on iteration 1300 = 0.08675647154450417
Training loss on iteration 1320 = 0.08050654046237468
Training loss on iteration 1340 = 0.0906971525400877
Training loss on iteration 1360 = 0.08577094618231058
Training loss on iteration 1380 = 0.08338507283478976
Training loss on iteration 1400 = 0.08050463777035474
Training loss on iteration 1420 = 0.08405383471399545
Training loss on iteration 1440 = 0.08798854947090148
Training loss on iteration 1460 = 0.08600355479866266
Training loss on iteration 1480 = 0.07593924161046743
Training loss on iteration 1500 = 0.07766142841428518
Training loss on iteration 1520 = 0.08444394059479236
Training loss on iteration 1540 = 0.08230554033070803
Training loss on iteration 1560 = 0.09415205102413893
Training loss on iteration 1580 = 0.09123233146965504
Training loss on iteration 1600 = 0.08520430363714696
Training loss on iteration 1620 = 0.08517963271588087
Training loss on iteration 1640 = 0.07855465114116669
Training loss on iteration 1660 = 0.0937155632302165
Training loss on iteration 1680 = 0.0833093348890543
Training loss on iteration 1700 = 0.08382495008409023
Training loss on iteration 1720 = 0.08958881720900536
Training loss on iteration 1740 = 0.10089720580726862
Training loss on iteration 1760 = 0.0821303503587842
Training loss on iteration 1780 = 0.07635876536369324
Training loss on iteration 1800 = 0.10974321644753218
Training loss on iteration 1820 = 0.08029816970229149
Training loss on iteration 1840 = 0.07827248070389033
Training loss on iteration 1860 = 0.07950373627245426
Training loss on iteration 1880 = 0.08710297588258982
Training loss on iteration 1900 = 0.08639379236847163
Training loss on iteration 1920 = 0.08836522791534662
Training loss on iteration 1940 = 0.08635749164968728
Training loss on iteration 1960 = 0.08684863280504942
Training loss on iteration 1980 = 0.0886399919167161
Training loss on iteration 2000 = 0.08408411424607039
Training loss on iteration 2020 = 0.08250167611986399
Training loss on iteration 2040 = 0.07765979785472155
Training loss on iteration 2060 = 0.07956462763249875
Training loss on iteration 2080 = 0.08146850969642401
Training loss on iteration 2100 = 0.09103632085025311
Training loss on iteration 2120 = 0.07882965952157975
Training loss on iteration 2140 = 0.08639387786388397
Training loss on iteration 2160 = 0.08150689974427223
Training loss on iteration 2180 = 0.08156345915049315
Training loss on iteration 2200 = 0.08296906817704439
Training loss on iteration 2220 = 0.0848653469234705
Training loss on iteration 2240 = 0.07460563629865646
Training loss on iteration 2260 = 0.09363281913101673
Training loss on iteration 2280 = 0.08656892050057649
Training loss on iteration 2300 = 0.08212548363953828
Training loss on iteration 2320 = 0.08525763638317585
Training loss on iteration 2340 = 0.08566020131111145
Training loss on iteration 2360 = 0.10296694729477167
Training loss on iteration 2380 = 0.08139791749417782
Training loss on iteration 2400 = 0.23018015827983618
Training loss on iteration 2420 = 0.08326945826411247
Training loss on iteration 2440 = 0.1091101286932826
Training loss on iteration 2460 = 0.08538205120712519
Training loss on iteration 2480 = 0.08492194451391696
Training loss on iteration 2500 = 0.07953181751072406
Training loss on iteration 2520 = 0.07891401071101427
Training loss on iteration 2540 = 0.07931854575872421
Training loss on iteration 2560 = 0.09386228304356337
Training loss on iteration 2580 = 0.08370803147554398
Training loss on iteration 2600 = 0.09895920399576426
Training loss on iteration 2620 = 0.07817246727645397
Training loss on iteration 2640 = 0.07971130963414907
Training loss on iteration 2660 = 0.09187841396778822
Training loss on iteration 2680 = 0.08532590419054031
Training loss on iteration 2700 = 0.08901564180850982
Training loss on iteration 2720 = 0.08028657790273427
Training loss on iteration 2740 = 0.08310193680226803
Training loss on iteration 2760 = 0.083062095195055
Training loss on iteration 2780 = 0.07640702780336142
Training loss on iteration 2800 = 0.09013667460530997
Training loss on iteration 2820 = 0.0893376475200057
Training loss on iteration 2840 = 0.08650193829089403
Training loss on iteration 2860 = 0.09612949714064598
Training loss on iteration 2880 = 0.08214980941265822
Training loss on iteration 2900 = 0.08371243439614773
Training loss on iteration 2920 = 0.08791023772209883
Training loss on iteration 2940 = 0.08330365680158139
Training loss on iteration 2960 = 0.09237971678376197
Training loss on iteration 2980 = 0.08867532350122928
Training loss on iteration 3000 = 0.10964105110615492
Training loss on iteration 3020 = 0.08671662379056215
Training loss on iteration 3040 = 0.08417293224483728
Training loss on iteration 3060 = 0.08001445513218641
Training loss on iteration 3080 = 0.0856818739324808
Training loss on iteration 3100 = 0.07888096421957017
Training loss on iteration 3120 = 0.08371055703610182
Training loss on iteration 3140 = 0.08918441776186228
Training loss on iteration 3160 = 0.0788270017132163
Training loss on iteration 3180 = 0.0847602317109704
Training loss on iteration 3200 = 0.0926884576678276
Training loss on iteration 3220 = 0.07834417652338743
Training loss on iteration 3240 = 0.09460352808237076
Training loss on iteration 3260 = 0.07716603446751832
Training loss on iteration 3280 = 0.08213655445724725
Training loss on iteration 3300 = 0.10048969816416502
Training loss on iteration 3320 = 0.1000482864677906
Training loss on iteration 3340 = 0.0758804839104414
Training loss on iteration 3360 = 0.08338580876588822
Training loss on iteration 3380 = 0.09113606512546539
Training loss on iteration 3400 = 0.08338942602276803
Training loss on iteration 3420 = 0.08539210762828589
Training loss on iteration 3440 = 0.0826756190508604
Training loss on iteration 3460 = 0.08320849668234587
Training loss on iteration 3480 = 0.09342394098639488
Training loss on iteration 3500 = 0.07841094303876162
Training loss on iteration 3520 = 0.0850809844210744
Training loss on iteration 3540 = 0.09678225740790367
Training loss on iteration 3560 = 0.08431449681520461
Training loss on iteration 3580 = 0.08578895963728428
Training loss on iteration 3600 = 0.08694543819874526
Training loss on iteration 3620 = 0.08615207653492689
Training loss on iteration 3640 = 0.09078114703297616
Training loss on iteration 3660 = 0.08492729123681783
Training loss on iteration 3680 = 0.09131604544818402
Training loss on iteration 3700 = 0.08604348003864289
Training loss on iteration 3720 = 0.08757766708731651
Training loss on iteration 3740 = 0.08542498182505369
Training loss on iteration 3760 = 0.07925946079194546
Training loss on iteration 3780 = 0.09340115655213595
Training loss on iteration 3800 = 0.08372099418193102
Training loss on iteration 3820 = 0.08140639439225197
Training loss on iteration 3840 = 0.09200751688331366
Training loss on iteration 3860 = 0.07916355468332767
Training loss on iteration 3880 = 0.09096247591078281
Training loss on iteration 3900 = 0.10075685307383538
Training loss on iteration 3920 = 0.08587209228426218
Training loss on iteration 3940 = 0.08651049826294184
Training loss on iteration 3960 = 0.08481431268155575
Training loss on iteration 3980 = 0.08615706358104944
Training loss on iteration 4000 = 0.0900193952023983
Training loss on iteration 4020 = 0.08721484411507845
Training loss on iteration 4040 = 0.08728376589715481
Training loss on iteration 4060 = 0.08005847241729498
Training loss on iteration 4080 = 0.08870339989662171
Training loss on iteration 4100 = 0.0867123207077384
Training loss on iteration 4120 = 0.08574243672192097
Training loss on iteration 4140 = 0.08777351435273886
Training loss on iteration 4160 = 0.08849103674292565
Training loss on iteration 4180 = 0.08582515493035317
Training loss on iteration 4200 = 0.09554016999900342
Training loss on iteration 4220 = 0.10900155119597912
Training loss on iteration 4240 = 0.09068022053688765
Training loss on iteration 4260 = 0.08679027203470469
Training loss on iteration 4280 = 0.08064140807837247
Training loss on iteration 4300 = 0.08411763198673725
Training loss on iteration 4320 = 0.09371924642473459
Training loss on iteration 4340 = 0.07914544828236103
Training loss on iteration 4360 = 0.08355269376188516
Training loss on iteration 4380 = 0.09129917621612549
Training loss on iteration 4400 = 0.0960660083219409
Training loss on iteration 4420 = 0.083957364782691
Training loss on iteration 4440 = 0.08678617216646671
Training loss on iteration 4460 = 0.08847969342023135
Training loss on iteration 4480 = 0.08088159505277873
Training loss on iteration 4500 = 0.08413288705050945
Training loss on iteration 4520 = 0.07826457396149636
Training loss on iteration 4540 = 0.08564644437283278
Training loss on iteration 4560 = 0.10609986204653979
Training loss on iteration 4580 = 0.08205512911081314
Training loss on iteration 4600 = 0.09459022805094719
Training loss on iteration 4620 = 0.0817935885861516
Training loss on iteration 4640 = 0.0895872263237834
Training loss on iteration 4660 = 0.08575620912015439
Training loss on iteration 4680 = 0.0829193701967597
Training loss on iteration 4700 = 0.08240922912955284
Training loss on iteration 4720 = 0.08727108631283045
Training loss on iteration 4740 = 0.08554484602063894
Training loss on iteration 4760 = 0.09602747391909361
Training loss on iteration 4780 = 0.09377768021076918
Training loss on iteration 4800 = 0.0890518980100751
Training loss on iteration 4820 = 0.08264843095093966
Training loss on iteration 4840 = 0.08044145256280899
Training loss on iteration 4860 = 0.09383678920567036
Training loss on iteration 4880 = 0.09093811959028245
Training loss on iteration 4900 = 0.08515741173177957
Training loss on iteration 4920 = 0.08374379258602857
Training loss on iteration 4940 = 0.09218997545540333
Training loss on iteration 4960 = 0.09050575457513332
Training loss on iteration 4980 = 0.08093864470720291
Training loss on iteration 5000 = 0.103955103084445
Training loss on iteration 5020 = 0.08332236241549254
Training loss on iteration 5040 = 0.08464312385767699
Training loss on iteration 5060 = 0.08490070663392543
Training loss on iteration 5080 = 0.08391094505786896
Training loss on iteration 5100 = 0.09299269914627076
Training loss on iteration 5120 = 0.08166928347200156
Training loss on iteration 5140 = 0.14168404936790466
Training loss on iteration 5160 = 0.08722055051475763
Training loss on iteration 5180 = 0.08668909333646298
Training loss on iteration 5200 = 0.08393216375261545
Training loss on iteration 5220 = 0.09755493719130755
Training loss on iteration 5240 = 0.08826870545744896
Training loss on iteration 5260 = 0.08396890386939049
Training loss on iteration 5280 = 0.0928565425798297
Training loss on iteration 5300 = 0.0924016460776329
Training loss on iteration 5320 = 0.08608656264841556
Training loss on iteration 5340 = 0.07579600866883993
Training loss on iteration 5360 = 0.08451002985239028
Training loss on iteration 5380 = 0.08918177746236325
Training loss on iteration 5400 = 0.08136015329509974
Training loss on iteration 5420 = 0.0975246425718069
Training loss on iteration 5440 = 0.08549723699688912
Training loss on iteration 5460 = 0.09193281065672636
Training loss on iteration 5480 = 0.08924769051373005
Training loss on iteration 5500 = 0.08506934121251106
Training loss on iteration 5520 = 0.0906191872432828
Training loss on iteration 5540 = 0.08208563532680273
Training loss on iteration 5560 = 0.08391616977751255
Training loss on iteration 5580 = 0.07528243195265531
Training loss on iteration 5600 = 0.09092767350375652
Training loss on iteration 5620 = 0.08339304048568011
Training loss on iteration 5640 = 0.08032606616616249
Training loss on iteration 5660 = 0.07921365853399039
Training loss on iteration 5680 = 0.09620925914496184
Training loss on iteration 5700 = 0.08865100611001253
Training loss on iteration 5720 = 0.10900993309915066
Training loss on iteration 5740 = 0.10660230983048677
Training loss on iteration 5760 = 0.0864235544577241
Training loss on iteration 5780 = 0.08877362906932831
Training loss on iteration 5800 = 0.0864633971825242
Training loss on iteration 5820 = 0.08746006563305855
Training loss on iteration 5840 = 0.09430001322180033
Training loss on iteration 5860 = 0.09143374729901552
Training loss on iteration 5880 = 0.0857056338340044
Training loss on iteration 5900 = 0.1027859089896083
Training loss on iteration 5920 = 0.09118581898510456
Training loss on iteration 5940 = 0.09242588728666305
Training loss on iteration 5960 = 0.10525501109659671
Training loss on iteration 5980 = 0.08351772874593735
Training loss on iteration 6000 = 0.09062360264360905
Training loss on iteration 6020 = 0.08418038450181484
Training loss on iteration 6040 = 0.08109788671135902
Training loss on iteration 6060 = 0.09011933729052543
Training loss on iteration 6080 = 0.08817398436367511
Training loss on iteration 6100 = 0.07719267588108777
Training loss on iteration 6120 = 0.1054310405626893
Training loss on iteration 6140 = 0.09036788791418075
Training loss on iteration 6160 = 0.09364367984235286
Training loss on iteration 6180 = 0.07751789204776287
Training loss on iteration 6200 = 0.09138488620519639
Training loss on iteration 6220 = 0.1047885064035654
Training loss on iteration 6240 = 0.08672862648963928
Training loss on iteration 6260 = 0.08194354698061942
Training loss on iteration 6280 = 0.0858080305159092
Training loss on iteration 6300 = 0.09386854562908412
Training loss on iteration 6320 = 0.082891096919775
Training loss on iteration 6340 = 0.09843450114130974
Training loss on iteration 6360 = 0.10477793030440807
Training loss on iteration 6380 = 0.08871483672410249
Training loss on iteration 0 = 0.05548476800322533
Training loss on iteration 20 = 0.08559756949543953
Training loss on iteration 40 = 0.0730640297755599
Training loss on iteration 60 = 0.09675097577273846
Training loss on iteration 80 = 0.08246802538633347
Training loss on iteration 100 = 0.08621509578078985
Training loss on iteration 120 = 0.07948820106685162
Training loss on iteration 140 = 0.0790733901783824
Training loss on iteration 160 = 0.07278374526649714
Training loss on iteration 180 = 0.08439192213118077
Training loss on iteration 200 = 0.07871272079646588
Training loss on iteration 220 = 0.08553355764597655
Training loss on iteration 240 = 0.07956858929246664
Training loss on iteration 260 = 0.080110421217978
Training loss on iteration 280 = 0.07683127485215664
Training loss on iteration 300 = 0.08809421006590128
Training loss on iteration 320 = 0.08046127911657094
Training loss on iteration 340 = 0.08442234508693218
Training loss on iteration 360 = 0.08247356209903955
Training loss on iteration 380 = 0.0753187445923686
Training loss on iteration 400 = 0.07463657837361097
Training loss on iteration 420 = 0.08058193791657686
Training loss on iteration 440 = 0.07423407137393952
Training loss on iteration 460 = 0.07605165522545576
Training loss on iteration 480 = 0.07811251115053892
Training loss on iteration 500 = 0.08077614158391952
Training loss on iteration 520 = 0.08074594009667635
Training loss on iteration 540 = 0.0800770914182067
Training loss on iteration 560 = 0.10736578796058893
Training loss on iteration 580 = 0.08406345658004284
Training loss on iteration 600 = 0.10119067821651698
Training loss on iteration 620 = 0.0810196552425623
Training loss on iteration 640 = 0.07865727599710226
Training loss on iteration 660 = 0.07654777616262436
Training loss on iteration 680 = 0.0880268957465887
Training loss on iteration 700 = 0.08096231538802386
Training loss on iteration 720 = 0.10001898296177388
Training loss on iteration 740 = 0.0775287615135312
Training loss on iteration 760 = 0.0953671308234334
Training loss on iteration 780 = 0.08049172982573509
Training loss on iteration 800 = 0.09008177667856217
Training loss on iteration 820 = 0.09346630461513997
Training loss on iteration 840 = 0.07532281316816807
Training loss on iteration 860 = 0.07788788918405772
Training loss on iteration 880 = 0.08331147003918886
Training loss on iteration 900 = 0.09333840645849704
Training loss on iteration 920 = 0.09870912991464138
Training loss on iteration 940 = 0.08513056822121143
Training loss on iteration 960 = 0.09103390984237195
Training loss on iteration 980 = 0.08185988701879979
Training loss on iteration 1000 = 0.08636319246143102
Training loss on iteration 1020 = 0.0873157661408186
Training loss on iteration 1040 = 0.0861156502738595
Training loss on iteration 1060 = 0.08022924661636352
Training loss on iteration 1080 = 0.07814238239079714
Training loss on iteration 1100 = 0.09513260200619697
Training loss on iteration 1120 = 0.10052718427032233
Training loss on iteration 1140 = 0.09303514156490564
Training loss on iteration 1160 = 0.08783640712499619
Training loss on iteration 1180 = 0.08319420162588358
Training loss on iteration 1200 = 0.0834083829075098
Training loss on iteration 1220 = 0.08494239021092653
Training loss on iteration 1240 = 0.24227932896465063
Training loss on iteration 1260 = 0.08063783627003432
Training loss on iteration 1280 = 0.08451208285987377
Training loss on iteration 1300 = 0.0895020293071866
Training loss on iteration 1320 = 0.09542084112763405
Training loss on iteration 1340 = 0.0852164113894105
Training loss on iteration 1360 = 0.08670712821185589
Training loss on iteration 1380 = 0.08101942241191865
Training loss on iteration 1400 = 0.08561898339539767
Training loss on iteration 1420 = 0.08869863953441381
Training loss on iteration 1440 = 0.07641482446342707
Training loss on iteration 1460 = 0.07720053773373366
Training loss on iteration 1480 = 0.07837320528924466
Training loss on iteration 1500 = 0.08792657852172851
Training loss on iteration 1520 = 0.07634644787758589
Training loss on iteration 1540 = 0.08063591681420804
Training loss on iteration 1560 = 0.08138849996030331
Training loss on iteration 1580 = 0.08631677404046059
Training loss on iteration 1600 = 0.08417069353163242
Training loss on iteration 1620 = 0.09374653156846761
Training loss on iteration 1640 = 0.07694078460335732
Training loss on iteration 1660 = 0.08487985711544752
Training loss on iteration 1680 = 0.08507283572107553
Training loss on iteration 1700 = 0.08139643501490354
Training loss on iteration 1720 = 0.07816293649375439
Training loss on iteration 1740 = 0.08515819665044547
Training loss on iteration 1760 = 0.08312040381133556
Training loss on iteration 1780 = 0.08502532113343478
Training loss on iteration 1800 = 0.08350923135876656
Training loss on iteration 1820 = 0.08576934468001127
Training loss on iteration 1840 = 0.07836481127887965
Training loss on iteration 1860 = 0.07737597618252039
Training loss on iteration 1880 = 0.09168978985399008
Training loss on iteration 1900 = 0.08182299733161927
Training loss on iteration 1920 = 0.0888521259650588
Training loss on iteration 1940 = 0.07689682878553868
Training loss on iteration 1960 = 0.08574353046715259
Training loss on iteration 1980 = 0.07682697866111994
Training loss on iteration 2000 = 0.08755101058632135
Training loss on iteration 2020 = 0.08667730577290059
Training loss on iteration 2040 = 0.0833783695474267
Training loss on iteration 2060 = 0.08191673271358013
Training loss on iteration 2080 = 0.0870794016867876
Training loss on iteration 2100 = 0.08252141997218132
Training loss on iteration 2120 = 0.08430821932852268
Training loss on iteration 2140 = 0.08113259933888912
Training loss on iteration 2160 = 0.09806014224886894
Training loss on iteration 2180 = 0.08489138968288898
Training loss on iteration 2200 = 0.08368609752506018
Training loss on iteration 2220 = 0.07737747430801392
Training loss on iteration 2240 = 0.0803364209830761
Training loss on iteration 2260 = 0.07835228163748979
Training loss on iteration 2280 = 0.0779574440792203
Training loss on iteration 2300 = 0.0774311039596796
Training loss on iteration 2320 = 0.08456404283642768
Training loss on iteration 2340 = 0.09167155772447586
Training loss on iteration 2360 = 0.07756305001676082
Training loss on iteration 2380 = 0.08148686233907938
Training loss on iteration 2400 = 0.08889510519802571
Training loss on iteration 2420 = 0.07635252345353365
Training loss on iteration 2440 = 0.09049502778798342
Training loss on iteration 2460 = 0.08533320222049952
Training loss on iteration 2480 = 0.08883970510214567
Training loss on iteration 2500 = 0.07449726108461618
Training loss on iteration 2520 = 0.0808999640867114
Training loss on iteration 2540 = 0.08189976159483195
Training loss on iteration 2560 = 0.08214981015771627
Training loss on iteration 2580 = 0.08862629234790802
Training loss on iteration 2600 = 0.07560697197914124
Training loss on iteration 2620 = 0.08359447158873082
Training loss on iteration 2640 = 0.08451691903173923
Training loss on iteration 2660 = 0.09046290181577206
Training loss on iteration 2680 = 0.08111615497618914
Training loss on iteration 2700 = 0.08782336823642253
Training loss on iteration 2720 = 0.08686832226812839
Training loss on iteration 2740 = 0.09219414889812469
Training loss on iteration 2760 = 0.08670568466186523
Training loss on iteration 2780 = 0.07676989976316691
Training loss on iteration 2800 = 0.09169094320386648
Training loss on iteration 2820 = 0.09015919957309962
Training loss on iteration 2840 = 0.07988215740770102
Training loss on iteration 2860 = 0.08230755217373371
Training loss on iteration 2880 = 0.08480884600430727
Training loss on iteration 2900 = 0.08629219606518745
Training loss on iteration 2920 = 0.08671953584998845
Training loss on iteration 2940 = 0.08251031525433064
Training loss on iteration 2960 = 0.083614881336689
Training loss on iteration 2980 = 0.08322815466672182
Training loss on iteration 3000 = 0.0844432657584548
Training loss on iteration 3020 = 0.08641300592571496
Training loss on iteration 3040 = 0.08220243845134974
Training loss on iteration 3060 = 0.08123809341341257
Training loss on iteration 3080 = 0.09877459909766913
Training loss on iteration 3100 = 0.08079667631536722
Training loss on iteration 3120 = 0.08856383450329304
Training loss on iteration 3140 = 0.08158127423375845
Training loss on iteration 3160 = 0.09287136904895306
Training loss on iteration 3180 = 0.08261065427213907
Training loss on iteration 3200 = 0.08609474338591099
Training loss on iteration 3220 = 0.08403559885919094
Training loss on iteration 3240 = 0.08959002457559109
Training loss on iteration 3260 = 0.08920763432979584
Training loss on iteration 3280 = 0.08459818717092275
Training loss on iteration 3300 = 0.08863533195108175
Training loss on iteration 3320 = 0.08365494515746832
Training loss on iteration 3340 = 0.09745322745293379
Training loss on iteration 3360 = 0.08325160779058934
Training loss on iteration 3380 = 0.07720355857163667
Training loss on iteration 3400 = 0.08751672748476266
Training loss on iteration 3420 = 0.07847332041710615
Training loss on iteration 3440 = 0.08277947045862674
Training loss on iteration 3460 = 0.08328472208231688
Training loss on iteration 3480 = 0.08393439501523972
Training loss on iteration 3500 = 0.08686747197061777
Training loss on iteration 3520 = 0.09546737764030695
Training loss on iteration 3540 = 0.08393339049071073
Training loss on iteration 3560 = 0.08563539087772369
Training loss on iteration 3580 = 0.0840412287041545
Training loss on iteration 3600 = 0.08831028770655394
Training loss on iteration 3620 = 0.09324301742017269
Training loss on iteration 3640 = 0.08916390538215638
Training loss on iteration 3660 = 0.08831187393516302
Training loss on iteration 3680 = 0.09531566984951496
Training loss on iteration 3700 = 0.08875091671943665
Training loss on iteration 3720 = 0.07958928104490041
Training loss on iteration 3740 = 0.08748567495495081
Training loss on iteration 3760 = 0.08567194789648055
Training loss on iteration 3780 = 0.08784485384821891
Training loss on iteration 3800 = 0.08206584062427283
Training loss on iteration 3820 = 0.09647349268198013
Training loss on iteration 3840 = 0.10004869811236858
Training loss on iteration 3860 = 0.08926071431487799
Training loss on iteration 3880 = 0.08258468639105558
Training loss on iteration 3900 = 0.09071615245193243
Training loss on iteration 3920 = 0.08195538390427828
Training loss on iteration 3940 = 0.14456937052309513
Training loss on iteration 3960 = 0.09178650602698327
Training loss on iteration 3980 = 0.08279114346951247
Training loss on iteration 4000 = 0.08308409117162227
Training loss on iteration 4020 = 0.09289251025766135
Training loss on iteration 4040 = 0.08625264093279839
Training loss on iteration 4060 = 0.09041074141860009
Training loss on iteration 4080 = 0.08637348674237728
Training loss on iteration 4100 = 0.07378589622676372
Training loss on iteration 4120 = 0.08377099297940731
Training loss on iteration 4140 = 0.08433259129524232
Training loss on iteration 4160 = 0.09543499667197466
Training loss on iteration 4180 = 0.09311401657760143
Training loss on iteration 4200 = 0.08474742397665977
Training loss on iteration 4220 = 0.09062729515135289
Training loss on iteration 4240 = 0.08089360389858484
Training loss on iteration 4260 = 0.11157897338271142
Training loss on iteration 4280 = 0.07629669774323702
Training loss on iteration 4300 = 0.07779357265681028
Training loss on iteration 4320 = 0.08568676076829433
Training loss on iteration 4340 = 0.08829913064837455
Training loss on iteration 4360 = 0.08350297249853611
Training loss on iteration 4380 = 0.08219315018504858
Training loss on iteration 4400 = 0.09057368375360966
Training loss on iteration 4420 = 0.08875783104449511
Training loss on iteration 4440 = 0.08948853258043528
Training loss on iteration 4460 = 0.08207510076463223
Training loss on iteration 4480 = 0.08075623493641615
Training loss on iteration 4500 = 0.08339546341449022
Training loss on iteration 4520 = 0.08992466405034065
Training loss on iteration 4540 = 0.08681079819798469
Training loss on iteration 4560 = 0.08838725574314595
Training loss on iteration 4580 = 0.08544572740793228
Training loss on iteration 4600 = 0.0943812096491456
Training loss on iteration 4620 = 0.08542226739227772
Training loss on iteration 4640 = 0.08164323251694441
Training loss on iteration 4660 = 0.0817406304180622
Training loss on iteration 4680 = 0.08630041237920523
Training loss on iteration 4700 = 0.08872328512370586
Training loss on iteration 4720 = 0.11598215755075217
Training loss on iteration 4740 = 0.08547697048634291
Training loss on iteration 4760 = 0.0892729751765728
Training loss on iteration 4780 = 0.08728120010346174
Training loss on iteration 4800 = 0.08212546948343516
Training loss on iteration 4820 = 0.08914215341210366
Training loss on iteration 4840 = 0.07826143074780703
Training loss on iteration 4860 = 0.08325834292918444
Training loss on iteration 4880 = 0.07888360358774663
Training loss on iteration 4900 = 0.07971364185214043
Training loss on iteration 4920 = 0.09404712747782469
Training loss on iteration 4940 = 0.08626344501972198
Training loss on iteration 4960 = 0.08812640085816384
Training loss on iteration 4980 = 0.09272366166114807
Training loss on iteration 5000 = 0.0979364337399602
Training loss on iteration 5020 = 0.08198593892157077
Training loss on iteration 5040 = 0.07969813458621502
Training loss on iteration 5060 = 0.09047757294028998
Training loss on iteration 5080 = 0.08618741296231747
Training loss on iteration 5100 = 0.0796125628054142
Training loss on iteration 5120 = 0.08818067442625761
Training loss on iteration 5140 = 0.08694940041750669
Training loss on iteration 5160 = 0.0790329521521926
Training loss on iteration 5180 = 0.08304828479886055
Training loss on iteration 5200 = 0.0831008704379201
Training loss on iteration 5220 = 0.08149594739079476
Training loss on iteration 5240 = 0.0780243493616581
Training loss on iteration 5260 = 0.08464308213442565
Training loss on iteration 5280 = 0.08154801372438669
Training loss on iteration 5300 = 0.08615896683186293
Training loss on iteration 5320 = 0.2359978325664997
Training loss on iteration 5340 = 0.08445107955485583
Training loss on iteration 5360 = 0.09235171712934971
Training loss on iteration 5380 = 0.0950826307758689
Training loss on iteration 5400 = 0.0867836382240057
Training loss on iteration 5420 = 0.07914516106247901
Training loss on iteration 5440 = 0.07575424555689096
Training loss on iteration 5460 = 0.08400318846106529
Training loss on iteration 5480 = 0.08123064450919629
Training loss on iteration 5500 = 0.08405126836150885
Training loss on iteration 5520 = 0.08058088514953851
Training loss on iteration 5540 = 0.08510102964937687
Training loss on iteration 5560 = 0.09207445476204157
Training loss on iteration 5580 = 0.08994872625917197
Training loss on iteration 5600 = 0.08298931196331978
Training loss on iteration 5620 = 0.08351386301219463
Training loss on iteration 5640 = 0.09119912795722485
Training loss on iteration 5660 = 0.0817525938153267
Training loss on iteration 5680 = 0.08718706369400024
Training loss on iteration 5700 = 0.08715920932590962
Training loss on iteration 5720 = 0.09113810192793607
Training loss on iteration 5740 = 0.08174135480076075
Training loss on iteration 5760 = 0.08452045414596795
Training loss on iteration 5780 = 0.08601199705153703
Training loss on iteration 5800 = 0.113422616943717
Training loss on iteration 5820 = 0.08530545141547918
Training loss on iteration 5840 = 0.08153236396610737
Training loss on iteration 5860 = 0.08600716572254896
Training loss on iteration 5880 = 0.08180416524410247
Training loss on iteration 5900 = 0.10629860516637564
Training loss on iteration 5920 = 0.09493757467716932
Training loss on iteration 5940 = 0.09172595273703336
Training loss on iteration 5960 = 0.08137028031051159
Training loss on iteration 5980 = 0.08073447588831187
Training loss on iteration 6000 = 0.08962913230061531
Training loss on iteration 6020 = 0.08533067256212234
Training loss on iteration 6040 = 0.08676071595400572
Training loss on iteration 6060 = 0.0750523617491126
Training loss on iteration 6080 = 0.08826206177473069
Training loss on iteration 6100 = 0.08928204514086246
Training loss on iteration 6120 = 0.08005777057260274
Training loss on iteration 6140 = 0.08122182860970498
Training loss on iteration 6160 = 0.09165393374860287
Training loss on iteration 6180 = 0.08610067330300808
Training loss on iteration 6200 = 0.08154019322246313
Training loss on iteration 6220 = 0.09020051714032888
Training loss on iteration 6240 = 0.08743277676403523
Training loss on iteration 6260 = 0.08777422402054072
Training loss on iteration 6280 = 0.09319628961384296
Training loss on iteration 6300 = 0.0914801586419344
Training loss on iteration 6320 = 0.0802003936842084
Training loss on iteration 6340 = 0.08531891871243716
Training loss on iteration 6360 = 0.08901663701981306
Training loss on iteration 6380 = 0.08560221511870622
Training loss on iteration 0 = 0.09527956694364548
Training loss on iteration 20 = 0.07341534364968538
Training loss on iteration 40 = 0.0722144840285182
Training loss on iteration 60 = 0.07146064173430204
Training loss on iteration 80 = 0.07440246399492026
Training loss on iteration 100 = 0.08230602331459522
Training loss on iteration 120 = 0.07877846825867892
Training loss on iteration 140 = 0.09105728510767222
Training loss on iteration 160 = 0.07752224802970886
Training loss on iteration 180 = 0.09036848023533821
Training loss on iteration 200 = 0.0776521509513259
Training loss on iteration 220 = 0.07890960276126861
Training loss on iteration 240 = 0.08981803394854068
Training loss on iteration 260 = 0.0711215402930975
Training loss on iteration 280 = 0.0832353588193655
Training loss on iteration 300 = 0.07724887412041426
Training loss on iteration 320 = 0.08387954384088517
Training loss on iteration 340 = 0.07615542951971292
Training loss on iteration 360 = 0.07413930092006922
Training loss on iteration 380 = 0.07658089511096478
Training loss on iteration 400 = 0.07269104719161987
Training loss on iteration 420 = 0.0857976010069251
Training loss on iteration 440 = 0.07512008212506771
Training loss on iteration 460 = 0.0730817012488842
Training loss on iteration 480 = 0.07797804623842239
Training loss on iteration 500 = 0.0937903368845582
Training loss on iteration 520 = 0.07925906870514154
Training loss on iteration 540 = 0.0864467591047287
Training loss on iteration 560 = 0.08116862550377846
Training loss on iteration 580 = 0.07809540573507548
Training loss on iteration 600 = 0.07875574138015509
Training loss on iteration 620 = 0.07452279664576053
Training loss on iteration 640 = 0.07480102479457855
Training loss on iteration 660 = 0.08644196577370167
Training loss on iteration 680 = 0.0784970372915268
Training loss on iteration 700 = 0.08077983912080526
Training loss on iteration 720 = 0.08046262338757515
Training loss on iteration 740 = 0.08114441912621259
Training loss on iteration 760 = 0.08678779434412717
Training loss on iteration 780 = 0.08306124713271856
Training loss on iteration 800 = 0.07954876683652401
Training loss on iteration 820 = 0.08529657684266567
Training loss on iteration 840 = 0.0786373969167471
Training loss on iteration 860 = 0.08375590667128563
Training loss on iteration 880 = 0.07602733168751001
Training loss on iteration 900 = 0.07337745353579521
Training loss on iteration 920 = 0.07677041720598936
Training loss on iteration 940 = 0.07981760203838348
Training loss on iteration 960 = 0.09379884228110313
Training loss on iteration 980 = 0.08566630948334933
Training loss on iteration 1000 = 0.07863617632538081
Training loss on iteration 1020 = 0.10635576583445072
Training loss on iteration 1040 = 0.08002778682857752
Training loss on iteration 1060 = 0.08412431031465531
Training loss on iteration 1080 = 0.07901805769652129
Training loss on iteration 1100 = 0.07598464041948319
Training loss on iteration 1120 = 0.08032437115907669
Training loss on iteration 1140 = 0.08761542029678822
Training loss on iteration 1160 = 0.0780521823093295
Training loss on iteration 1180 = 0.08623684216290713
Training loss on iteration 1200 = 0.08261015079915524
Training loss on iteration 1220 = 0.08298517055809498
Training loss on iteration 1240 = 0.08143568858504295
Training loss on iteration 1260 = 0.07826231457293034
Training loss on iteration 1280 = 0.07869025729596615
Training loss on iteration 1300 = 0.08545082937926055
Training loss on iteration 1320 = 0.07941743806004524
Training loss on iteration 1340 = 0.08147149495780467
Training loss on iteration 1360 = 0.08590837717056274
Training loss on iteration 1380 = 0.09859320521354675
Training loss on iteration 1400 = 0.0752398258075118
Training loss on iteration 1420 = 0.08269937559962273
Training loss on iteration 1440 = 0.07782917730510235
Training loss on iteration 1460 = 0.07896701451390982
Training loss on iteration 1480 = 0.07728716209530831
Training loss on iteration 1500 = 0.08175816126167774
Training loss on iteration 1520 = 0.07593917530030012
Training loss on iteration 1540 = 0.08066338617354632
Training loss on iteration 1560 = 0.08954820558428764
Training loss on iteration 1580 = 0.08177429307252168
Training loss on iteration 1600 = 0.09669983107596636
Training loss on iteration 1620 = 0.08608434498310089
Training loss on iteration 1640 = 0.10275870133191348
Training loss on iteration 1660 = 0.08212873823940754
Training loss on iteration 1680 = 0.07678029425442219
Training loss on iteration 1700 = 0.07704805918037891
Training loss on iteration 1720 = 0.09946399256587028
Training loss on iteration 1740 = 0.07165743913501502
Training loss on iteration 1760 = 0.09793621711432934
Training loss on iteration 1780 = 0.0874130941927433
Training loss on iteration 1800 = 0.0841299606487155
Training loss on iteration 1820 = 0.08234036900103092
Training loss on iteration 1840 = 0.0895799221470952
Training loss on iteration 1860 = 0.08182780388742686
Training loss on iteration 1880 = 0.0802516968920827
Training loss on iteration 1900 = 0.08638488240540028
Training loss on iteration 1920 = 0.08379304762929678
Training loss on iteration 1940 = 0.08720547743141652
Training loss on iteration 1960 = 0.08809763491153717
Training loss on iteration 1980 = 0.09030212685465813
Training loss on iteration 2000 = 0.08320473656058311
Training loss on iteration 2020 = 0.08919775672256947
Training loss on iteration 2040 = 0.08145389948040246
Training loss on iteration 2060 = 0.0789644856005907
Training loss on iteration 2080 = 0.08046436756849289
Training loss on iteration 2100 = 0.07830409165471792
Training loss on iteration 2120 = 0.08247627336531878
Training loss on iteration 2140 = 0.08758394289761781
Training loss on iteration 2160 = 0.07929453663527966
Training loss on iteration 2180 = 0.08474072609096765
Training loss on iteration 2200 = 0.08492254000157118
Training loss on iteration 2220 = 0.0910467404872179
Training loss on iteration 2240 = 0.0821858698502183
Training loss on iteration 2260 = 0.08135444652289152
Training loss on iteration 2280 = 0.08153007496148348
Training loss on iteration 2300 = 0.08482046090066434
Training loss on iteration 2320 = 0.08738503120839596
Training loss on iteration 2340 = 0.08594755455851555
Training loss on iteration 2360 = 0.08369934894144535
Training loss on iteration 2380 = 0.08086020555347204
Training loss on iteration 2400 = 0.0872881269082427
Training loss on iteration 2420 = 0.08485174421221017
Training loss on iteration 2440 = 0.08243379313498736
Training loss on iteration 2460 = 0.091205676458776
Training loss on iteration 2480 = 0.09211911372840405
Training loss on iteration 2500 = 0.08432606216520071
Training loss on iteration 2520 = 0.08255659751594066
Training loss on iteration 2540 = 0.08580959867686033
Training loss on iteration 2560 = 0.10860375706106425
Training loss on iteration 2580 = 0.08661131095141172
Training loss on iteration 2600 = 0.07554104402661324
Training loss on iteration 2620 = 0.07754771653562784
Training loss on iteration 2640 = 0.07779743559658528
Training loss on iteration 2660 = 0.07330609578639269
Training loss on iteration 2680 = 0.08075299356132745
Training loss on iteration 2700 = 0.09488648232072591
Training loss on iteration 2720 = 0.0886774331331253
Training loss on iteration 2740 = 0.0808890575543046
Training loss on iteration 2760 = 0.08492344133555889
Training loss on iteration 2780 = 0.09710806775838136
Training loss on iteration 2800 = 0.0877658085897565
Training loss on iteration 2820 = 0.0801816375926137
Training loss on iteration 2840 = 0.07978749126195908
Training loss on iteration 2860 = 0.07818360049277544
Training loss on iteration 2880 = 0.08696749825030566
Training loss on iteration 2900 = 0.079208698682487
Training loss on iteration 2920 = 0.08101623021066189
Training loss on iteration 2940 = 0.08300492390990258
Training loss on iteration 2960 = 0.09023486878722906
Training loss on iteration 2980 = 0.08619885966181755
Training loss on iteration 3000 = 0.07837501727044582
Training loss on iteration 3020 = 0.08421035762876272
Training loss on iteration 3040 = 0.08869204688817263
Training loss on iteration 3060 = 0.07749932557344437
Training loss on iteration 3080 = 0.08498195800930261
Training loss on iteration 3100 = 0.0892869271337986
Training loss on iteration 3120 = 0.08340058270841837
Training loss on iteration 3140 = 0.0839722553268075
Training loss on iteration 3160 = 0.08507718741893769
Training loss on iteration 3180 = 0.08725354466587305
Training loss on iteration 3200 = 0.08265246246010065
Training loss on iteration 3220 = 0.08599401507526636
Training loss on iteration 3240 = 0.08547466564923525
Training loss on iteration 3260 = 0.07952682375907898
Training loss on iteration 3280 = 0.07629752568900586
Training loss on iteration 3300 = 0.08156390804797412
Training loss on iteration 3320 = 0.07735069859772921
Training loss on iteration 3340 = 0.0826671727001667
Training loss on iteration 3360 = 0.08200948387384414
Training loss on iteration 3380 = 0.10574872568249702
Training loss on iteration 3400 = 0.07863095086067914
Training loss on iteration 3420 = 0.08575338311493397
Training loss on iteration 3440 = 0.08689132574945688
Training loss on iteration 3460 = 0.08632842544466257
Training loss on iteration 3480 = 0.08468871675431729
Training loss on iteration 3500 = 0.08370459731668234
Training loss on iteration 3520 = 0.08345575984567404
Training loss on iteration 3540 = 0.0817587710916996
Training loss on iteration 3560 = 0.09079045113176107
Training loss on iteration 3580 = 0.0758660800755024
Training loss on iteration 3600 = 0.08299473430961371
Training loss on iteration 3620 = 0.08231649845838547
Training loss on iteration 3640 = 0.08721047937870026
Training loss on iteration 3660 = 0.0864146713167429
Training loss on iteration 3680 = 0.08503333684056998
Training loss on iteration 3700 = 0.0776230439543724
Training loss on iteration 3720 = 0.08052272014319897
Training loss on iteration 3740 = 0.0752832267433405
Training loss on iteration 3760 = 0.0823271481320262
Training loss on iteration 3780 = 0.08891370072960854
Training loss on iteration 3800 = 0.09313297737389803
Training loss on iteration 3820 = 0.08851384781301022
Training loss on iteration 3840 = 0.07871373426169156
Training loss on iteration 3860 = 0.07568447813391685
Training loss on iteration 3880 = 0.07831772584468126
Training loss on iteration 3900 = 0.08507367260754109
Training loss on iteration 3920 = 0.09796102046966552
Training loss on iteration 3940 = 0.08010684233158827
Training loss on iteration 3960 = 0.09303174391388894
Training loss on iteration 3980 = 0.092778873257339
Training loss on iteration 4000 = 0.0906298415735364
Training loss on iteration 4020 = 0.08610583022236824
Training loss on iteration 4040 = 0.08312638122588396
Training loss on iteration 4060 = 0.09007339589297772
Training loss on iteration 4080 = 0.08348772637546062
Training loss on iteration 4100 = 0.0836003601551056
Training loss on iteration 4120 = 0.08438611794263125
Training loss on iteration 4140 = 0.0773754756897688
Training loss on iteration 4160 = 0.08202494084835052
Training loss on iteration 4180 = 0.07925538327544927
Training loss on iteration 4200 = 0.08361719120293856
Training loss on iteration 4220 = 0.08260549250990153
Training loss on iteration 4240 = 0.2367676230147481
Training loss on iteration 4260 = 0.08665259201079607
Training loss on iteration 4280 = 0.09046101905405521
Training loss on iteration 4300 = 0.08432608991861343
Training loss on iteration 4320 = 0.08511475659906864
Training loss on iteration 4340 = 0.08493505399674177
Training loss on iteration 4360 = 0.09507201798260212
Training loss on iteration 4380 = 0.07577651720494032
Training loss on iteration 4400 = 0.08625321425497531
Training loss on iteration 4420 = 0.10769076887518167
Training loss on iteration 4440 = 0.08576702903956175
Training loss on iteration 4460 = 0.10722453650087119
Training loss on iteration 4480 = 0.08304765019565821
Training loss on iteration 4500 = 0.08178097363561392
Training loss on iteration 4520 = 0.08330814521759748
Training loss on iteration 4540 = 0.09268838614225387
Training loss on iteration 4560 = 0.08106507398188115
Training loss on iteration 4580 = 0.08782882783561945
Training loss on iteration 4600 = 0.09590796325355769
Training loss on iteration 4620 = 0.0801560526713729
Training loss on iteration 4640 = 0.0886924782767892
Training loss on iteration 4660 = 0.09005956836044789
Training loss on iteration 4680 = 0.08365534208714961
Training loss on iteration 4700 = 0.08759413380175829
Training loss on iteration 4720 = 0.23935587294399738
Training loss on iteration 4740 = 0.08184313736855983
Training loss on iteration 4760 = 0.08868056219071149
Training loss on iteration 4780 = 0.08096092529594898
Training loss on iteration 4800 = 0.08235721401870251
Training loss on iteration 4820 = 0.08193816374987364
Training loss on iteration 4840 = 0.0864827822893858
Training loss on iteration 4860 = 0.0767401933670044
Training loss on iteration 4880 = 0.08576946593821048
Training loss on iteration 4900 = 0.08181146159768105
Training loss on iteration 4920 = 0.0877828823402524
Training loss on iteration 4940 = 0.09368581883609295
Training loss on iteration 4960 = 0.08831864409148693
Training loss on iteration 4980 = 0.08866151012480258
Training loss on iteration 5000 = 0.08756427317857743
Training loss on iteration 5020 = 0.07557613309472799
Training loss on iteration 5040 = 0.08562637511640787
Training loss on iteration 5060 = 0.07327774781733751
Training loss on iteration 5080 = 0.08979051914066076
Training loss on iteration 5100 = 0.08202520813792943
Training loss on iteration 5120 = 0.08789148759096861
Training loss on iteration 5140 = 0.08725620526820421
Training loss on iteration 5160 = 0.08193652015179395
Training loss on iteration 5180 = 0.09467853102833032
Training loss on iteration 5200 = 0.0849718714132905
Training loss on iteration 5220 = 0.07925106156617404
Training loss on iteration 5240 = 0.10485416501760483
Training loss on iteration 5260 = 0.09239730108529329
Training loss on iteration 5280 = 0.09045742377638817
Training loss on iteration 5300 = 0.13756515681743622
Training loss on iteration 5320 = 0.08532741740345955
Training loss on iteration 5340 = 0.07991630770266056
Training loss on iteration 5360 = 0.08472235016524791
Training loss on iteration 5380 = 0.0868639824911952
Training loss on iteration 5400 = 0.07954270672053099
Training loss on iteration 5420 = 0.08507353458553553
Training loss on iteration 5440 = 0.0794600047171116
Training loss on iteration 5460 = 0.0791219787672162
Training loss on iteration 5480 = 0.08633276540786028
Training loss on iteration 5500 = 0.08747077304869891
Training loss on iteration 5520 = 0.08933729957789183
Training loss on iteration 5540 = 0.07812058776617051
Training loss on iteration 5560 = 0.0880654590204358
Training loss on iteration 5580 = 0.08779542688280344
Training loss on iteration 5600 = 0.08455375973135233
Training loss on iteration 5620 = 0.08312048520892859
Training loss on iteration 5640 = 0.09076380953192711
Training loss on iteration 5660 = 0.07852603867650032
Training loss on iteration 5680 = 0.08010707907378674
Training loss on iteration 5700 = 0.08597093038260936
Training loss on iteration 5720 = 0.08818121105432511
Training loss on iteration 5740 = 0.07887838315218687
Training loss on iteration 5760 = 0.08698531202971935
Training loss on iteration 5780 = 0.08176911473274232
Training loss on iteration 5800 = 0.08066686615347862
Training loss on iteration 5820 = 0.08524978887289762
Training loss on iteration 5840 = 0.08909158632159234
Training loss on iteration 5860 = 0.0835901740938425
Training loss on iteration 5880 = 0.07383356411010027
Training loss on iteration 5900 = 0.08602792881429196
Training loss on iteration 5920 = 0.08474546223878861
Training loss on iteration 5940 = 0.08036057539284229
Training loss on iteration 5960 = 0.08901309482753277
Training loss on iteration 5980 = 0.08837643302977086
Training loss on iteration 6000 = 0.07710565216839313
Training loss on iteration 6020 = 0.08829257842153311
Training loss on iteration 6040 = 0.08245999459177256
Training loss on iteration 6060 = 0.08785192798823119
Training loss on iteration 6080 = 0.0876985140144825
Training loss on iteration 6100 = 0.08742891978472471
Training loss on iteration 6120 = 0.08586944080889225
Training loss on iteration 6140 = 0.08001422379165887
Training loss on iteration 6160 = 0.08903842959553003
Training loss on iteration 6180 = 0.08638371285051108
Training loss on iteration 6200 = 0.08501903396099805
Training loss on iteration 6220 = 0.0839269582182169
Training loss on iteration 6240 = 0.09050112999975682
Training loss on iteration 6260 = 0.08203253485262393
Training loss on iteration 6280 = 0.10549158565700054
Training loss on iteration 6300 = 0.10198633354157209
Training loss on iteration 6320 = 0.08986248094588518
Training loss on iteration 6340 = 0.0874086257070303
Training loss on iteration 6360 = 0.08874690756201745
Training loss on iteration 6380 = 0.09513144828379154
Training loss on iteration 0 = 0.0663696676492691
Training loss on iteration 20 = 0.08125290926545858
Training loss on iteration 40 = 0.07845117859542369
Training loss on iteration 60 = 0.08551726024597883
Training loss on iteration 80 = 0.08688867166638374
Training loss on iteration 100 = 0.08865438867360353
Training loss on iteration 120 = 0.08118791803717613
Training loss on iteration 140 = 0.08682780154049397
Training loss on iteration 160 = 0.07752442006021738
Training loss on iteration 180 = 0.0820608090609312
Training loss on iteration 200 = 0.08014470376074315
Training loss on iteration 220 = 0.07700454816222191
Training loss on iteration 240 = 0.07398253809660674
Training loss on iteration 260 = 0.07699374351650476
Training loss on iteration 280 = 0.08328308686614036
Training loss on iteration 300 = 0.07619306780397891
Training loss on iteration 320 = 0.08229514434933663
Training loss on iteration 340 = 0.08788522072136402
Training loss on iteration 360 = 0.08206311892718077
Training loss on iteration 380 = 0.0875229150056839
Training loss on iteration 400 = 0.07937698028981685
Training loss on iteration 420 = 0.08034777492284775
Training loss on iteration 440 = 0.07603229954838753
Training loss on iteration 460 = 0.07312339115887881
Training loss on iteration 480 = 0.07902241386473179
Training loss on iteration 500 = 0.07446292005479335
Training loss on iteration 520 = 0.08492654953151942
Training loss on iteration 540 = 0.08167193494737149
Training loss on iteration 560 = 0.10894820094108582
Training loss on iteration 580 = 0.09031894356012345
Training loss on iteration 600 = 0.06925975196063519
Training loss on iteration 620 = 0.08166575618088245
Training loss on iteration 640 = 0.07764448001980781
Training loss on iteration 660 = 0.08108652997761964
Training loss on iteration 680 = 0.07964924573898316
Training loss on iteration 700 = 0.0852364057675004
Training loss on iteration 720 = 0.08047615978866815
Training loss on iteration 740 = 0.07324547991156578
Training loss on iteration 760 = 0.07112493552267551
Training loss on iteration 780 = 0.08234137427061797
Training loss on iteration 800 = 0.07693179082125426
Training loss on iteration 820 = 0.0804867148399353
Training loss on iteration 840 = 0.0712138207629323
Training loss on iteration 860 = 0.08991054072976112
Training loss on iteration 880 = 0.08777365442365408
Training loss on iteration 900 = 0.08379165083169937
Training loss on iteration 920 = 0.07514357082545757
Training loss on iteration 940 = 0.07944440450519323
Training loss on iteration 960 = 0.08850679136812686
Training loss on iteration 980 = 0.07978893667459488
Training loss on iteration 1000 = 0.07927516121417284
Training loss on iteration 1020 = 0.07688745055347682
Training loss on iteration 1040 = 0.08324127718806267
Training loss on iteration 1060 = 0.07942451369017363
Training loss on iteration 1080 = 0.07860082555562257
Training loss on iteration 1100 = 0.08007602263242006
Training loss on iteration 1120 = 0.07301134821027518
Training loss on iteration 1140 = 0.08690827041864395
Training loss on iteration 1160 = 0.08293768595904112
Training loss on iteration 1180 = 0.08703408129513264
Training loss on iteration 1200 = 0.08711035083979368
Training loss on iteration 1220 = 0.0866384707391262
Training loss on iteration 1240 = 0.07981926631182432
Training loss on iteration 1260 = 0.07837905623018741
Training loss on iteration 1280 = 0.07838696017861366
Training loss on iteration 1300 = 0.08534804992377758
Training loss on iteration 1320 = 0.07761213146150112
Training loss on iteration 1340 = 0.09187361709773541
Training loss on iteration 1360 = 0.07844312284141779
Training loss on iteration 1380 = 0.08500257078558207
Training loss on iteration 1400 = 0.08952867221087217
Training loss on iteration 1420 = 0.0812372075393796
Training loss on iteration 1440 = 0.08399919159710408
Training loss on iteration 1460 = 0.07608334682881832
Training loss on iteration 1480 = 0.09028299748897553
Training loss on iteration 1500 = 0.08304934948682785
Training loss on iteration 1520 = 0.07917439937591553
Training loss on iteration 1540 = 0.09722080864012242
Training loss on iteration 1560 = 0.07893524523824454
Training loss on iteration 1580 = 0.08685086630284786
Training loss on iteration 1600 = 0.07178993076086045
Training loss on iteration 1620 = 0.0858166079968214
Training loss on iteration 1640 = 0.0818100232630968
Training loss on iteration 1660 = 0.08216697294265032
Training loss on iteration 1680 = 0.08047176096588374
Training loss on iteration 1700 = 0.07296966593712569
Training loss on iteration 1720 = 0.07609588317573071
Training loss on iteration 1740 = 0.08427449073642493
Training loss on iteration 1760 = 0.07667336240410805
Training loss on iteration 1780 = 0.08078124728053808
Training loss on iteration 1800 = 0.07593303304165602
Training loss on iteration 1820 = 0.08002840746194124
Training loss on iteration 1840 = 0.08289873898029328
Training loss on iteration 1860 = 0.08877481333911419
Training loss on iteration 1880 = 0.09194236174225807
Training loss on iteration 1900 = 0.077430584654212
Training loss on iteration 1920 = 0.07333435975015164
Training loss on iteration 1940 = 0.08544034734368325
Training loss on iteration 1960 = 0.07941520903259516
Training loss on iteration 1980 = 0.09062066040933132
Training loss on iteration 2000 = 0.08019200395792722
Training loss on iteration 2020 = 0.08475717846304179
Training loss on iteration 2040 = 0.07569446414709091
Training loss on iteration 2060 = 0.08420216124504805
Training loss on iteration 2080 = 0.08153893016278743
Training loss on iteration 2100 = 0.08203726299107075
Training loss on iteration 2120 = 0.0792546221986413
Training loss on iteration 2140 = 0.08093206007033586
Training loss on iteration 2160 = 0.08906967733055353
Training loss on iteration 2180 = 0.08919764757156372
Training loss on iteration 2200 = 0.07756430022418499
Training loss on iteration 2220 = 0.08680232707411051
Training loss on iteration 2240 = 0.0848935093730688
Training loss on iteration 2260 = 0.10091718304902315
Training loss on iteration 2280 = 0.08130975719541311
Training loss on iteration 2300 = 0.08172629196196794
Training loss on iteration 2320 = 0.09214320201426744
Training loss on iteration 2340 = 0.0819475781172514
Training loss on iteration 2360 = 0.07948727495968341
Training loss on iteration 2380 = 0.07720218989998102
Training loss on iteration 2400 = 0.08379741460084915
Training loss on iteration 2420 = 0.0781868064776063
Training loss on iteration 2440 = 0.07155251745134592
Training loss on iteration 2460 = 0.09513522237539292
Training loss on iteration 2480 = 0.08322090245783328
Training loss on iteration 2500 = 0.08324134070426226
Training loss on iteration 2520 = 0.07903223820030689
Training loss on iteration 2540 = 0.08792009074240922
Training loss on iteration 2560 = 0.0910703431814909
Training loss on iteration 2580 = 0.0801112400367856
Training loss on iteration 2600 = 0.0884131658822298
Training loss on iteration 2620 = 0.08265528418123722
Training loss on iteration 2640 = 0.07920707743614912
Training loss on iteration 2660 = 0.07879821863025427
Training loss on iteration 2680 = 0.08324664551764727
Training loss on iteration 2700 = 0.08735706936568022
Training loss on iteration 2720 = 0.08148449715226888
Training loss on iteration 2740 = 0.08135874960571528
Training loss on iteration 2760 = 0.08758458197116852
Training loss on iteration 2780 = 0.08351254370063543
Training loss on iteration 2800 = 0.08151611257344485
Training loss on iteration 2820 = 0.08593944218009711
Training loss on iteration 2840 = 0.085649473965168
Training loss on iteration 2860 = 0.09392345286905765
Training loss on iteration 2880 = 0.08597165700048208
Training loss on iteration 2900 = 0.0804326955229044
Training loss on iteration 2920 = 0.07352203354239464
Training loss on iteration 2940 = 0.07925037927925586
Training loss on iteration 2960 = 0.07797576021403074
Training loss on iteration 2980 = 0.07884638104587793
Training loss on iteration 3000 = 0.08771493081003427
Training loss on iteration 3020 = 0.08704853467643262
Training loss on iteration 3040 = 0.08332230411469936
Training loss on iteration 3060 = 0.0920936718583107
Training loss on iteration 3080 = 0.0845819441601634
Training loss on iteration 3100 = 0.07232974227517844
Training loss on iteration 3120 = 0.07771606538444757
Training loss on iteration 3140 = 0.08240858223289252
Training loss on iteration 3160 = 0.08309029061347246
Training loss on iteration 3180 = 0.08399894423782825
Training loss on iteration 3200 = 0.08103769328445196
Training loss on iteration 3220 = 0.09455841630697251
Training loss on iteration 3240 = 0.07934770490974188
Training loss on iteration 3260 = 0.08667287658900022
Training loss on iteration 3280 = 0.0777594916522503
Training loss on iteration 3300 = 0.07036388386040926
Training loss on iteration 3320 = 0.07757019530981779
Training loss on iteration 3340 = 0.22527674157172442
Training loss on iteration 3360 = 0.08527043964713812
Training loss on iteration 3380 = 0.08183722030371428
Training loss on iteration 3400 = 0.08132438305765391
Training loss on iteration 3420 = 0.08487946260720491
Training loss on iteration 3440 = 0.08209043890237808
Training loss on iteration 3460 = 0.0840623401105404
Training loss on iteration 3480 = 0.08415402881801129
Training loss on iteration 3500 = 0.07936254460364581
Training loss on iteration 3520 = 0.0890476094558835
Training loss on iteration 3540 = 0.0774620857089758
Training loss on iteration 3560 = 0.0885469626635313
Training loss on iteration 3580 = 0.08492664378136397
Training loss on iteration 3600 = 0.08280080109834671
Training loss on iteration 3620 = 0.0818140048533678
Training loss on iteration 3640 = 0.08565691895782948
Training loss on iteration 3660 = 0.08861396200954914
Training loss on iteration 3680 = 0.08104335516691208
Training loss on iteration 3700 = 0.09068315699696541
Training loss on iteration 3720 = 0.08437444642186165
Training loss on iteration 3740 = 0.08427478913217783
Training loss on iteration 3760 = 0.07902461979538203
Training loss on iteration 3780 = 0.08508597668260336
Training loss on iteration 3800 = 0.08289771843701602
Training loss on iteration 3820 = 0.08278038520365953
Training loss on iteration 3840 = 0.08308253455907107
Training loss on iteration 3860 = 0.07624408695846796
Training loss on iteration 3880 = 0.07824352011084557
Training loss on iteration 3900 = 0.07525501474738121
Training loss on iteration 3920 = 0.0848406009376049
Training loss on iteration 3940 = 0.0802911004051566
Training loss on iteration 3960 = 0.07994695715606212
Training loss on iteration 3980 = 0.08336468208581209
Training loss on iteration 4000 = 0.07794490847736597
Training loss on iteration 4020 = 0.0745681520551443
Training loss on iteration 4040 = 0.0871385021135211
Training loss on iteration 4060 = 0.08620914462953806
Training loss on iteration 4080 = 0.08560862056910992
Training loss on iteration 4100 = 0.08367488738149405
Training loss on iteration 4120 = 0.08358919396996498
Training loss on iteration 4140 = 0.08986275736242533
Training loss on iteration 4160 = 0.08528345171362162
Training loss on iteration 4180 = 0.08704877626150846
Training loss on iteration 4200 = 0.08879559449851512
Training loss on iteration 4220 = 0.08059345483779908
Training loss on iteration 4240 = 0.08512838110327721
Training loss on iteration 4260 = 0.07802085876464844
Training loss on iteration 4280 = 0.07831177040934563
Training loss on iteration 4300 = 0.0798274775967002
Training loss on iteration 4320 = 0.0708821127191186
Training loss on iteration 4340 = 0.07823950536549092
Training loss on iteration 4360 = 0.07658204808831215
Training loss on iteration 4380 = 0.0847847493365407
Training loss on iteration 4400 = 0.07960205636918545
Training loss on iteration 4420 = 0.0791961669921875
Training loss on iteration 4440 = 0.08201038278639317
Training loss on iteration 4460 = 0.0916548315435648
Training loss on iteration 4480 = 0.0809386495500803
Training loss on iteration 4500 = 0.07778419740498066
Training loss on iteration 4520 = 0.0843253331258893
Training loss on iteration 4540 = 0.08180766273289919
Training loss on iteration 4560 = 0.0886354522779584
Training loss on iteration 4580 = 0.0787578858435154
Training loss on iteration 4600 = 0.08556213658303022
Training loss on iteration 4620 = 0.08156385440379381
Training loss on iteration 4640 = 0.08348109386861324
Training loss on iteration 4660 = 0.07703176196664571
Training loss on iteration 4680 = 0.0898476041853428
Training loss on iteration 4700 = 0.08963918853551149
Training loss on iteration 4720 = 0.08748770579695701
Training loss on iteration 4740 = 0.08851316217333079
Training loss on iteration 4760 = 0.0784687815234065
Training loss on iteration 4780 = 0.07998691368848085
Training loss on iteration 4800 = 0.08662841096520424
Training loss on iteration 4820 = 0.08056295234709979
Training loss on iteration 4840 = 0.08263428695499897
Training loss on iteration 4860 = 0.07977272886782885
Training loss on iteration 4880 = 0.10047704484313727
Training loss on iteration 4900 = 0.08087083660066127
Training loss on iteration 4920 = 0.07369646281003953
Training loss on iteration 4940 = 0.08458018992096186
Training loss on iteration 4960 = 0.08001269977539778
Training loss on iteration 4980 = 0.07433301825076341
Training loss on iteration 5000 = 0.08081169184297324
Training loss on iteration 5020 = 0.0891973614692688
Training loss on iteration 5040 = 0.09117023590952159
Training loss on iteration 5060 = 0.07588511370122433
Training loss on iteration 5080 = 0.07911920044571161
Training loss on iteration 5100 = 0.07596468180418015
Training loss on iteration 5120 = 0.08163157515227795
Training loss on iteration 5140 = 0.07466614376753569
Training loss on iteration 5160 = 0.08292717356234788
Training loss on iteration 5180 = 0.08833883870393038
Training loss on iteration 5200 = 0.08253930006176233
Training loss on iteration 5220 = 0.08771268762648106
Training loss on iteration 5240 = 0.07808942552655936
Training loss on iteration 5260 = 0.0805381579324603
Training loss on iteration 5280 = 0.08984021246433258
Training loss on iteration 5300 = 0.07334420066326856
Training loss on iteration 5320 = 0.0807032274082303
Training loss on iteration 5340 = 0.08090328257530928
Training loss on iteration 5360 = 0.08827298786491156
Training loss on iteration 5380 = 0.0791873101145029
Training loss on iteration 5400 = 0.12162300944328308
Training loss on iteration 5420 = 0.08187570963054895
Training loss on iteration 5440 = 0.07810548171401024
Training loss on iteration 5460 = 0.23275227304548024
Training loss on iteration 5480 = 0.10940453745424747
Training loss on iteration 5500 = 0.09008682388812303
Training loss on iteration 5520 = 0.08521496970206499
Training loss on iteration 5540 = 0.07860405296087265
Training loss on iteration 5560 = 0.08236746080219745
Training loss on iteration 5580 = 0.08524806499481201
Training loss on iteration 5600 = 0.07369649149477482
Training loss on iteration 5620 = 0.09008176997303963
Training loss on iteration 5640 = 0.08220725897699595
Training loss on iteration 5660 = 0.08149167764931917
Training loss on iteration 5680 = 0.08805439192801714
Training loss on iteration 5700 = 0.08287607505917549
Training loss on iteration 5720 = 0.08688260540366173
Training loss on iteration 5740 = 0.139340578019619
Training loss on iteration 5760 = 0.08019619565457106
Training loss on iteration 5780 = 0.09003678131848573
Training loss on iteration 5800 = 0.0811073448508978
Training loss on iteration 5820 = 0.08795420601963996
Training loss on iteration 5840 = 0.07978383433073759
Training loss on iteration 5860 = 0.07348730973899364
Training loss on iteration 5880 = 0.08776644300669431
Training loss on iteration 5900 = 0.10165472701191902
Training loss on iteration 5920 = 0.08022862542420625
Training loss on iteration 5940 = 0.0871205285191536
Training loss on iteration 5960 = 0.07816434688866139
Training loss on iteration 5980 = 0.10803925544023514
Training loss on iteration 6000 = 0.11526227723807096
Training loss on iteration 6020 = 0.08217372186481953
Training loss on iteration 6040 = 0.08207002282142639
Training loss on iteration 6060 = 0.0921265659853816
Training loss on iteration 6080 = 0.09321128986775876
Training loss on iteration 6100 = 0.08574096374213695
Training loss on iteration 6120 = 0.08813567664474249
Training loss on iteration 6140 = 0.08887328412383795
Training loss on iteration 6160 = 0.07594935707747937
Training loss on iteration 6180 = 0.08713242448866368
Training loss on iteration 6200 = 0.09329588040709495
Training loss on iteration 6220 = 0.09877746496349574
Training loss on iteration 6240 = 0.08406262379139662
Training loss on iteration 6260 = 0.08880582209676505
Training loss on iteration 6280 = 0.08276425041258335
Training loss on iteration 6300 = 0.07810717038810253
Training loss on iteration 6320 = 0.08691701963543892
Training loss on iteration 6340 = 0.08455820456147194
Training loss on iteration 6360 = 0.09064641650766134
Training loss on iteration 6380 = 0.09608637150377035
Training loss on iteration 0 = 0.09077225625514984
Training loss on iteration 20 = 0.07670436948537826
Training loss on iteration 40 = 0.07781156953424215
Training loss on iteration 60 = 0.08124489393085241
Training loss on iteration 80 = 0.08008471131324768
Training loss on iteration 100 = 0.07211307007819415
Training loss on iteration 120 = 0.07615835908800364
Training loss on iteration 140 = 0.07599258646368981
Training loss on iteration 160 = 0.07460512612015009
Training loss on iteration 180 = 0.07622057385742664
Training loss on iteration 200 = 0.09604843128472566
Training loss on iteration 220 = 0.08260441273450851
Training loss on iteration 240 = 0.07539078425616026
Training loss on iteration 260 = 0.0675694776698947
Training loss on iteration 280 = 0.07020997554063797
Training loss on iteration 300 = 0.08045916054397821
Training loss on iteration 320 = 0.07708964962512255
Training loss on iteration 340 = 0.08385721929371356
Training loss on iteration 360 = 0.07959773391485214
Training loss on iteration 380 = 0.08177107889205218
Training loss on iteration 400 = 0.08278101868927479
Training loss on iteration 420 = 0.08131124563515187
Training loss on iteration 440 = 0.07761783674359321
Training loss on iteration 460 = 0.08074674513190985
Training loss on iteration 480 = 0.0776442639529705
Training loss on iteration 500 = 0.07391770947724581
Training loss on iteration 520 = 0.08312128465622663
Training loss on iteration 540 = 0.07623032107949257
Training loss on iteration 560 = 0.0819348443299532
Training loss on iteration 580 = 0.08888714779168368
Training loss on iteration 600 = 0.07738424893468618
Training loss on iteration 620 = 0.22924876529723406
Training loss on iteration 640 = 0.08160756044089794
Training loss on iteration 660 = 0.07981218304485083
Training loss on iteration 680 = 0.07886761780828237
Training loss on iteration 700 = 0.08091786950826645
Training loss on iteration 720 = 0.0793765714392066
Training loss on iteration 740 = 0.07247981168329716
Training loss on iteration 760 = 0.07885131910443306
Training loss on iteration 780 = 0.09072616025805473
Training loss on iteration 800 = 0.08403304982930422
Training loss on iteration 820 = 0.07486821357160807
Training loss on iteration 840 = 0.08200646750628948
Training loss on iteration 860 = 0.07634214088320732
Training loss on iteration 880 = 0.07639462053775788
Training loss on iteration 900 = 0.08301855269819498
Training loss on iteration 920 = 0.07748554311692715
Training loss on iteration 940 = 0.0795454265549779
Training loss on iteration 960 = 0.07550563588738442
Training loss on iteration 980 = 0.08455254565924405
Training loss on iteration 1000 = 0.07693222090601921
Training loss on iteration 1020 = 0.07638848870992661
Training loss on iteration 1040 = 0.08645375818014145
Training loss on iteration 1060 = 0.09204715620726348
Training loss on iteration 1080 = 0.07551525179296732
Training loss on iteration 1100 = 0.0694799181073904
Training loss on iteration 1120 = 0.08159274291247129
Training loss on iteration 1140 = 0.06939037442207337
Training loss on iteration 1160 = 0.07651465591043234
Training loss on iteration 1180 = 0.08570630718022584
Training loss on iteration 1200 = 0.07767509054392577
Training loss on iteration 1220 = 0.08493163473904133
Training loss on iteration 1240 = 0.10144608579576016
Training loss on iteration 1260 = 0.08592824544757605
Training loss on iteration 1280 = 0.07890608757734299
Training loss on iteration 1300 = 0.08548540100455285
Training loss on iteration 1320 = 0.07812657486647367
Training loss on iteration 1340 = 0.08316246271133423
Training loss on iteration 1360 = 0.07503272984176874
Training loss on iteration 1380 = 0.07433027196675539
Training loss on iteration 1400 = 0.07777127772569656
Training loss on iteration 1420 = 0.08046080581843854
Training loss on iteration 1440 = 0.08265345953404904
Training loss on iteration 1460 = 0.09305882342159748
Training loss on iteration 1480 = 0.08290615007281303
Training loss on iteration 1500 = 0.08041457273066044
Training loss on iteration 1520 = 0.07545759566128254
Training loss on iteration 1540 = 0.08113325610756875
Training loss on iteration 1560 = 0.07789617143571377
Training loss on iteration 1580 = 0.0734568590298295
Training loss on iteration 1600 = 0.08343625348061323
Training loss on iteration 1620 = 0.08309071958065033
Training loss on iteration 1640 = 0.0781282415613532
Training loss on iteration 1660 = 0.08232996761798858
Training loss on iteration 1680 = 0.09640899114310741
Training loss on iteration 1700 = 0.07510258257389069
Training loss on iteration 1720 = 0.07272183913737536
Training loss on iteration 1740 = 0.07826643157750368
Training loss on iteration 1760 = 0.08233836907893419
Training loss on iteration 1780 = 0.07810554131865502
Training loss on iteration 1800 = 0.08075068481266498
Training loss on iteration 1820 = 0.08043951224535703
Training loss on iteration 1840 = 0.08057571649551391
Training loss on iteration 1860 = 0.08122856300324202
Training loss on iteration 1880 = 0.08129640016704798
Training loss on iteration 1900 = 0.08151973076164723
Training loss on iteration 1920 = 0.080495367012918
Training loss on iteration 1940 = 0.08324852306395769
Training loss on iteration 1960 = 0.07412767671048641
Training loss on iteration 1980 = 0.08087703119963408
Training loss on iteration 2000 = 0.08264308534562588
Training loss on iteration 2020 = 0.0841038018465042
Training loss on iteration 2040 = 0.07934851981699467
Training loss on iteration 2060 = 0.08198772761970759
Training loss on iteration 2080 = 0.07313551679253578
Training loss on iteration 2100 = 0.0790533322840929
Training loss on iteration 2120 = 0.08078986760228872
Training loss on iteration 2140 = 0.08243522867560386
Training loss on iteration 2160 = 0.09669245481491089
Training loss on iteration 2180 = 0.07694992870092392
Training loss on iteration 2200 = 0.08767708595842123
Training loss on iteration 2220 = 0.07922837883234024
Training loss on iteration 2240 = 0.0825922666117549
Training loss on iteration 2260 = 0.09154001846909524
Training loss on iteration 2280 = 0.08254605885595083
Training loss on iteration 2300 = 0.07922719661146402
Training loss on iteration 2320 = 0.08721587043255567
Training loss on iteration 2340 = 0.07804921139031648
Training loss on iteration 2360 = 0.08773901145905257
Training loss on iteration 2380 = 0.07654294427484273
Training loss on iteration 2400 = 0.08139031454920768
Training loss on iteration 2420 = 0.09416068829596043
Training loss on iteration 2440 = 0.08384679723531008
Training loss on iteration 2460 = 0.0692719854414463
Training loss on iteration 2480 = 0.08232000879943371
Training loss on iteration 2500 = 0.08354867920279503
Training loss on iteration 2520 = 0.08324408680200576
Training loss on iteration 2540 = 0.08359370063990354
Training loss on iteration 2560 = 0.07410215605050326
Training loss on iteration 2580 = 0.07453457396477461
Training loss on iteration 2600 = 0.07830612454563379
Training loss on iteration 2620 = 0.1045077795162797
Training loss on iteration 2640 = 0.08306614253669978
Training loss on iteration 2660 = 0.08038841970264912
Training loss on iteration 2680 = 0.08781950827687979
Training loss on iteration 2700 = 0.07868391312658787
Training loss on iteration 2720 = 0.0781168645247817
Training loss on iteration 2740 = 0.07958527598530055
Training loss on iteration 2760 = 0.10412190556526184
Training loss on iteration 2780 = 0.08514425344765186
Training loss on iteration 2800 = 0.08666678983718157
Training loss on iteration 2820 = 0.08043411877006293
Training loss on iteration 2840 = 0.08369946386665106
Training loss on iteration 2860 = 0.08559254258871078
Training loss on iteration 2880 = 0.08155798222869634
Training loss on iteration 2900 = 0.08985325060784817
Training loss on iteration 2920 = 0.08556482177227735
Training loss on iteration 2940 = 0.10632834397256374
Training loss on iteration 2960 = 0.0790148288011551
Training loss on iteration 2980 = 0.07855160720646381
Training loss on iteration 3000 = 0.08895829729735852
Training loss on iteration 3020 = 0.08085004109889268
Training loss on iteration 3040 = 0.0749680558219552
Training loss on iteration 3060 = 0.07905107289552689
Training loss on iteration 3080 = 0.08026902936398983
Training loss on iteration 3100 = 0.07561934366822243
Training loss on iteration 3120 = 0.07703857440501452
Training loss on iteration 3140 = 0.07964404840022325
Training loss on iteration 3160 = 0.07772054821252823
Training loss on iteration 3180 = 0.07757496796548366
Training loss on iteration 3200 = 0.08243761844933033
Training loss on iteration 3220 = 0.07883605603128671
Training loss on iteration 3240 = 0.08593791238963604
Training loss on iteration 3260 = 0.07869354523718357
Training loss on iteration 3280 = 0.07995597515255212
Training loss on iteration 3300 = 0.08308891244232655
Training loss on iteration 3320 = 0.08321052081882954
Training loss on iteration 3340 = 0.08388874400407076
Training loss on iteration 3360 = 0.07966040261089802
Training loss on iteration 3380 = 0.079991140589118
Training loss on iteration 3400 = 0.08173701837658882
Training loss on iteration 3420 = 0.09477658737450838
Training loss on iteration 3440 = 0.08419940061867237
Training loss on iteration 3460 = 0.07549848221242428
Training loss on iteration 3480 = 0.08710600584745407
Training loss on iteration 3500 = 0.07571961060166359
Training loss on iteration 3520 = 0.08459415901452302
Training loss on iteration 3540 = 0.07748957853764296
Training loss on iteration 3560 = 0.08313106000423431
Training loss on iteration 3580 = 0.0763191569596529
Training loss on iteration 3600 = 0.07342112492769956
Training loss on iteration 3620 = 0.0823689928278327
Training loss on iteration 3640 = 0.07998194005340338
Training loss on iteration 3660 = 0.08092159051448107
Training loss on iteration 3680 = 0.07195009347051382
Training loss on iteration 3700 = 0.08257067278027534
Training loss on iteration 3720 = 0.0857656266540289
Training loss on iteration 3740 = 0.2352424342185259
Training loss on iteration 3760 = 0.07704311534762383
Training loss on iteration 3780 = 0.07558723501861095
Training loss on iteration 3800 = 0.08408799469470978
Training loss on iteration 3820 = 0.08282905519008636
Training loss on iteration 3840 = 0.08453799095004796
Training loss on iteration 3860 = 0.07723564933985472
Training loss on iteration 3880 = 0.08006583042442798
Training loss on iteration 3900 = 0.08939330689609051
Training loss on iteration 3920 = 0.07998966965824365
Training loss on iteration 3940 = 0.08414183761924506
Training loss on iteration 3960 = 0.08106372989714146
Training loss on iteration 3980 = 0.08112634755671025
Training loss on iteration 4000 = 0.08840611111372709
Training loss on iteration 4020 = 0.07971335425972939
Training loss on iteration 4040 = 0.0859516903758049
Training loss on iteration 4060 = 0.07381337080150843
Training loss on iteration 4080 = 0.08847707398235798
Training loss on iteration 4100 = 0.08213989455252886
Training loss on iteration 4120 = 0.07560086958110332
Training loss on iteration 4140 = 0.07972144205123186
Training loss on iteration 4160 = 0.09222237262874841
Training loss on iteration 4180 = 0.08086351379752159
Training loss on iteration 4200 = 0.0734549505636096
Training loss on iteration 4220 = 0.08492343351244927
Training loss on iteration 4240 = 0.1423920152708888
Training loss on iteration 4260 = 0.08230240065604448
Training loss on iteration 4280 = 0.0807937839999795
Training loss on iteration 4300 = 0.0772452387958765
Training loss on iteration 4320 = 0.0774889949709177
Training loss on iteration 4340 = 0.0769113851711154
Training loss on iteration 4360 = 0.07688226625323295
Training loss on iteration 4380 = 0.07363429572433233
Training loss on iteration 4400 = 0.09445029515773058
Training loss on iteration 4420 = 0.0743633596226573
Training loss on iteration 4440 = 0.08285512588918209
Training loss on iteration 4460 = 0.0879317244514823
Training loss on iteration 4480 = 0.10311701651662589
Training loss on iteration 4500 = 0.08095900844782591
Training loss on iteration 4520 = 0.08743010610342025
Training loss on iteration 4540 = 0.07232740689069032
Training loss on iteration 4560 = 0.07962463609874249
Training loss on iteration 4580 = 0.08706497140228749
Training loss on iteration 4600 = 0.08552449159324169
Training loss on iteration 4620 = 0.07863719165325164
Training loss on iteration 4640 = 0.08366679362952709
Training loss on iteration 4660 = 0.07237225193530321
Training loss on iteration 4680 = 0.08080315124243498
Training loss on iteration 4700 = 0.08212550636380911
Training loss on iteration 4720 = 0.0805670315399766
Training loss on iteration 4740 = 0.07810223121196032
Training loss on iteration 4760 = 0.08807725813239813
Training loss on iteration 4780 = 0.08755183462053537
Training loss on iteration 4800 = 0.07777728009968995
Training loss on iteration 4820 = 0.07576769664883613
Training loss on iteration 4840 = 0.09036485087126493
Training loss on iteration 4860 = 0.08397917114198208
Training loss on iteration 4880 = 0.08399885352700949
Training loss on iteration 4900 = 0.08149333130568266
Training loss on iteration 4920 = 0.07720414325594901
Training loss on iteration 4940 = 0.07729665320366622
Training loss on iteration 4960 = 0.08714948706328869
Training loss on iteration 4980 = 0.08346528559923172
Training loss on iteration 5000 = 0.08411287721246481
Training loss on iteration 5020 = 0.08981609717011452
Training loss on iteration 5040 = 0.07754724714905023
Training loss on iteration 5060 = 0.08521941173821687
Training loss on iteration 5080 = 0.08951384127140045
Training loss on iteration 5100 = 0.07705969288945198
Training loss on iteration 5120 = 0.08016555272042751
Training loss on iteration 5140 = 0.07498602364212274
Training loss on iteration 5160 = 0.08677732143551112
Training loss on iteration 5180 = 0.08158341813832522
Training loss on iteration 5200 = 0.08111100178211927
Training loss on iteration 5220 = 0.088551564514637
Training loss on iteration 5240 = 0.08657118249684573
Training loss on iteration 5260 = 0.07426080014556646
Training loss on iteration 5280 = 0.08702568300068378
Training loss on iteration 5300 = 0.08335145078599453
Training loss on iteration 5320 = 0.08324896451085806
Training loss on iteration 5340 = 0.08205508999526501
Training loss on iteration 5360 = 0.07794498782604933
Training loss on iteration 5380 = 0.08529228270053864
Training loss on iteration 5400 = 0.07878506425768136
Training loss on iteration 5420 = 0.08329447358846664
Training loss on iteration 5440 = 0.08780822306871414
Training loss on iteration 5460 = 0.09560233112424613
Training loss on iteration 5480 = 0.11143632605671883
Training loss on iteration 5500 = 0.08892588671296835
Training loss on iteration 5520 = 0.08737903740257025
Training loss on iteration 5540 = 0.08435859400779008
Training loss on iteration 5560 = 0.08640098813921213
Training loss on iteration 5580 = 0.08269358184188605
Training loss on iteration 5600 = 0.07802010588347912
Training loss on iteration 5620 = 0.08090489022433758
Training loss on iteration 5640 = 0.08879931606352329
Training loss on iteration 5660 = 0.08627178184688092
Training loss on iteration 5680 = 0.08593833185732365
Training loss on iteration 5700 = 0.08813592828810216
Training loss on iteration 5720 = 0.08848178386688232
Training loss on iteration 5740 = 0.0890235947445035
Training loss on iteration 5760 = 0.0790013799443841
Training loss on iteration 5780 = 0.09804351665079594
Training loss on iteration 5800 = 0.08833622392266989
Training loss on iteration 5820 = 0.08051333706825972
Training loss on iteration 5840 = 0.0916157377883792
Training loss on iteration 5860 = 0.08766367789357901
Training loss on iteration 5880 = 0.08024023622274398
Training loss on iteration 5900 = 0.08300563376396894
Training loss on iteration 5920 = 0.09272832944989204
Training loss on iteration 5940 = 0.08859091456979513
Training loss on iteration 5960 = 0.07854043915867806
Training loss on iteration 5980 = 0.08117134738713502
Training loss on iteration 6000 = 0.07582263816148042
Training loss on iteration 6020 = 0.08495668042451143
Training loss on iteration 6040 = 0.08822770677506923
Training loss on iteration 6060 = 0.09843220524489879
Training loss on iteration 6080 = 0.08357261195778846
Training loss on iteration 6100 = 0.09092748221009969
Training loss on iteration 6120 = 0.08301266692578793
Training loss on iteration 6140 = 0.08212479557842016
Training loss on iteration 6160 = 0.08318489659577608
Training loss on iteration 6180 = 0.07886395696550608
Training loss on iteration 6200 = 0.08422790151089429
Training loss on iteration 6220 = 0.08165020793676377
Training loss on iteration 6240 = 0.07472584992647172
Training loss on iteration 6260 = 0.08490064591169358
Training loss on iteration 6280 = 0.08201709110289812
Training loss on iteration 6300 = 0.08587128296494484
Training loss on iteration 6320 = 0.08582279477268458
Training loss on iteration 6340 = 0.08382593989372253
Training loss on iteration 6360 = 0.08588091358542442
Training loss on iteration 6380 = 0.07693343237042427
